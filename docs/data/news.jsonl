{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:33:25.043974", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準多為單一控制環境，僅AI代理能使用工具與世界互動，使用者被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者皆可使用工具在共享動態環境中操作，考驗代理的協調與溝通能力。τ²-Bench包含可程式化任務生成器，能創造多樣化、可驗證的任務，並具備與環境緊密結合的使用者模擬器，提高模擬真實度。實驗顯示，代理在雙重控制環境下的表現明顯下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，未來在家裡設定網路，不再需要看著複雜的說明書。你可以直接跟AI客服對話，AI會一步步引導你操作數據機和路由器，就像朋友在旁邊教你一樣。", "醫院的AI掛號系統，不只幫你預約，還會根據你的症狀，引導你填寫正確的病歷資料，甚至教你如何在家量血壓、準備看診需要的資料，讓你看病更有效率。", "汽車導航不只告訴你怎麼走，還能在你開車遇到問題時，像爆胎了，AI會一步步引導你更換輪胎，確保安全。"], "pitch": "各位投資人，我們正在打造下一代的AI互動模式！傳統AI只能單向提供資訊，但我們的τ²-Bench技術，讓AI能像一位優秀的協作夥伴，與使用者共同完成任務。想像一下，未來的客服中心，AI不再只是回答問題，而是能引導客戶解決複雜的技術問題，大幅降低人力成本，提高客戶滿意度。這項技術的應用範圍極廣，從智慧家庭、遠程醫療到工業自動化，都蘊藏著巨大的商業潛力。我們正在申請專利，並積極尋找合作夥伴，共同開創這個全新的AI市場。現在加入我們，您將站在AI革命的最前線！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:34:55.861797", "title_zh": "Flow-Anything：從大規模單視角圖像中學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，但現有方法受限於動畫合成數據集的訓練，難以應用於真實世界。為了解決這個問題，我們提出了Flow-Anything，一個大規模數據生成框架，旨在從任何真實世界的單視角圖像中學習光流估計。我們首先利用單眼深度估計網路將單視角圖像轉換為3D表示，再利用物件獨立體積渲染模組和深度感知修復模組來模擬3D表示中的動態物件，從而生成逼真的光流訓練數據集（FA-Flow Dataset）。實驗證明，從大規模真實世界圖像生成光流訓練數據，效果優於最先進的無監督方法和在合成數據集上訓練的有監督方法。我們的模型可以作為基礎模型，提升各種下游影片任務的性能。", "applications": ["**智慧駕駛輔助系統：** 想像一下，你的汽車能更精準地判斷周圍車輛和行人的移動速度和方向，即使在惡劣天氣或光線不足的情況下，也能做出更安全的反應，避免碰撞。", "**運動賽事分析：** 透過分析球員在球場上的光流，可以更精準地追蹤他們的動作，分析戰術的執行效率，甚至預測他們的下一步動作，提供教練和球員更有效的訓練和比賽策略。", "**安全監控系統：** 監控系統可以更準確地檢測異常行為，例如有人跌倒或發生衝突，及時發出警報，保障公共安全。"], "pitch": "各位投資人，我們正在打造的是下一代視覺智能的基石！Flow-Anything 不僅僅是一個光流估計模型，而是一個能夠從海量真實世界圖像中自主學習的 AI 引擎。想像一下，它能賦予機器人更敏銳的視覺感知能力，讓它們在複雜的環境中自由穿梭；它能讓無人機更精準地進行航拍測繪，實現智慧城市管理；它甚至能幫助開發者創造出前所未有的 AR/VR 體驗。我們已經證明了 Flow-Anything 在真實世界數據上的卓越性能，遠超現有技術。現在，我們需要您的支持，將這項技術推向市場，搶占先機，共同開創一個由視覺智能驅動的全新時代！ 我們預期在三年內，Flow-Anything 將成為智慧駕駛、機器人、安防監控等領域的關鍵技術，市場規模將達到數十億美元！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:36:36.761479", "title_zh": "動態視角合成作為一個反問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個反問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，而無需更新權重或使用輔助模組。我們首先發現了零終端訊噪比排程對確定性反演造成的根本障礙，並透過引入一種新的雜訊表示法，即K階遞迴雜訊表示法，來解決這個問題。我們推導出此表示法的閉合形式表達式，從而實現VAE編碼和DDIM反轉潛在變量之間的精確有效對齊。為了合成由相機運動產生的新可見區域，我們引入了隨機潛在調製，它對潛在空間執行可見性感知採樣，以完成被遮擋的區域。綜合實驗表明，動態視角合成可以透過雜訊初始化階段的結構化潛在變量操作有效地執行。", "applications": ["**虛擬實境旅遊體驗：** 想像一下，你只需要用手機拍攝一段影片，就能將它轉換成360度的VR體驗，讓你身歷其境地重溫旅行的美好回憶，甚至探索從未去過的地方。", "**電影特效製作：** 電影製作人員可以使用這項技術，從現有的影片素材中創造出全新的視角和場景，節省大量拍攝成本和時間，讓特效更加逼真自然。", "**線上購物：** 顧客可以透過手機影片，從各個角度觀看商品，就像在實體店面一樣，提升購物體驗和購買意願。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它能將任何單眼影片轉換成高品質的3D動態視角，無需複雜的建模或昂貴的設備。試想一下，這項技術能應用於遊戲、電影、VR/AR、電商等各個領域，創造出前所未有的沉浸式體驗。例如，在遊戲中，玩家可以從任何角度觀看自己的角色，甚至可以創造出獨一無二的遊戲視角。在電商領域，顧客可以透過360度視角，全方位了解商品細節，大幅提升購買意願。更令人興奮的是，我們正在探索將這項技術應用於自動駕駛領域，透過合成多個視角，提升感知能力，讓自動駕駛更加安全可靠。我們的團隊擁有深厚的技術積累和創新能力，相信在各位的支持下，我們一定能將這項技術推向市場，創造巨大的商業價值，成為下一代視覺技術的領導者！", "audio": "docs/data/audios/2506.08004v1.wav"}
