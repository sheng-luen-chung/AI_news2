{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:33:25.043974", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準多為單一控制環境，僅AI代理能使用工具與世界互動，使用者被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者皆可使用工具在共享動態環境中操作，考驗代理的協調與溝通能力。τ²-Bench包含可程式化任務生成器，能創造多樣化、可驗證的任務，並具備與環境緊密結合的使用者模擬器，提高模擬真實度。實驗顯示，代理在雙重控制環境下的表現明顯下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，未來在家裡設定網路，不再需要看著複雜的說明書。你可以直接跟AI客服對話，AI會一步步引導你操作數據機和路由器，就像朋友在旁邊教你一樣。", "醫院的AI掛號系統，不只幫你預約，還會根據你的症狀，引導你填寫正確的病歷資料，甚至教你如何在家量血壓、準備看診需要的資料，讓你看病更有效率。", "汽車導航不只告訴你怎麼走，還能在你開車遇到問題時，像爆胎了，AI會一步步引導你更換輪胎，確保安全。"], "pitch": "各位投資人，我們正在打造下一代的AI互動模式！傳統AI只能單向提供資訊，但我們的τ²-Bench技術，讓AI能像一位優秀的協作夥伴，與使用者共同完成任務。想像一下，未來的客服中心，AI不再只是回答問題，而是能引導客戶解決複雜的技術問題，大幅降低人力成本，提高客戶滿意度。這項技術的應用範圍極廣，從智慧家庭、遠程醫療到工業自動化，都蘊藏著巨大的商業潛力。我們正在申請專利，並積極尋找合作夥伴，共同開創這個全新的AI市場。現在加入我們，您將站在AI革命的最前線！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:34:55.861797", "title_zh": "Flow-Anything：從大規模單視角圖像中學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，但現有方法受限於動畫合成數據集的訓練，難以應用於真實世界。為了解決這個問題，我們提出了Flow-Anything，一個大規模數據生成框架，旨在從任何真實世界的單視角圖像中學習光流估計。我們首先利用單眼深度估計網路將單視角圖像轉換為3D表示，再利用物件獨立體積渲染模組和深度感知修復模組來模擬3D表示中的動態物件，從而生成逼真的光流訓練數據集（FA-Flow Dataset）。實驗證明，從大規模真實世界圖像生成光流訓練數據，效果優於最先進的無監督方法和在合成數據集上訓練的有監督方法。我們的模型可以作為基礎模型，提升各種下游影片任務的性能。", "applications": ["**智慧駕駛輔助系統：** 想像一下，你的汽車能更精準地判斷周圍車輛和行人的移動速度和方向，即使在惡劣天氣或光線不足的情況下，也能做出更安全的反應，避免碰撞。", "**運動賽事分析：** 透過分析球員在球場上的光流，可以更精準地追蹤他們的動作，分析戰術的執行效率，甚至預測他們的下一步動作，提供教練和球員更有效的訓練和比賽策略。", "**安全監控系統：** 監控系統可以更準確地檢測異常行為，例如有人跌倒或發生衝突，及時發出警報，保障公共安全。"], "pitch": "各位投資人，我們正在打造的是下一代視覺智能的基石！Flow-Anything 不僅僅是一個光流估計模型，而是一個能夠從海量真實世界圖像中自主學習的 AI 引擎。想像一下，它能賦予機器人更敏銳的視覺感知能力，讓它們在複雜的環境中自由穿梭；它能讓無人機更精準地進行航拍測繪，實現智慧城市管理；它甚至能幫助開發者創造出前所未有的 AR/VR 體驗。我們已經證明了 Flow-Anything 在真實世界數據上的卓越性能，遠超現有技術。現在，我們需要您的支持，將這項技術推向市場，搶占先機，共同開創一個由視覺智能驅動的全新時代！ 我們預期在三年內，Flow-Anything 將成為智慧駕駛、機器人、安防監控等領域的關鍵技術，市場規模將達到數十億美元！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:36:36.761479", "title_zh": "動態視角合成作為一個反問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個反問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，而無需更新權重或使用輔助模組。我們首先發現了零終端訊噪比排程對確定性反演造成的根本障礙，並透過引入一種新的雜訊表示法，即K階遞迴雜訊表示法，來解決這個問題。我們推導出此表示法的閉合形式表達式，從而實現VAE編碼和DDIM反轉潛在變量之間的精確有效對齊。為了合成由相機運動產生的新可見區域，我們引入了隨機潛在調製，它對潛在空間執行可見性感知採樣，以完成被遮擋的區域。綜合實驗表明，動態視角合成可以透過雜訊初始化階段的結構化潛在變量操作有效地執行。", "applications": ["**虛擬實境旅遊體驗：** 想像一下，你只需要用手機拍攝一段影片，就能將它轉換成360度的VR體驗，讓你身歷其境地重溫旅行的美好回憶，甚至探索從未去過的地方。", "**電影特效製作：** 電影製作人員可以使用這項技術，從現有的影片素材中創造出全新的視角和場景，節省大量拍攝成本和時間，讓特效更加逼真自然。", "**線上購物：** 顧客可以透過手機影片，從各個角度觀看商品，就像在實體店面一樣，提升購物體驗和購買意願。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它能將任何單眼影片轉換成高品質的3D動態視角，無需複雜的建模或昂貴的設備。試想一下，這項技術能應用於遊戲、電影、VR/AR、電商等各個領域，創造出前所未有的沉浸式體驗。例如，在遊戲中，玩家可以從任何角度觀看自己的角色，甚至可以創造出獨一無二的遊戲視角。在電商領域，顧客可以透過360度視角，全方位了解商品細節，大幅提升購買意願。更令人興奮的是，我們正在探索將這項技術應用於自動駕駛領域，透過合成多個視角，提升感知能力，讓自動駕駛更加安全可靠。我們的團隊擁有深厚的技術積累和創新能力，相信在各位的支持下，我們一定能將這項技術推向市場，創造巨大的商業價值，成為下一代視覺技術的領導者！", "audio": "docs/data/audios/2506.08004v1.wav"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:15:25.013920", "title_zh": "理解隱私感知運算的錯誤敏感性", "summary_zh": "同態加密（HE）允許在加密數據上進行安全計算，無需解密，為保護隱私的計算提供了絕佳機會。醫療保健、金融和政府等領域對數據隱私和安全極為重視，可藉由HE在敏感數據上實現第三方計算和服務。HE的安全性依賴於添加到數據中的雜訊，使相對簡單的問題在計算上變得難以處理。然而，這種以錯誤為中心的機制也帶來了新的挑戰，即HE本身的容錯性和穩健性。本研究深入探討HE應用對位元錯誤的敏感性，並詳細分析了CKKS方案的錯誤特性，為同態加密的穩健性和錯誤敏感性研究奠定基礎。", "applications": ["**醫療數據共享：** 醫院之間可以安全地共享患者的基因組數據，用於疾病研究，而無需暴露患者的個人隱私。", "**金融交易安全：** 銀行可以在加密的交易數據上進行風險評估和欺詐檢測，保護客戶的財務信息。", "**投票系統：** 選舉投票可以在完全加密的狀態下進行，確保選票的保密性，同時允許公開驗證選舉結果的正確性。"], "pitch": "各位創投先進，想像一下，一個數據永遠處於加密狀態，使用過程中也受到保護的世界！這就是同態加密（HE）的願景，也是我們正在實現的目標。HE如同數據安全的聖杯，將徹底改變醫療、金融、政府等對隱私極其敏感的行業。我們的研究不僅深入理解了HE的錯誤敏感性，更為其大規模應用掃清了障礙。試想，未來的AI模型可以在完全加密的醫療數據上進行訓練，藥廠無需接觸任何原始數據即可開發新藥；銀行可以安全地分析數百萬筆交易，預防金融犯罪，同時保護用戶隱私。這不僅僅是技術突破，更是一場數據安全革命！我們團隊的研究成果將為HE的硬體和軟體優化提供關鍵指導，使其更可靠、更高效。隨著數據隱私意識的日益增強，HE市場將迎來爆發式增長。現在投資我們，您將站在這場變革的最前沿，共同打造一個更安全、更隱私的數據未來！預計五年內，基於我們技術的HE解決方案將滲透到各個行業，市場規模將達到數十億美元。不要錯過這個千載難逢的機會！", "audio": "docs/data/audios/2506.07957v1.wav"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:17:50.350602", "title_zh": "基於大型模型的機器聯覺 (SoM)：AI原生智慧多模感測-通訊整合", "summary_zh": "為了支援未來的智慧多功能第六代 (6G) 無線通訊網路，機器聯覺 (SoM) 被提出作為一種新穎的人工智慧 (AI) 原生智慧多模感測-通訊整合範例。本文首次針對 SoM 系統設計的大型模型 (FMs) 進行系統分類，將其分為通用 FMs（尤其是大型語言模型 (LLMs)）和 SoM 領域特定的 FMs（稱為無線基礎模型）。我們提出了基於 LLM 的路徑損耗生成 (LLM4PG) 和散射體生成 (LLM4SG) 方案，以及用於 SoM 機制探索的無線通道基礎模型 (WiCo)、用於 SoM 增強型收發器設計的基於 LLM 的無線多任務 SoM 收發器 (LLM4WM) 和無線基礎模型 (WiFo)，以及用於 SoM 增強型協作感知的無線協作感知基礎模型 (WiPo)。", "applications": ["想像一下，未來自駕車能透過整合視覺、雷達、以及其他車輛分享的資訊，更精準地預測路況，就像人類能透過多種感官來判斷環境一樣，大幅提升行車安全。", "在智慧工廠中，機器人能整合視覺、聽覺、觸覺等多種感測資訊，更靈敏地執行複雜任務，例如辨識瑕疵品、調整生產線參數，提高生產效率和產品品質。", "未來的醫療診斷，醫生可以利用AI整合病人的生理數據、影像資料、以及口述病史，更全面地了解病情，做出更精準的診斷和治療方案。"], "pitch": "各位投資人，我們正在開發一種革命性的技術，名為「機器聯覺」(SoM)。它就像是賦予機器擁有多重感官融合的能力，讓它們能更聰明、更靈敏地應對複雜的現實世界。想像一下，一個具備人類般直覺的AI系統，可以廣泛應用於自駕車、智慧工廠、醫療診斷等領域，徹底顛覆這些產業。我們的技術基於最新的大型模型，突破了傳統AI的局限性，具有更強的泛化能力和適應性。這是一個千載難逢的投資機會，讓我們一起打造一個由智慧機器主導的未來！初期我們將專注在自駕車領域，與Tier 1 供應商合作，快速將技術落地，搶佔市場先機。預計三年內，我們的技術將成為自駕車的標配，五年內將擴展到其他領域，創造巨大的商業價值。不要錯過這個機會，加入我們，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.07647v1.wav"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:19:15.460624", "title_zh": "MADFormer：混合自迴歸與擴散轉換器用於連續圖像生成", "summary_zh": "MADFormer結合了自迴歸（AR）和擴散模型，旨在優化圖像生成。它將圖像分割成空間區塊，利用AR層進行全局條件設定，而擴散層則在區塊內進行迭代局部細化。實驗證明，區塊分割能有效提升高解析度圖像的生成效果，垂直混合AR和擴散層則能在運算資源有限的情況下，顯著提升圖像品質。MADFormer的研究結果為未來混合生成模型的設計提供了實用指導原則，尤其在高解析度圖像生成領域展現了巨大的潛力。", "applications": ["**客製化頭像生成：**使用者只需提供少量資訊，就能生成獨一無二、高解析度的個人頭像，可用於社群媒體或遊戲。", "**老照片修復：**將模糊、破損的老照片透過AI技術進行修復，恢復清晰細節，讓珍貴回憶重現。", "**藝術創作輔助：**藝術家可以利用這項技術，輸入草稿或概念，快速生成多種風格的高品質藝術作品，激發創作靈感。"], "pitch": "各位創投，MADFormer不僅僅是一項技術，它代表著圖像生成領域的未來！想像一下，一個能以極高效率和品質生成圖像的AI引擎，它的應用範圍將無遠弗屆。從遊戲美術資源的快速生成、電影特效的製作，到廣告行銷素材的客製化，甚至在虛擬實境和元宇宙中創造逼真場景，MADFormer都能扮演關鍵角色。更重要的是，我們已經證明，在運算資源有限的情況下，MADFormer也能超越現有技術。這意味著更低的成本、更快的部署速度，以及更廣泛的市場潛力。現在投資MADFormer，您將搶佔AI圖像生成市場的先機，共同開創視覺內容的新紀元！我們預計，在三年內，MADFormer將成為業界標準，市值突破十億美元！", "audio": "docs/data/audios/2506.07999v1.wav"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:24:04.792554", "title_zh": "學生作業自動化AI評分之實施考量", "summary_zh": "本研究探討在K-12教育環境中導入AI評分平台的可行性，與19位教師合作進行試驗。透過平台使用紀錄、問卷和訪談，觀察教師如何運用AI產生的評分標準和回饋。研究發現，教師重視AI快速提供的敘述性回饋，但對自動評分抱持懷疑，強調人工監督的必要性。學生歡迎快速且著重修改建議的回饋，但對完全由AI評分感到質疑。本研究旨在為設計可信賴、以教師為中心的AI評估工具提供參考，在強化回饋的同時，維持教學主導權。", "applications": ["國小老師批改作文不用再熬夜！AI幫忙抓出文法錯誤、提供修改建議，老師只要專注在內容的引導，讓每個孩子都能愛上寫作。", "大學教授出了一份程式作業，AI能快速檢查程式碼的bug，並給予學生改善方向，讓教授有更多時間指導學生更進階的程式技巧。", "線上學習平台能夠根據學生提交的報告，AI立即提供個人化的回饋，讓學生隨時都能獲得學習上的支持，學習效果UP UP！"], "pitch": "各位投資人，想像一下，一個AI能大幅減輕老師負擔、提升學生學習效率的未來！我們的AI評分平台，不僅能快速提供回饋，更能客製化評估標準，協助老師進行更精準的教學。雖然目前我們強調人工監督，但隨著技術不斷演進，未來AI將能獨立完成初步評分，甚至預測學生的學習瓶頸，提供個人化學習方案。這不只是一個評分工具，更是一個教育革新的起點！市場潛力巨大，從K-12到高等教育，甚至企業培訓，都需要這樣一個能提升效率、降低成本的AI助手。現在投資，您將成為教育AI化的領航者，共同打造更美好的教育未來！", "audio": "docs/data/audios/2506.07955v1.wav"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:25:51.149942", "title_zh": "SurgBench：手術影片分析的統一大型基準", "summary_zh": "SurgBench是一個大型手術影片基準測試框架，包含一個預訓練資料集SurgBench-P和一個評估基準SurgBench-E。SurgBench-P涵蓋22種手術程序和11個專科的5300萬幀影像，SurgBench-E提供六個類別的評估，包含階段分類、相機運動、工具識別、疾病診斷、動作分類和器官檢測，共72個細粒度任務。實驗顯示，現有的影片基礎模型難以在不同的手術影片分析任務中推廣，而使用SurgBench-P進行預訓練可以顯著提高性能，並對未見過的手術程序和模態展現卓越的跨領域泛化能力。SurgBench有助於開發更強大的手術影片分析模型，進而實現手術自動化決策、技能評估和術後品質提升。", "applications": ["醫生可以利用SurgBench訓練的模型，在手術過程中即時獲得工具使用建議，減少失誤，提升手術效率。", "醫學院學生可以透過SurgBench建立的虛擬手術環境，反覆練習手術技巧，降低學習曲線，並在安全環境下熟悉各種手術流程。", "病患家屬可以藉由SurgBench分析手術影片，更了解手術過程，並作為術後照護的參考依據，提升對醫療品質的信心。"], "pitch": "各位投資人，我們正在打造手術室的AI大腦！SurgBench不僅是一個龐大的手術影片資料庫，更是一個賦能醫療AI的引擎。想像一下，AI能即時分析手術影片，協助醫生做出更精準的判斷，降低手術風險，提升成功率。這不僅能減少醫療糾紛，更能大幅降低醫療成本。未來，SurgBench甚至能支援遠程手術，讓偏鄉地區也能享有頂尖醫療資源。我們預期，SurgBench將成為手術機器人、醫療影像分析等領域的基石，引領醫療AI的革命，創造數十億美元的市場價值。現在加入，您將成為這場醫療AI革命的領航者！", "audio": "docs/data/audios/2506.07603v1.wav"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:28:20.700342", "title_zh": "權重的生成模型：泛化還是記憶？", "summary_zh": "近年來，生成模型在圖像和影片生成領域取得了巨大成功，因此人們開始探索使用它們來合成有效的神經網路權重。這些方法將訓練好的神經網路檢查點作為訓練資料，並旨在於推理過程中生成高性能的神經網路權重。然而，研究發現這些方法在很大程度上是通過記憶來合成權重，產生的權重要么是訓練檢查點的複製品，要么只是簡單的插值。它們無法勝過簡單的基線方法，例如在權重中添加噪聲或採用簡單的權重集成。而且，即使修改與圖像擴散模型中記憶相關的建模因素或應用資料增強，也無法有效緩解這種記憶現象。這項研究對當前生成模型可以建模的資料類型進行了實際評估，並強調需要在新領域中更仔細地評估生成模型。", "applications": ["客製化AI模型：想像一下，你可以根據自己的需求，快速生成一個AI模型，例如，針對特定疾病的醫療影像分析模型，或針對特定風格的藝術創作模型，而不需要從頭開始訓練。", "AI模型的自動修復：當AI模型因為資料偏移而表現不佳時，可以利用生成模型快速生成新的權重，讓模型恢復到最佳狀態，就像AI模型的自動醫生一樣。", "保護AI模型的智慧財產權：透過生成模型，可以將訓練好的AI模型轉換成另一種形式，既保留了模型的功能，又避免了原始權重被直接複製，從而保護模型的智慧財產權。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能讓AI模型的開發和應用變得前所未有的簡單和高效。雖然目前的研究顯示生成模型在權重生成方面存在記憶問題，但這也代表我們有巨大的突破空間！想像一下，如果我們能克服這個難題，就能夠按需生成各種高度客製化的AI模型，徹底顛覆AI產業。例如，我們可以為每個客戶量身打造獨一無二的AI解決方案，從醫療診斷到金融風控，再到藝術創作，應用前景無限廣闊。更重要的是，這項技術還能有效保護AI模型的智慧財產權，讓您投資的AI技術不再容易被複製和盜用。現在投資，您將有機會參與塑造AI的未來，並獲得豐厚的回報！我們需要您的資金來進一步研究和開發，克服當前的技術瓶頸，最終實現AI模型的真正泛化和創造，而不是單純的記憶和複製。這不僅是一項技術投資，更是一項對未來的投資，是對AI無限可能的投資！", "audio": "docs/data/audios/2506.07998v1.wav"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:14:58.562119", "title_zh": "具成本效益的最佳化主動式AI模型評估", "summary_zh": "生成式AI系統的開發需要持續評估、資料獲取和標註，這在資源和時間上都非常昂貴。為了降低成本，快速迭代通常需要依賴合成標註資料，但這可能導致嚴重的偏差。本研究開發了一種新型的、具成本意識的方法，能主動平衡使用低成本但通常不準確的弱評估者（例如基於模型的自動評估器，用於自動評估生成內容的品質）和更昂貴但更準確的強評估者（例如人類）。我們的目標是在總標註預算限制下，產生目標「強」評估的低變異數、無偏估計。藉由主動和預測驅動的統計推論，我們推導出了一系列具成本效益的最佳化策略，用於在弱評估者和強評估者之間分配給定的標註預算，從而最大化統計效率。使用合成和真實世界的數據，我們驗證了這些策略在哪些條件下可以改進現有方法。我們發現，特別是在範例難度差異很大的任務中，我們的策略可以用比標準評估方法低得多的總標註預算，實現相同的估計精度。", "applications": ["**線上課程平台：** 平台可以使用這項技術，自動評估學生提交的作業品質。系統可以先用AI自動評分，針對AI難以判斷或AI評分可信度較低的作業，再交由真人老師評分，節省老師的時間和精力，同時確保評分品質。", "**客服機器人訓練：** 訓練客服機器人需要大量的對話資料。這項技術可以用來判斷哪些對話需要人工客服介入，哪些可以直接由機器人處理。系統可以使用低成本的AI模型初步判斷，再將難以判斷的對話轉交給真人客服，提高客服效率，降低人力成本。", "**內容創作平台：** 平台可以使用這項技術來評估使用者生成內容（例如文章、圖片、影片）的品質。系統可以先用AI自動評估，針對AI難以判斷或AI評分可信度較低的內容，再交由真人編輯審核，確保平台內容品質，提升使用者體驗。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它將徹底改變AI模型的評估方式。想像一下，您不再需要投入大量資金聘請專家來評估AI模型的品質，也不用擔心使用低品質的合成資料會導致模型出現偏差。我們的技術能夠智慧地平衡使用低成本的AI評估器和高精度的真人評估器，在預算有限的情況下，最大化評估效率。這項技術的應用範圍非常廣泛，從自動駕駛、醫療診斷到金融風控，任何需要高品質AI模型的領域都將受益。更重要的是，隨著AI技術的快速發展，對模型評估的需求也將越來越大，我們的技術將成為市場上的必需品。我們預計，在未來五年內，AI模型評估市場將達到數十億美元的規模，而我們將在這個市場中佔據領先地位。現在加入我們，共同打造AI評估的未來，實現百倍甚至千倍的投資回報！", "audio": "docs/data/audios/2506.07949v1.wav"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:16:23.097350", "title_zh": "MIRA：用於真實世界健康數據的醫療時間序列基礎模型", "summary_zh": "MIRA是一個專為醫療時間序列預測設計的基礎模型。它利用連續時間旋轉位置編碼處理不規則時間間隔，並透過頻率特定的專家混合層來處理不同的採樣率，以及基於神經常微分方程的連續動態外推區塊來模擬潛在狀態的連續軌跡，從而實現精準預測。MIRA在包含超過4540億個時間點的大規模醫療語料庫上進行預訓練，在零樣本和微調基準測試中，其預測誤差平均降低了10%（異地分佈情境）和7%（同地分佈情境）。同時我們也建立了一個全面的基準，為未來醫療時間序列建模的研究奠定基礎。", "applications": ["想像一下，醫院可以利用MIRA精準預測病患的病情變化，提早發現潛在風險，例如心臟病發作或敗血症，讓醫生有更多時間採取預防措施，提高病患的存活率。", "長照中心可以透過MIRA監測長者的生理數據，預測跌倒或失智症惡化的風險，及早提供協助，減輕照顧者的負擔，提升長者的生活品質。", "個人化的健康管理App可以使用MIRA分析使用者的睡眠、運動和飲食數據，預測罹患慢性疾病的風險，提供客製化的健康建議，幫助使用者維持健康的生活方式。"], "pitch": "各位投資人，我們正處於醫療AI的黃金時代！MIRA，不只是一個模型，而是醫療時間序列分析的全新基礎設施。想像一下，一個能夠精準預測疾病進程、優化治療方案的AI，將徹底改變醫療產業。MIRA在大型醫療數據集上的卓越表現已經證明了它的潛力，相較於現有方案，MIRA能更有效地利用醫療數據，解決數據稀缺和隱私限制等難題。這意味著更低的開發成本、更快的部署速度和更廣泛的應用場景。從精準醫療到遠程監護，從藥物研發到保險理賠，MIRA的應用前景無可限量。我們相信，MIRA將成為醫療AI領域的領頭羊，為投資者帶來豐厚的回報。現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.07584v1.wav"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:17:45.268193", "title_zh": "重新思考多模態擴散轉換器中的跨模態互動", "summary_zh": "多模態擴散轉換器（MM-DiT）在文本驅動的視覺生成方面取得了顯著進展，但現有模型在文本提示與生成內容的精確對齊方面仍有困難。研究發現，MM-DiT的注意力機制存在兩個主要問題：視覺和文本模態之間的token不平衡導致跨模態注意力被抑制；缺乏時間步感知的注意力權重，阻礙了對齊。為了解決這些問題，研究提出溫度調整跨模態注意力（TACA），一種參數高效的方法，通過溫度縮放和時間步相關調整動態地重新平衡多模態互動。結合LoRA微調，TACA能以極小的計算開銷顯著提高文本-圖像對齊。", "applications": ["**智慧型購物體驗：** 想像一下，你只要用文字描述想要的衣服款式、顏色和材質，AI就能立刻生成穿在你身上的模擬圖，讓你輕鬆找到最適合自己的商品，省去試穿的麻煩。", "**個人化藝術創作：** 不擅長繪畫也沒關係！只要輸入你腦海中的畫面描述，AI就能幫你生成獨一無二的藝術作品，讓每個人都能成為藝術家。", "**遊戲與虛擬實境內容生成：** 遊戲開發者或VR內容創作者可以透過文字快速生成遊戲場景、角色外觀，大幅降低開發成本，並讓玩家擁有更豐富、更客製化的遊戲體驗。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓文字描述直接轉化為栩栩如生的圖像，精準度遠超現有模型。想像一下，未來廣告商可以根據受眾的細微偏好，即時生成高度客製化的廣告素材；建築師可以快速將客戶的口頭想法轉化為逼真的3D模型；甚至是醫療領域，醫生可以透過患者的描述生成病灶圖像，輔助診斷。這項技術的潛力無窮，我們相信它將徹底改變內容創作、設計、行銷等各個領域。現在加入我們，共同打造圖像生成的未來，分享這項技術帶來的巨大商業價值！", "audio": "docs/data/audios/2506.07986v1.wav"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:24:11.283915", "title_zh": "利用C-偶與C-奇量子關聯的$D\\bar{D}$對，對$D^0\\to K^-π^+$ 與 $\\bar{D}^0\\to K^-π^+$ 衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的測量$D^0$與$\\bar{D}^0$介子衰變振幅間強相位差的技術，利用正負電子碰撞產生能量高於$\\\\psi(3770)$產生閾值的量子關聯$D\\bar{D}$對。在這種能量下，$D\\bar{D}$對產生於電荷共軛對稱性的偶數和奇數本徵態。利用此技術，首次使用此類數據樣本確定了$D^0$-$\\bar{D^0}$相對強相位。使用BESIII實驗在質心能量4.13-4.23 GeV下收集的7.13 fb$^{-1}$積分光度的數據集，測得$D^0\\to K^-\\\\pi^+$ 與 $\\bar{D}^0\\to K^-\\\\pi^+$ 衰變之間的強相位差$\\delta^{D}_{K\\\\pi}$為$\\delta^{D}_{K\\\\pi}=\\\\left(192.8^{+11.0 + 1.9}_{-12.4 -2.4}\\\\right)^\\\\circ$。", "applications": ["想像一下，我們能更精準地預測天氣變化，不再只是看氣象雲圖，而是能深入了解大氣中微小粒子的交互作用，提前預知極端氣候的發生。", "未來的醫療診斷將更快速準確。醫生可以透過分析人體內微量物質的衰變模式，早期發現癌症或其他疾病的徵兆，大幅提高治療成功率。", "在材料科學領域，這項技術能幫助我們設計出更堅固、更耐用的新材料。透過精確控制材料內部的微觀結構，製造出能承受極端環境的超級材料。"], "pitch": "各位創投先進，我們帶來的是一項劃時代的技術突破，它將徹底改變我們對亞原子世界的理解。這項針對D介子衰變的研究，不僅驗證了量子理論的精確性，更開啟了通往未知物理學的大門。想像一下，如果我們能掌握控制亞原子粒子的能力，就能夠開發出能量密度極高的新型電池，徹底解決能源危機。我們甚至可以製造出超微型機器人，深入人體修復受損細胞，實現真正的長生不老。這項技術的潛在商業價值是難以估量的，它將引領下一次科技革命，成為未來世界的基石。現在加入我們，一起投資這項偉大的事業，共同開創人類歷史的新紀元！", "audio": "docs/data/audios/2506.07907v1.wav"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:25:34.862297", "title_zh": "超級編碼網路：用於影片理解的多模態編碼器之遞迴關聯", "summary_zh": "本研究提出一個名為「超級編碼網路」(SEN) 的創新架構，旨在提升影片理解能力。SEN透過遞迴關聯多模態編碼器，更深入地融合不同資訊來源，克服了傳統方法僅透過對比學習對齊編碼器的局限性。SEN將預訓練編碼器視為「超級神經元」，利用遞迴關聯區塊(RA)逐步融合多模態資訊，有效編碼更深層次的多模態互動。實驗結果顯示，SEN在追蹤、辨識、聊天和編輯等四個代表性影片任務上均有顯著提升，例如在像素級追蹤上，平均Jaccard指數提高了2.7%，時間一致性(TC)下降了8.8%。", "applications": ["智慧監控：透過分析監視器畫面中的人物動作和場景變化，自動識別異常行為，例如跌倒、打架等，並及時發出警報。", "自動駕駛：結合車載鏡頭和感測器數據，更精確地理解周圍環境，例如識別行人意圖、預測其他車輛的行駛軌跡，從而提升駕駛安全性。", "影音娛樂：讓影片編輯軟體更聰明，例如自動為影片添加字幕、根據劇情生成精彩片段、甚至根據用戶的文字描述修改影片內容。"], "pitch": "各位投資人，我們正在打造影片理解的未來！想像一下，讓AI像人一樣理解影片內容，這將釋放巨大的商業潛力。我們的「超級編碼網路」技術，就像為AI裝上了一雙更敏銳的眼睛和一個更聰明的大腦，它能更深入地理解影片中的每一個細節，從而實現更精準的分析和更智能的應用。這不僅僅是一項技術，更是一個平台，一個可以賦能各行各業的基礎設施。我們預計，未來在智慧安防、自動駕駛、影音娛樂等領域，都將出現基於我們技術的殺手級應用。現在加入我們，一起引領AI影片理解的浪潮，共同分享這千億美元的市場盛宴！", "audio": "docs/data/audios/2506.07576v1.wav"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:26:56.379104", "title_zh": "基於稀疏注意力Transformer與擴散模型的有效地震數據插值", "summary_zh": "本研究提出一種名為Diff-spaformer的新型深度學習框架，用於解決地震數據插值問題。該模型結合了Transformer架構和擴散模型，利用地震先驗提取網絡(SPEN)作為橋梁。通過全層稀疏多頭注意力和前饋傳播捕捉全局信息分佈，並利用擴散模型提供穩健的先驗指導。模型採用負平方歐幾里德距離計算稀疏親和力矩陣，更適合地震數據建模。實驗結果表明，該模型在隨機和連續缺失數據的插值保真度和計算效率方面均有所提升，為複雜地質條件下的高效地震數據重建提供了一種新範例。", "applications": ["石油探勘公司可以利用此技術更精準地重建地震數據，降低勘探成本，並提高油井鑽探的成功率。", "地質災害預測單位可以利用此技術更有效地分析地震波數據，提升地震預警的準確性，減少人員傷亡。", "工程建設公司在進行隧道或橋樑建設時，可以利用此技術更準確地評估地質結構，確保工程安全。"], "pitch": "各位投資人，想像一下，我們能更精準地看穿地底下的秘密！Diff-spaformer技術，就像是為地球做了一次高解析度的CT掃描。它不僅大幅提升地震數據的分析效率，更降低了勘探成本，為能源產業帶來革命性的突破。試想，如果我們能更精準地找到石油、天然氣，甚至地熱能源，這將為全球能源結構帶來多大的改變？此外，這項技術在防災減災領域也潛力無窮，能更準確地預測地震和滑坡等地質災害，保護人民生命財產安全。我們相信，Diff-spaformer將開啟一個全新的地質勘探與防災時代，成為下一個引領科技浪潮的獨角獸！現在加入我們，一起挖掘地球的寶藏，創造無限可能！", "audio": "docs/data/audios/2506.07923v1.wav"}
{"query": "AI", "id": "2506.07906v1", "url": "http://arxiv.org/abs/2506.07906v1", "title": "First observation of quantum correlations in $e^+e^-\\to XD\\bar{D}$ and $C$-even constrained $D\\bar{D}$ pairs", "summary": "The study of meson pairs produced with quantum correlations gives direct\naccess to parameters that are challenging to measure in other systems. In this\nLetter, the existence of quantum correlations due to charge-conjugation\nsymmetry $C$ are demonstrated in $D\\bar{D}$ pairs produced through the\nprocesses $e^+e^-\\to D\\bar{D}$, $e^+e^- \\to D^{*}\\bar{D}$, and $e^+e^- \\to\nD^{*} \\bar{D}^*$, where the lack of charge superscripts refers to an admixture\nof neutral-charm-meson particle and antiparticle states, using $7.13 \\text{\nfb}^{-1}$ of $e^+e^-$ collision data collected by the BESIII experiment between\ncenter-of-mass energies of $4.13-4.23 \\text{ GeV}$. Processes with either\n$C$-even or $C$-odd constraints are identified and separated. A procedure is\npresented that harnesses the entangled production process to enable\nmeasurements of $D^0$-meson hadronic parameters. This study provides the first\nconfirmation of quantum correlations in $e^+e^-\\to X D\\bar{D}$ processes and\nthe first observation of a $C$-even constrained $D\\bar{D}$ system. The\nprocedure is applied to measure $\\delta^{D}_{K\\pi}$, the strong phase between\nthe $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decay amplitudes, which\nresults in the determination of $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$. The potential for measurements of other\nhadronic decay parameters and charm mixing with these and future datasets is\nalso discussed.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:13:24.015493", "title_zh": "首次觀測到 $e^+e^-\\\\\\to XD\\bar{D}$ 和 $C$-宇稱約束 $D\\bar{D}$ 對中的量子關聯", "summary_zh": "本研究利用北京譜儀III（BESIII）實驗收集的正負電子碰撞數據，首次證實在$e^+e^-\\\\to XD\\bar{D}$過程中存在量子關聯，並首次觀測到$C$-宇稱約束的$D\\bar{D}$系統。透過分析$D\\bar{D}$對的產生過程，我們得以研究介子對的量子關聯，進而測量其他系統難以測量的參數。我們還提出了一種利用量子糾纏的產生過程來測量$D^0$介子強子參數的方法，並成功測量了$D^0\\to K^-\\pi^+$和$\\bar{D}^0\\to K^-\\pi^+$衰變振幅之間的強相位差$\\delta^{D}_{K\\pi}$。此研究為未來測量其他強子衰變參數和魅力混合提供了可能性。", "applications": ["【醫療影像增強】想像一下，醫生在看X光片或核磁共振時，影像不夠清晰，難以判斷細微病灶。利用量子關聯技術，我們可以提升影像的解析度，讓醫生能更精準地診斷疾病，及早發現癌症等問題。", "【超安全通訊】現在的網路安全越來越重要，但還是有很多漏洞。如果用量子關聯來加密訊息，就像用一把只有你和接收者才知道的鑰匙，而且一旦有人試圖偷看，鑰匙就會自動銷毀，確保資訊安全。", "【更精準的導航】現在的GPS有時候會不準，尤其是在高樓林立的城市裡。利用量子關聯，我們可以打造更精準的導航系統，無論是自動駕駛還是無人機送貨，都能更安全可靠。"], "pitch": "各位投資人，我們發現了量子力學中一個令人振奮的現象，並將其應用於前所未有的精確測量和資訊技術。想像一下，一個能以前所未有的精度分析亞原子粒子的世界，這不僅僅是科學突破，更是通往新技術的鑰匙。我們已經證明了在介子對中存在量子關聯，這為我們打開了一扇通往高精度測量的大門，在材料科學、醫學成像和安全通訊領域具有顛覆性的潛力。我們的技術能夠實現更安全、無法破解的通訊系統，在網路安全至關重要的時代，這是一項極具價值的資產。此外，它還能推動醫學診斷的發展，實現更早、更準確的疾病檢測。更重要的是，這項技術將會是量子計算發展的重要基石。我們團隊擁有一流的科學家和工程師，正處於這場變革的前沿。現在是加入我們，共同塑造量子技術的未來，並從這項突破性創新中獲得豐厚回報的絕佳時機。我們相信，這項技術的投資回報將遠遠超過您的預期，並將為您的投資組合增加巨大的價值。", "audio": "docs/data/audios/2506.07906v1.wav"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:14:49.230234", "title_zh": "用於H&E到IHC虛擬染色的跨通道感知學習", "summary_zh": "本研究提出一種新的跨通道感知學習（CCPL）策略，旨在解決現有H&E到IHC虛擬染色研究中忽略細胞核與細胞膜之間跨通道關聯性的問題。CCPL首先將HER2免疫組織化學染色分解為細胞核和細胞膜對應的蘇木精和DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，提取雙通道特徵並測量核膜間的相關性。同時，通過特徵蒸餾損失和光密度圖的統計分析，提升模型特徵提取能力並確保染色一致性。實驗結果表明，CCPL有效保留病理特徵，生成高質量虛擬染色圖像，為多媒體醫療數據的自動病理診斷提供強有力支持。", "applications": ["**個性化醫療診斷：** 想像一下，未來醫生可以根據你的H&E染色切片，快速生成各種IHC虛擬染色結果，精準預測你對不同藥物的反應，制定最適合你的治療方案，避免不必要的副作用。", "**遠程病理診斷：** 偏遠地區的醫院可能缺乏昂貴的IHC染色設備，但透過我們的技術，他們只需傳輸H&E染色切片，就能遠程生成IHC虛擬染色圖像，讓專家進行診斷，提升醫療資源的可及性。", "**病理教學與研究：** 醫學生和研究人員可以使用我們的技術，從現有的H&E染色切片中，快速生成大量不同IHC標記的虛擬染色圖像，用於學習、教學和研究，加速病理學的發展。"], "pitch": "各位投資人，我們正在開發一項革命性的病理診斷技術——基於跨通道感知學習的虛擬染色。想像一下，一個無需昂貴試劑和耗時流程，就能從普通H&E染色切片生成各種IHC染色的未來。這不僅能大幅降低病理診斷的成本和時間，還能加速新藥研發，實現個性化醫療。我們的技術基於最先進的AI模型，並已在實驗室中驗證其準確性和可靠性。我們預計，隨著數字病理學的普及，市場對虛擬染色的需求將呈指數級增長。現在加入我們，您將有機會參與一場醫療革命，共同打造一個更精準、更高效、更普惠的醫療未來！我們的目標是成為病理AI領域的領導者，引領下一代診斷技術的發展。", "audio": "docs/data/audios/2506.07559v1.wav"}
{"query": "AI", "id": "2506.07907v2", "url": "http://arxiv.org/abs/2506.07907v2", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between the decay\namplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T00:56:31.677881", "title_zh": "利用C-宇稱偶與奇的量子關聯D零反D零對，對D零->K負π正與反D零->K負π正衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的強相位差測量技術，利用能量高於ψ(3770)產生閾值的正負電子碰撞產生的量子關聯D零反D零對。此方法利用了電荷共軛對稱性的偶與奇本徵態下產生的D零反D零對。透過此技術，我們首次利用此類數據樣本確定了D零-反D零的相對強相位。使用北京譜儀III實驗在4.13-4.23 GeV質心能量下收集的7.13 fb-1積分光度數據集，測得D零->K負π正與反D零->K負π正衰變之間的強相位差δDKπ為(192.8+11.0+1.9−12.4−2.4)度。", "applications": ["想像一下，我們在機場安檢時，可以更精準地判斷行李中是否有違禁品。這項技術就像是更敏銳的X光，能分辨出極微小的物質差異，提高安檢效率。", "在醫療領域，醫生可以利用類似的原理，更準確地診斷癌症。透過分析癌細胞的微小變化，這項技術有助於早期發現並制定更有效的治療方案。", "在材料科學領域，工程師可以利用這項技術開發更堅固、更耐用的材料。透過精確控制材料的微觀結構，我們可以製造出性能更優越的產品，例如更輕便的汽車或更耐用的橋樑。"], "pitch": "各位創投先進，我們帶來的是一項革命性的技術，它將改寫我們對物質世界的認知和操控方式。這項技術的核心在於精準測量基本粒子間的強相位差，雖然聽起來很學術，但它的應用潛力卻是無可限量的。想像一下，我們可以打造出前所未有的精準感測器，應用於國防安全、醫療診斷、以及新材料開發等領域。更重要的是，這項技術是通往量子計算的鑰匙之一。透過精準控制量子態，我們有機會實現超越現有計算能力的量子電腦，進而引領下一次科技革命。現在投資，您將成為這場革命的先驅，共同開創一個充滿無限可能的未來！我們的團隊擁有頂尖的物理學家和工程師，我們已經完成了初步的實驗驗證，並準備好將這項技術商業化。我們需要您的資金支持，將實驗室成果轉化為實際產品，搶佔市場先機。這不僅是一項投資，更是一次參與改變世界的機會！", "audio": "docs/data/audios/2506.07907v2.wav"}
{"query": "Diffusion Model", "id": "2506.07902v1", "url": "http://arxiv.org/abs/2506.07902v1", "title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling", "summary": "Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.", "authors": ["Sifan Wang", "Zehao Dou", "Tong-Rui Liu", "Lu Lu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T00:57:53.110798", "title_zh": "FunDiff：基於函數空間的擴散模型，用於物理資訊生成建模", "summary_zh": "FunDiff 是一個創新的函數空間生成模型框架，它結合了潛在擴散過程和函數自編碼器架構。這個模型能處理不同離散化的輸入函數，生成可在任意位置評估的連續函數，並能無縫整合物理先驗知識。透過架構約束或基於物理資訊的損失函數，FunDiff 確保生成的樣本滿足基本的物理定律。理論分析表明，FunDiff 在函數空間中實現了最佳的密度估計收斂速度。在流體力學和固體力學等領域的實驗結果證明，FunDiff 能夠生成與目標分佈高度一致且符合物理規律的樣本，並且對雜訊和低解析度資料具有魯棒性。", "applications": ["天氣預報：利用 FunDiff 生成更精確、更穩定的天氣預測模型，能更準確預測颱風路徑、降雨量等，幫助人們提前做好防災準備。", "材料設計：在設計新材料時，FunDiff 可以預測材料在不同條件下的力學性能，加速新材料的開發，例如更輕、更堅固的汽車零件或更耐用的建築材料。", "醫療影像分析：FunDiff 可以生成更真實的醫療影像，幫助醫生進行疾病診斷和治療方案規劃，例如模擬手術過程或預測腫瘤生長情況。"], "pitch": "想像一下，如果我們能像設計積木一樣設計物理世界，會發生什麼？FunDiff 正是實現這一點的關鍵技術！它是一個基於擴散模型的函數空間生成框架，讓AI能夠理解並生成符合物理定律的複雜系統。這意味著，我們可以利用 FunDiff 加速新材料的發現，優化飛行器的設計，甚至預測氣候變化帶來的影響。更重要的是，FunDiff 具有極高的商業價值，它可以應用於各個領域，從航空航天到生物醫學，徹底改變產品設計和科學研究的方式。我們相信，FunDiff 將成為未來物理建模領域的基石，引領下一代工程和科學革命。現在投資 FunDiff，就是投資未來！", "audio": "docs/data/audios/2506.07902v1.wav"}
{"query": "AI", "id": "2506.09050v1", "url": "http://arxiv.org/abs/2506.09050v1", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:44:35.753035", "title_zh": "ALE-Bench：長時程目標導向演算法工程的基準測試", "summary_zh": "ALE-Bench是一個評估AI在解決複雜優化問題能力的新基準。它基於AtCoder Heuristic Contests的真實任務，這些任務涵蓋包裹遞送、排班、工廠生產計劃和電網平衡等領域。與傳統的短時編碼測試不同，ALE-Bench強調長時間迭代優化。評測顯示，雖然大型語言模型在特定問題上表現出色，但在跨問題一致性和長時程問題解決能力方面仍與人類存在差距。這個基準旨在推動AI在演算法工程領域的未來發展，為解決現實世界的複雜問題提供更強大的AI工具。", "applications": ["想像一下，物流公司可以利用這個技術，自動規劃最佳送貨路線，大幅降低油耗和時間成本，讓你的包裹更快送到。", "醫院可以用它來安排手術排程，確保資源有效分配，縮短病人等待時間，提升醫療服務品質。", "工廠可以運用這項技術，優化生產流程，減少浪費，提高產能，讓產品更快上市。"], "pitch": "各位投資人，我們正處於AI賦能各行各業的黃金時代！ALE-Bench不僅是一個基準測試，更是一個加速AI解決複雜問題的催化劑。想像一下，一個AI可以自動優化全球供應鏈，每年節省數十億美元；一個AI可以精準預測能源需求，實現更智能、更穩定的電網。我們的技術將引領下一代AI演算法的發展，解決傳統方法無法應付的難題。這是一個巨大的市場機會，我們正在尋找具有遠見的投資人，一起打造AI驅動的未來！我們預期，未來五年內，基於ALE-Bench的AI解決方案將滲透到物流、製造、能源、醫療等關鍵領域，創造巨大的經濟效益和社會價值。", "audio": "docs/data/audios/2506.09050v1.wav"}
{"query": "Foundation Model", "id": "2506.09042v1", "url": "http://arxiv.org/abs/2506.09042v1", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:46:06.060714", "title_zh": "宇宙駕駛夢：基於世界基礎模型的可擴展合成駕駛數據生成", "summary_zh": "自動駕駛系統的開發需要大量真實世界的數據，但收集和標註這些數據既耗時又昂貴，尤其是捕捉罕見的邊緣案例。為了解決這個問題，我們推出了Cosmos-Drive-Dreams，一個合成數據生成流程，旨在生成具有挑戰性的場景，以促進感知和駕駛策略訓練等下游任務。該流程的核心是Cosmos-Drive，一套基於NVIDIA Cosmos世界基礎模型專為駕駛領域設計的模型，能夠生成可控、高保真、多視角和時空一致的駕駛影片。實驗結果表明，我們生成的數據有助於緩解長尾分佈問題，並提高3D車道檢測、3D物體檢測和駕駛策略學習等下游任務的泛化能力。我們透過NVIDIA的Cosmos平台開源了我們的流程工具包、數據集和模型權重。", "applications": ["想像一下，未來你可以用手機APP客製化你的自動駕駛學習課程。你想在暴雨天開車？想練習在濃霧中停車？只要在APP上設定，系統就能生成對應的模擬場景，讓你安全地提升駕駛技能。", "遊戲開發者可以利用這項技術，快速生成各種逼真的駕駛場景，創造出更沉浸式的賽車遊戲或開放世界遊戲。不再需要花費大量時間和金錢去拍攝真實場景，就能擁有無限可能的遊戲世界。", "交通管理部門可以利用這項技術，模擬各種交通狀況，例如：施工路段、交通事故等，來測試和優化交通控制策略，從而提高道路安全和效率。"], "pitch": "各位創投先進，我們正在打造自動駕駛的「數據印鈔機」！Cosmos-Drive-Dreams不僅能大幅降低自動駕駛數據收集的成本，更能創造出真實世界難以企及的極端場景，例如：超高難度的冰雪路面、突發的動物穿越等等。這意味著，我們能加速自動駕駛技術的成熟，讓無人計程車、無人物流車更快落地。更重要的是，這項技術的應用範圍遠不止於自動駕駛。我們可以將它應用於虛擬實境、遊戲開發、甚至是城市規劃。想像一下，未來的建築師可以在虛擬環境中模擬各種交通流量，優化道路設計；電影導演可以輕鬆創造出史詩級的追逐場景。這是一個千億級別的市場，而我們擁有領先的技術和先發優勢，絕對是您不容錯過的投資機會！", "audio": "docs/data/audios/2506.09042v1.wav"}
{"query": "AI", "id": "2506.09049v1", "url": "http://arxiv.org/abs/2506.09049v1", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:18:50.061894", "title_zh": "VIKI-R：透過強化學習協調具體化多智能體合作", "summary_zh": "在動態環境中協調多個具體化智能體是人工智慧的核心挑戰。本研究提出VIKI-R框架，利用視覺語言模型(VLM)進行視覺推理，並透過強化學習協調多智能體合作。VIKI-R包含兩個階段：首先，使用Chain-of-Thought註解的示範數據微調預訓練的VLM；接著，在多層次獎勵訊號下進行強化學習。實驗證明，VIKI-R在所有任務層面都顯著優於基準方法，並能使異質智能體之間產生組合式的合作模式。VIKI-R和VIKI-Bench為具體化人工智慧系統中，多智能體視覺驅動合作提供了一個統一的測試平台和方法。", "applications": ["**智慧家庭管家團隊：** 想像一下，家裡不再只有一個掃地機器人，而是有一個團隊！VIKI-R能讓不同功能的機器人（掃地、擦地、整理）協同合作，更有效率地完成家務。例如，擦地機器人發現地上有髒污，會通知掃地機器人先來處理，再進行擦拭。", "**工廠協作機械手臂：** 在生產線上，不同功能的機械手臂不再各自為政，而是像一個團隊一樣協同工作。VIKI-R能讓它們根據視覺資訊判斷，分工合作組裝產品，提高生產效率和品質。例如，一個機械手臂負責定位零件，另一個負責鎖螺絲，兩者配合無間。", "**救災現場無人機協作：** 災難發生時，多架無人機可以透過VIKI-R協調合作，進行災情評估、搜索倖存者、運送物資等任務。例如，一架無人機負責高空偵察，另一架負責低空搜索，它們可以共享資訊，更快速地找到需要幫助的人。"], "pitch": "各位投資人，我們正站在AI協作的新浪潮之上！VIKI-R不僅僅是一個技術突破，它代表的是一個全新的協作模式。想像一下，未來世界，無數的機器人、無人機，甚至是軟體代理，都能像一個團隊一樣協同工作，解決複雜的問題。VIKI-R正是這個願景的基石。它能讓異質智能體之間產生前所未有的合作模式，大幅提升效率、降低成本。從智慧製造、智慧物流到智慧城市，VIKI-R的應用潛力無可限量。我們相信，VIKI-R將引領下一代AI發展，成為各行各業不可或缺的協作引擎。現在加入我們，共同打造AI協作的未來，贏得百億美元的市場！", "audio": "docs/data/audios/2506.09049v1.wav"}
{"query": "AI", "id": "2506.09002v1", "url": "http://arxiv.org/abs/2506.09002v1", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:15:09.759403", "title_zh": "透過混合程式分析與大型語言模型提升Rust單元測試覆蓋率", "summary_zh": "本研究提出PALM，一種結合程式分析與大型語言模型(LLM)的方法，旨在提升Rust程式的單元測試覆蓋率。PALM首先分析程式碼，找出分支條件並轉換為路徑約束。接著，利用這些約束條件與程式碼上下文資訊，設計提示詞引導LLM生成高效能的單元測試。實驗結果顯示，PALM能在短時間內顯著提升測試覆蓋率，某些情況下整體專案覆蓋率提升超過50%，平均覆蓋率達到75.77%，與人工測試相近。此方法成功整合程式分析與人工智慧，為自動化軟體測試開闢新方向。", "applications": ["想像一下，以後軟體更新再也不怕出包！PALM就像軟體的健康檢查醫生，自動幫忙找出潛在問題，讓你的手機App、電腦程式更穩定，再也不會用到一半突然當機。", "如果你是個遊戲玩家，一定很怕遇到Bug。有了PALM，遊戲開發者可以更快速地測試遊戲，確保遊戲體驗順暢，讓你玩得更開心，不用再忍受惱人的錯誤。", "現在很多家電都連上網路，像是智慧冰箱、智慧電視等等。PALM可以確保這些設備的軟體安全可靠，保護你的隱私，避免被駭客入侵。"], "pitch": "各位投資人，我們正在革新軟體測試的未來！PALM不僅僅是個工具，更是個能大幅降低軟體開發成本、提升軟體品質的革命性技術。想像一下，一個能自動生成高覆蓋率測試的系統，能讓軟體公司節省多少人力和時間？這意味著更快的產品上市速度、更低的維護成本、以及更高的客戶滿意度。隨著AI技術的進步，PALM的潛力是無限的！我們可以將它應用於各種程式語言、各種軟體平台，甚至可以客製化服務，滿足不同產業的需求。我們預計，在五年內，PALM將成為軟體測試領域的領導者，為投資者帶來豐厚的回報！現在加入我們，一起打造更可靠、更安全的軟體世界！", "audio": "docs/data/audios/2506.09002v1.wav"}
{"query": "Foundation Model", "id": "2506.08982v1", "url": "http://arxiv.org/abs/2506.08982v1", "title": "On Finetuning Tabular Foundation Models", "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:17:45.572972", "title_zh": "表格型基礎模型的微調研究", "summary_zh": "本研究探討如何針對表格型資料的基礎模型TabPFNv2進行最佳微調。TabPFNv2原本透過上下文學習，在小型數據集上表現優異，無需調整模型參數。然而，如何微調此類模型以適應特定數據集，以及微調如何改變其內部運作機制，仍是未解之謎。研究發現，完整微調在效率和效果上都是TabPFNv2的最佳選擇。微調後的模型，其測試樣本與訓練樣本之間的相似度計算更準確，進而提升了基於檢索的預測邏輯。實驗證明，微調後的TabPFNv2在包含5萬個物件的數據集上也能有效運作，並在學術數據集上取得領先成果。然而，在具有時間偏移和豐富特徵的數據集上，其穩定性較差。", "applications": ["**個人信用評估：** 銀行或貸款機構可以使用微調後的模型，根據你的交易紀錄、消費習慣等表格數據，更精準地評估你的信用風險，決定是否核准貸款或調整利率。", "**疾病診斷輔助：** 醫生可以輸入病患的各種檢驗數據（例如：血壓、血糖、膽固醇等），讓微調後的模型輔助判斷病患罹患特定疾病的風險，或推薦更精準的檢查項目。", "**產品推薦系統：** 電商平台可以利用微調後的模型，根據你的瀏覽紀錄、購買紀錄、人口統計資料等表格數據，更有效地推薦你可能感興趣的商品，提升銷售額。"], "pitch": "各位投資人，想像一下，未來AI不再需要海量資料訓練，而是像一位經驗豐富的專家，只需少量數據就能快速掌握新領域的知識！我們正在開發的技術，正是基於表格型基礎模型TabPFNv2的微調技術，讓AI在表格數據上實現真正的『即學即用』。這項技術的潛力無窮：在金融領域，它可以精準預測市場趨勢，抓住投資機會；在醫療領域，它可以加速新藥研發，改善診斷效率；在零售領域，它可以優化供應鏈管理，提升客戶滿意度。更重要的是，我們的技術降低了AI應用的門檻，讓中小企業也能輕鬆享受AI帶來的紅利。我們相信，這項技術將引領下一波AI革命，成為各行各業不可或缺的工具。現在加入我們，一起打造AI驅動的未來！", "audio": "docs/data/audios/2506.08982v1.wav"}
{"query": "Diffusion Model", "id": "2506.08809v1", "url": "http://arxiv.org/abs/2506.08809v1", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:19:05.918184", "title_zh": "HiSin：透過解析度導向的漸進式推論實現高效能高解析度正弦圖修復", "summary_zh": "在電腦斷層掃描中，高解析度正弦圖修復至關重要。遺失高頻投影可能導致明顯的偽影和診斷錯誤。我們提出HiSin，一種基於擴散模型的新框架，透過解析度導向的漸進式推論，實現高效正弦圖修復。HiSin在低解析度下逐步提取全局結構，並將高解析度推論延遲到小塊區域，從而實現記憶體高效的修復。同時，它還結合了頻率感知的塊跳過和結構自適應的步驟分配，以減少冗餘計算。實驗結果表明，HiSin降低了高達31.25%的峰值記憶體使用量，並減少了18.15%的推論時間，同時保持了跨數據集、解析度和遮罩條件下的修復準確性。", "applications": ["想像一下，機場安檢時，X光掃描儀因為某個角度被遮擋，導致行李圖像出現盲點。HiSin技術可以自動修復這些盲點，讓安檢人員更清楚地看到行李內部的物品，提高安全性。", "牙醫在進行X光檢查時，如果影像受到干擾或部分缺失，HiSin技術可以填補這些缺失，幫助牙醫更準確地診斷蛀牙或牙齒問題，減少誤判。", "古文物修復人員在使用X光掃描古代陶器或青銅器時，如果掃描數據不完整，HiSin技術可以重建出更完整的文物圖像，幫助他們更好地研究和保護這些珍貴的文化遺產。"], "pitch": "各位投資人，我們帶來的是HiSin技術，它將徹底改變醫學影像、安檢和文物保護等領域。現有的電腦斷層掃描技術在高解析度圖像處理上存在記憶體和計算瓶頸，導致成本高昂且效率低下。HiSin透過創新的解析度導向漸進式推論，大幅降低了資源消耗，同時保持甚至提升了圖像修復的準確性。這意味著更快的掃描速度、更低的硬體成本，以及更精確的診斷結果。想像一下，未來搭載HiSin技術的移動式醫療設備，可以深入偏遠地區，提供即時的、高品質的醫療影像服務。在安檢領域，更清晰的X光圖像將有效提升安全防護能力。在文物保護方面，HiSin將幫助我們解鎖更多歷史的秘密。我們相信，HiSin技術具有巨大的市場潛力，將為投資者帶來豐厚的回報。現在投資HiSin，就是投資未來！", "audio": "docs/data/audios/2506.08809v1.wav"}
{"query": "AI", "id": "2506.08998v1", "url": "http://arxiv.org/abs/2506.08998v1", "title": "On Monotonicity in AI Alignment", "summary": "Comparison-based preference learning has become central to the alignment of\nAI models with human preferences. However, these methods may behave\ncounterintuitively. After empirically observing that, when accounting for a\npreference for response $y$ over $z$, the model may actually decrease the\nprobability (and reward) of generating $y$ (an observation also made by\nothers), this paper investigates the root causes of (non) monotonicity, for a\ngeneral comparison-based preference learning framework that subsumes Direct\nPreference Optimization (DPO), Generalized Preference Optimization (GPO) and\nGeneralized Bradley-Terry (GBT). Under mild assumptions, we prove that such\nmethods still satisfy what we call local pairwise monotonicity. We also provide\na bouquet of formalizations of monotonicity, and identify sufficient conditions\nfor their guarantee, thereby providing a toolbox to evaluate how prone learning\nmodels are to monotonicity violations. These results clarify the limitations of\ncurrent methods and provide guidance for developing more trustworthy preference\nlearning algorithms.", "authors": ["Gilles Bareilles", "Julien Fageot", "Lê-Nguyên Hoang", "Peva Blanchard", "Wassim Bouaziz", "Sébastien Rouault", "El-Mahdi El-Mhamdi"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:23:55.474614", "title_zh": "論AI對齊中的單調性", "summary_zh": "基於比較的偏好學習已成為AI模型與人類偏好對齊的核心。然而，這些方法可能出現違反直覺的行為。本文探討了在考慮對回應y優於z的偏好時，模型實際上可能會降低生成y的機率（以及獎勵）的根本原因。研究針對包含直接偏好優化(DPO)、廣義偏好優化(GPO)和廣義Bradley-Terry(GBT)的通用比較偏好學習框架，分析(非)單調性的成因。在溫和的假設下，證明了這些方法仍然滿足局部成對單調性。並提供了一系列單調性的形式化定義，並確定了保證它們的充分條件，從而提供了一個工具箱來評估學習模型違反單調性的可能性。這些結果闡明了當前方法的局限性，並為開發更值得信賴的偏好學習算法提供了指導。", "applications": ["個人化推薦系統：確保推薦的商品或服務隨著使用者表達更多偏好而變得更好，不會出現使用者表示喜歡A後，系統反而推薦更差的B。", "自動駕駛系統：確保自動駕駛在學習人類駕駛習慣後，不會出現判斷邏輯混亂，例如學會禮讓行人後，反而更容易發生交通事故。", "醫療診斷輔助系統：確保AI在學習醫生的診斷偏好後，不會出現診斷結果不一致或矛盾的情況，例如學會某種疾病的特徵後，反而忽略了該疾病的早期症狀。"], "pitch": "各位創投先進，我們正在解決AI領域一個根本性的問題：如何確保AI的行為符合人類的直覺和期望。我們的研究揭示了現有AI偏好學習方法中存在的「單調性」問題，也就是說，AI有時候會學反了！想像一下，如果你的AI助理在你說喜歡某個餐廳後，反而不再推薦類似的餐廳，這是不是很荒謬？我們的技術能夠保證AI學習過程的單調性，確保AI始終朝着符合人類偏好的方向發展。這不僅能提升AI的可靠性和安全性，更能大幅改善使用者體驗，應用範圍極其廣泛，從個人化推薦、自動駕駛到醫療診斷，都有巨大的市場潛力。我們相信，隨著AI越來越普及，對AI行為可控性的需求也會越來越高，我們的技術將成為AI時代的基礎設施，具有極高的投資價值。現在投資我們，就是投資AI的未來！我們預期在三年內，我們的技術能被廣泛應用於各個領域，並成為AI偏好學習的行業標準，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.08998v1.wav"}
{"query": "Foundation Model", "id": "2506.08955v1", "url": "http://arxiv.org/abs/2506.08955v1", "title": "Segment Concealed Objects with Incomplete Supervision", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:25:14.129810", "title_zh": "利用不完整監督分割隱藏物體", "summary_zh": "本研究提出一個針對不完整監督隱藏物體分割(ISCOS)的統一方法，旨在解決在不完整標註數據下分割與環境無縫融合的物體。此方法利用視覺基礎模型「Segment Anything Model (SAM)」產生偽標籤，並透過一系列策略來生成、儲存和監督這些偽標籤，以確保穩健的網路訓練。此外，設計了一種混合粒度特徵分組模塊，通過對不同粒度的特徵進行分組和聚合，促進分割連貫性，從而更完整地分割單個和多個物體。實驗結果表明，該方法在多個ISCOS任務中實現了最先進的性能，並且可以作為一個即插即用的解決方案，增強現有模型的性能。", "applications": ["尋找失物：想像一下，你掉了鑰匙或錢包，但它們完美地融入了地毯或沙發中。這項技術可以幫助你快速找到它們，即使它們被巧妙地隱藏起來。", "醫療影像分析：醫生可以利用這項技術更準確地識別X光片或MRI中的微小病灶或異常，即使它們與周圍組織非常相似。", "自動駕駛安全：在惡劣天氣或光線條件下，車輛可以利用這項技術更可靠地檢測到行人、交通標誌或其他障礙物，即使它們被部分遮擋或偽裝。"], "pitch": "各位投資人，我們正處於一個AI視覺革命的風口浪尖！我們的技術，基於不完整監督的隱藏物體分割，不僅解決了業界一大難題，更開啟了無限的商業可能性。試想一下，無人商店的防盜系統，能夠精準識別被遮蔽的商品；國防安全領域，能有效偵測偽裝目標；智慧農業上，可辨識隱藏在作物中的病蟲害。這項技術的核心優勢在於其高適應性和低成本，無需大量人工標註數據，即可實現高精度分割。我們預計，未來五年內，隱藏物體分割技術的市場規模將達到數十億美元。現在加入我們，您將成為這場視覺革命的領航者，共同創造巨大的商業價值！", "audio": "docs/data/audios/2506.08955v1.wav"}
{"query": "Diffusion Model", "id": "2506.08796v1", "url": "http://arxiv.org/abs/2506.08796v1", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:26:24.795768", "title_zh": "流動多樣且高效：透過隨機速度場採樣學習動量流匹配", "summary_zh": "這項研究提出了一種新的修正流模型Discretized-RF，它將傳統修正流中的直線路徑離散化為一系列可變速度場的子路徑，藉此擴展採樣空間，尤其是在接近雜訊分佈時。與直接在資料上疊加雜訊不同，Discretized-RF在子路徑的速度上引入雜訊，改變其方向，從而提升生成結果的多樣性和多尺度雜訊建模能力。實驗證明，透過採樣隨機速度場學習動量流匹配，能夠產生多樣且高效的軌跡，並持續生成高品質且多樣化的結果。簡單來說，就是讓AI生成圖像更豐富、更真實，而且速度更快。", "applications": ["線上遊戲：快速生成多樣化的遊戲角色、場景和道具，降低開發成本，提升遊戲體驗。", "電影特效：生成逼真的特效畫面，例如爆炸、天氣變化等，縮短製作時間，降低製作門檻。", "藝術創作：藝術家可以利用這項技術快速生成各種風格的藝術作品，探索新的創作可能性。"], "pitch": "各位投資人，我們團隊帶來的是一項突破性的AI圖像生成技術，Discretized-RF。想像一下，一個能以閃電般的速度，創造出前所未有、栩栩如生的圖像的世界。傳統AI圖像生成技術的瓶頸在於生成速度慢、多樣性不足，而Discretized-RF完美解決了這些問題。它就像一位天賦異稟的藝術家，能根據您的需求，快速生成各種風格、各種主題的圖像，而且效果驚艷。這項技術的潛力無可限量，從遊戲開發、電影製作到廣告設計、教育娛樂，甚至醫療診斷，都將產生革命性的影響。我們預計，未來五年內，AI圖像生成市場將達到數百億美元的規模，而Discretized-RF將在這個市場中佔據領導地位，為各位帶來豐厚的回報。現在加入我們，共同開創AI圖像生成的新時代！", "audio": "docs/data/audios/2506.08796v1.wav"}
{"query": "AI", "id": "2506.08962v1", "url": "http://arxiv.org/abs/2506.08962v1", "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:14:30.063849", "title_zh": "研發中：大型語言模型強化的智慧導師系統，用於大學電路分析", "summary_zh": "本研究展示一個AI驅動的智慧導師系統，專為大學電路分析課程的學生提供作業評估與回饋。此系統運用大型語言模型，精心設計提示語，以優化對各種問題的回應。目前已部署於Microsoft Azure平台，並在美國東南部一所大型公立研究型大學的電機與電腦工程學院的電路分析課程中使用。智慧導師不僅提供個人化指導和回饋，還收集學生互動數據，並將其匯總後與課程講師共享，幫助教師即時了解學生的困難點，從而進行更有針對性的課堂教學。初步學生回饋顯示，90.9%的學生對此導師感到滿意。未來，我們將釋出完整分析報告，並探索此智慧導師在更廣泛工程領域的應用。", "applications": ["1. 小明在準備電路學期中考，遇到一個複雜的電路分析題目卡住了。他可以使用這個智慧導師，一步一步引導他解題，就像一位耐心的家教一樣，而且隨時隨地都能使用。", "2. 小華是一位電路學老師，他可以利用這個智慧導師收集到的學生提問數據，了解學生最常遇到的困難，並在課堂上更有針對性地講解這些概念，提升教學效率。", "3. 一家電子公司的新進工程師小李，在設計電路時遇到了一些技術問題。他可以利用這個智慧導師快速查找相關資料和解決方案，提升工作效率，加速產品開發。"], "pitch": "各位投資人，想像一下，未來的教育不再是單向的知識灌輸，而是個人化的學習體驗。我們開發的智慧導師系統，正是這場教育革命的先鋒！它不僅能提供客製化的電路分析教學，還能收集學生的學習數據，為教師提供即時回饋，真正實現了『因材施教』。更重要的是，這個系統可以擴展到其他工程領域，甚至整個 STEM 教育領域！我們預計，未來五年內，智慧導師系統將成為大學、職業學校，甚至是企業培訓的標配。這不僅是一個教育產品，更是一個數據驅動的教育平台，擁有巨大的商業潛力。現在加入我們，一起打造智慧教育的未來！", "audio": "docs/data/audios/2506.08962v1.wav"}
{"query": "Foundation Model", "id": "2506.08949v1", "url": "http://arxiv.org/abs/2506.08949v1", "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:16:08.821228", "title_zh": "SSS：基於高效提示的半監督SAM-2醫學影像分割", "summary_zh": "在醫學影像領域，如何有效利用大量未標記數據，同時減少對高品質像素級標註的依賴，是一大挑戰。本研究提出名為SSS的半監督學習方法，它基於Vision Foundation Models (如SAM-2) 的強大特徵提取能力，挖掘未標記醫學影像中的潛在知識，有效提升全監督醫學影像分割的性能。透過“弱到強”的一致性正規化框架，引入判別式特徵增強機制，探索不同數據增強策略產生的特徵差異，並結合物理約束與滑動窗口機制生成輸入提示，最終在ACDC和BHSD等多標籤數據集上驗證了該方法的優越性，在BHSD數據集上，SSS的Dice平均得分比現有最佳方法高出+3.65。", "applications": ["【更精準的癌症篩檢】想像一下，未來醫院的電腦能自動分析X光片、斷層掃描等影像，找出潛在的微小腫瘤，讓醫生能及早發現並治療癌症，大幅提高患者的存活率。", "【遠距醫療的福音】偏鄉地區醫療資源匱乏，透過這項技術，即使沒有經驗豐富的放射科醫師，也能利用AI輔助診斷，提供更快速、準確的醫療判斷，減少誤診和延誤治療的風險。", "【個人化的健康管理】結合穿戴式裝置的數據，AI能分析個人的生理影像，預測疾病風險，提供個人化的健康建議，例如：提醒你注意心臟健康、預防骨質疏鬆等。"], "pitch": "各位投資人，醫療影像AI的未來，掌握在我們手中！SSS不僅僅是一項技術，它代表著更精準、更普及、更個人化的醫療服務。試想，全球醫療影像市場規模龐大，而我們的SSS技術，能大幅降低醫療影像分析的成本，提高效率，讓更多人能負擔得起高品質的醫療服務。這意味著巨大的商業潛力！我們可以將SSS整合到現有的醫療設備中，授權給醫院和診所使用，甚至可以開發面向消費者的個人健康管理App。更進一步，隨著AI技術的不斷發展，SSS有望實現全自動化的影像診斷，解放醫療人員的勞動力，讓他們能更專注於患者的照護。現在投資SSS，就是投資醫療的未來！讓我們一起打造一個更健康、更美好的世界！", "audio": "docs/data/audios/2506.08949v1.wav"}
{"query": "Diffusion Model", "id": "2506.08677v1", "url": "http://arxiv.org/abs/2506.08677v1", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:17:21.611749", "title_zh": "MAMBO：用於乳房X光影像的高解析度生成方法", "summary_zh": "乳房X光檢查是乳癌檢測的金標準。本研究提出MAMBO模型，一種基於擴散模型的新穎方法，能生成高解析度的乳房X光影像。MAMBO整合了局部和全域的上下文資訊，生成逼真度極高的乳房X光片，最高可達3840x3840像素。這項技術能克服AI訓練中數據集不足的難題，可用於增強分類模型的訓練，並擴展到異常檢測。實驗證明，MAMBO在影像生成、超解析度和異常檢測方面具有優異的能力，有潛力提升乳房X光分析的準確性，並更早地檢測到病灶。", "applications": ["想像一下，以後在偏鄉地區，醫生可以透過AI生成的乳房X光影像進行初步篩檢，即使沒有足夠的真實案例，也能提供更及時的醫療服務。", "AI生成的乳房X光影像可以幫助醫學院的學生練習判讀，讓他們在接觸真實病人之前，就能累積更多經驗，提升診斷能力。", "未來，我們可以利用這項技術開發個人化的乳房X光風險評估系統，根據AI生成的不同情境，更精準地預測個人罹患乳癌的風險。"], "pitch": "各位投資人，我們正站在醫療AI革命的浪潮之上！乳癌是女性健康的頭號殺手，而早期發現是關鍵。MAMBO技術突破了數據限制，能生成高解析度的乳房X光影像，這不僅能提升現有AI輔助診斷的準確性，更開創了全新的商業模式。試想一下，我們可以將這項技術授權給醫院、診所，甚至開發遠程醫療平台，讓更多女性受益。此外，我們還能與藥廠合作，利用AI生成的數據加速新藥研發。更令人興奮的是，MAMBO的底層技術具有高度通用性，未來可應用於其他醫學影像領域，例如肺部X光、CT掃描等。這是一個千載難逢的投資機會，讓我們一起打造更健康、更智慧的未來！", "audio": "docs/data/audios/2506.08677v1.wav"}
{"query": "AI", "id": "2506.08945v1", "url": "http://arxiv.org/abs/2506.08945v1", "title": "Who is using AI to code? Global diffusion and impact of generative AI", "summary": "Generative coding tools promise big productivity gains, but uneven uptake\ncould widen skill and income gaps. We train a neural classifier to spot\nAI-generated Python functions in 80 million GitHub commits (2018-2024) by\n200,000 developers and track how fast--and where--these tools take hold. By\nDecember 2024, AI wrote an estimated 30.1% of Python functions from U.S.\ncontributors, versus 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4%\nin Russia and 11.7% in China. Newer GitHub users use AI more than veterans,\nwhile male and female developers adopt at similar rates. Within-developer\nfixed-effects models show that moving to 30% AI use raises quarterly commits by\n2.4%. Coupling this effect with occupational task and wage data puts the annual\nvalue of AI-assisted coding in the United States at $9.6-$14.4 billion, rising\nto $64-$96 billion if we assume higher estimates of productivity effects\nreported by randomized control trials. Moreover, generative AI prompts learning\nand innovation, leading to increases in the number of new libraries and library\ncombinations that programmers use. In short, AI usage is already widespread but\nhighly uneven, and the intensity of use, not only access, drives measurable\ngains in output and exploration.", "authors": ["Simone Daniotti", "Johannes Wachs", "Xiangnan Feng", "Frank Neffke"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:18:05.301258", "title_zh": "誰在使用AI編碼？生成式AI的全球擴散與影響", "summary_zh": "這項研究分析了GitHub上8000萬個提交，追蹤生成式AI在程式碼編寫中的應用。截至2024年底，美國開發者使用AI編寫了約30.1%的Python函數，領先於德國、法國、印度、俄羅斯和中國。研究發現，新用戶比老手更常使用AI，且男女開發者採用率相似。AI使用率達到30%可使季度提交量增加2.4%。美國AI輔助編碼的年價值估計為96億至144億美元，若採用更高生產力估計，可達640億至960億美元。此外，AI還促進了學習與創新，增加了程式設計師使用的新函式庫和組合。總之，AI的使用已相當普遍，但高度不均，且使用強度而非存取權限，才是產出和探索的關鍵。", "applications": ["**加速App開發：** 想像一下，你想要開發一個新的手機App，有了AI編碼工具，可以大幅縮短開發時間，更快將你的創意變成現實，搶佔市場先機。", "**客製化網站架設：** 如果你需要一個專屬的個人或公司網站，但又不懂程式碼，AI編碼工具可以根據你的需求自動生成網站程式碼，讓你可以輕鬆擁有專業級的網站。", "**程式碼錯誤偵錯：** 對於程式設計師來說，最頭痛的就是找bug。AI編碼工具可以快速掃描程式碼，找出潛在的錯誤，節省大量的除錯時間，提升工作效率。"], "pitch": "各位創投夥伴，我們正在見證一場程式碼編寫的革命！這項研究證明，生成式AI不僅已經廣泛應用於程式開發，更帶來了顯著的生產力提升。想像一下，一個程式設計師團隊，在AI的輔助下，可以完成過去數倍的工作量，這意味著更快的產品迭代、更低的開發成本，以及更強大的市場競爭力。我們預計，未來五年內，AI編碼工具將成為所有軟體開發團隊的標配，就像現在的雲端服務一樣。我們正在打造下一代的AI編碼平台，結合最先進的AI技術和人性化的使用者介面，目標是成為這個新興市場的領導者。現在加入我們，共同開創AI編碼的黃金時代！我們相信，這項技術將會徹底改變整個軟體產業，帶來巨大的投資回報。", "audio": "docs/data/audios/2506.08945v1.wav"}
{"query": "Foundation Model", "id": "2506.08936v1", "url": "http://arxiv.org/abs/2506.08936v1", "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead.", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:19:30.928461", "title_zh": "BioLangFusion：DNA、mRNA與蛋白質語言模型的多模態融合", "summary_zh": "BioLangFusion 是一種整合 DNA、mRNA 和蛋白質語言模型的簡單方法，能產生統一的分子表示。它基於分子生物學的中心法則，將不同模態的嵌入向量在具有生物意義的密碼子層級對齊，確保跨模態的直接對應。BioLangFusion 研究了三種融合技術：密碼子層級的嵌入向量串聯、基於多實例學習的熵正則化注意力池化，以及跨模態多頭注意力。這些方法無需額外的預訓練或修改基礎模型，即可輕鬆整合現有的基於序列的基礎模型。在五項分子特性預測任務中，BioLangFusion 優於強大的單模態基準模型，表明即使是預訓練模型的簡單融合，也能以最小的開銷捕獲互補的多重組學信息。", "applications": ["個性化醫療：根據你的基因、mRNA 和蛋白質數據，預測你對特定藥物的反應，從而選擇最適合你的治療方案，避免不必要的副作用。", "疾病早期診斷：通過分析血液或體液中的 DNA、mRNA 和蛋白質的變化，在疾病發展的早期階段就能夠檢測到，及早進行干預。", "新藥開發：加速新藥的篩選和設計過程。通過模擬不同的分子組合，預測它們的藥效和安全性，從而節省大量的實驗時間和成本。"], "pitch": "各位創投先進，我們正在開發 BioLangFusion，這是一項革命性的技術，它能將 DNA、mRNA 和蛋白質數據融合，以前所未有的精度預測生物分子的特性。想像一下，我們能夠像解讀程式碼一樣解讀生命密碼，加速新藥開發、實現精準醫療、甚至預測疾病的發生。這不僅僅是一項技術，而是一個巨大的市場機會。目前藥物開發的成功率極低，耗時長且成本高昂。BioLangFusion 能大幅降低藥物開發的風險和成本，加速新藥上市。更重要的是，我們正在構建一個平台，讓研究人員、藥廠和醫療機構都能夠利用我們的技術，共同推動生命科學的發展。未來的醫療將是預測性的、個性化的，而 BioLangFusion 將是實現這一願景的關鍵。我們相信，BioLangFusion 有潜力成为生物科技领域的下一个独角兽，为投资者带来丰厚的回报。現在加入我們，一起改變世界！", "audio": "docs/data/audios/2506.08936v1.wav"}
{"query": "Diffusion Model", "id": "2506.08632v1", "url": "http://arxiv.org/abs/2506.08632v1", "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:20:52.051991", "title_zh": "RoboSwap：一個基於GAN驅動的影片擴散框架，用於無監督機器手臂替換", "summary_zh": "RoboSwap是一個創新的影片編輯框架，旨在解決機器人學習中跨平台泛化的數據稀缺問題。它結合了GAN和擴散模型，無需配對的影片數據，即可將一個影片中的機器手臂替換成另一個。首先，系統會分割出手臂，利用GAN模型進行手臂轉換，然後將轉換後的手臂融合回原始影片背景，並使用擴散模型來增強連貫性、運動真實感和物體互動。實驗證明，RoboSwap在結構連貫性和運動一致性方面優於現有技術，為機器人學習生成可靠的跨平台數據提供了一個強大的解決方案。這將大幅降低機器人學習的數據收集成本，加速相關技術的發展。", "applications": ["遠端醫療手術訓練：醫生可以利用RoboSwap生成不同類型機器手臂在手術中的模擬影片，進行更全面的訓練，無需實際操作各種昂貴的設備。", "工廠自動化流程設計：工程師可以快速模擬不同機器手臂在生產線上的工作情況，優化流程設計，提高生產效率，而無需耗時地進行物理原型測試。", "虛擬實境遊戲開發：遊戲開發者可以使用RoboSwap輕鬆創建各種機器人角色，並模擬它們的動作，豐富遊戲內容，提升玩家的沉浸式體驗。"], "pitch": "各位投資人，想像一下，未來機器人無所不在，從工廠到醫院，從家庭到太空。但要讓它們真正聰明，需要大量的訓練數據。RoboSwap的出現，徹底改變了機器人學習的遊戲規則！它就像機器人界的Photoshop，能以極低的成本，生成各種機器手臂的影片數據，讓AI快速學習。這意味著，我們不再需要花費巨額資金和時間去收集真實數據，就能訓練出更強大的機器人。這項技術的潛在市場規模極其龐大，涵蓋工業自動化、醫療機器人、虛擬實境等多個領域。我們相信，RoboSwap將成為機器人時代的關鍵基礎設施，而現在就是投資這個潛力獨角獸的最佳時機！", "audio": "docs/data/audios/2506.08632v1.wav"}
{"query": "AI", "id": "2506.08935v1", "url": "http://arxiv.org/abs/2506.08935v1", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "authors": ["Andrew Shin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:12:34.399500", "title_zh": "電競玩家能訓練出數學推理模型嗎？", "summary_zh": "本研究證明，即使使用一張普通的電競級GPU，也能訓練出強大的數學推理模型。透過結合強化學習和記憶體優化技術，我們在僅有16GB記憶體的RTX 3080 Ti上，訓練出一個15億參數的模型，其數學推理能力甚至超越了許多更大的模型。這項成果挑戰了「頂尖數學推理需要龐大運算資源」的傳統觀念，有助於普及高效能AI研究，讓更多人能參與其中，降低AI開發門檻。", "applications": ["輔助學生學習：學生在家也能利用一般電腦，執行AI數學解題模型，獲得更個人化的學習輔導。", "金融風險評估：小型金融機構或個人投資者，能用更低成本的AI模型進行風險分析，做出更明智的決策。", "科學研究加速：研究人員在資源有限的情況下，也能快速訓練AI模型，加速科學發現的過程。"], "pitch": "想像一下，過去需要Google等級的超級電腦才能做到的數學推理，現在一台電競主機就能搞定！這項技術打破了AI發展的硬體限制，讓AI不再是少數科技巨頭的專利。我們正在打造一個AI民主化的未來，讓各行各業、甚至個人，都能輕鬆駕馭AI的力量。這不僅能加速科學研究、提升教育品質，更能催生出無數創新應用。例如，我們預期未來每個人都能擁有自己的AI數學助理，解決生活中的各種難題。現在投資我們，您將站在AI革命的最前線，共同開創這個無限可能的未來！", "audio": "docs/data/audios/2506.08935v1.wav"}
{"query": "Foundation Model", "id": "2506.08902v1", "url": "http://arxiv.org/abs/2506.08902v1", "title": "Intention-Conditioned Flow Occupancy Models", "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:14:07.747235", "title_zh": "意圖條件化流佔據模型", "summary_zh": "本研究提出一種名為「意圖條件化流佔據模型」(InFOM) 的強化學習預訓練方法。InFOM利用流匹配技術，預測智能體在未來可能訪問的狀態分布（佔據度量）。模型中加入潛在變數捕捉使用者意圖，提升表達能力並實現廣義策略改進。實驗結果顯示，InFOM在36個基於狀態和4個基於圖像的基準測試任務中，相較於其他預訓練方法，平均回報提升1.8倍，成功率提升36%。InFOM為強化學習領域的大規模預訓練提供了一個有效途徑，有助於解決樣本效率和穩健性等核心挑戰。", "applications": ["智慧家庭：想像一下，你的智慧家庭不再只是被動執行指令，而是能預測你的需求。InFOM技術能讓智慧家庭系統學習你的生活習慣（意圖），提前調整燈光、溫度，甚至準備咖啡，打造更舒適便利的生活。", "自動駕駛：自動駕駛汽車可以利用InFOM預測其他車輛或行人的行為模式，例如預測行人是否打算穿越馬路。這能提升自動駕駛系統的安全性，減少事故發生。", "個人化醫療：InFOM可以用於預測病患的健康狀態發展趨勢。透過分析病患的病史、生活習慣等資訊（意圖），預測未來可能出現的健康問題，以便及早介入治療，達到個人化醫療的效果。"], "pitch": "各位投資人，我們正站在AI發展的下一個浪潮前沿！InFOM技術，意圖條件化流佔據模型，是強化學習領域的革命性突破，它將徹底改變機器與環境互動的方式。想像一下，不再需要耗費大量時間和資源訓練AI適應新環境，InFOM讓AI具備預測未來、理解使用者意圖的能力，從而實現更高效、更智能的決策。這意味著什麼？\n\n首先，在自動駕駛領域，InFOM將使汽車不僅能「看到」周圍環境，更能「預測」其他車輛和行人的意圖，大幅提升安全性，加速自動駕駛的普及。\n\n其次，在智慧製造領域，InFOM能讓機器人預測生產線的潛在問題，優化生產流程，降低成本，提高效率，助力企業實現智慧轉型。\n\n更重要的是，InFOM的潛力遠不止於此。它還可以應用於金融、醫療、能源等各個領域，例如預測股市走勢、診斷疾病、優化能源分配等等。這是一個數十億美元的市場，而我們擁有領先的技術和強大的團隊，有信心在這個市場中佔據主導地位。\n\n現在正是投資InFOM的最佳時機！我們需要您的資金，加速技術開發，擴大市場規模，共同打造一個更智能、更美好的未來！", "audio": "docs/data/audios/2506.08902v1.wav"}
{"query": "Diffusion Model", "id": "2506.08617v1", "url": "http://arxiv.org/abs/2506.08617v1", "title": "Diffusion model for analyzing quantum fingerprints in conductance fluctuation", "summary": "A conditional diffusion model has been developed to analyze intricate\nconductance fluctuations called universal conductance fluctuations or quantum\nfingerprints appearing in quantum transport phenomena. The model reconstructs\nimpurity arrangements and quantum interference patterns in nanometals by using\nmagnetoconductance data, providing a novel approach to analyze complex data\nbased on machine learning. In addition, we visualize the attention weights in\nthe model, which efficiently extract information on the non-local correlation\nof the electron wave functions, and the score functions, which represent the\nforce fields in the wave-function space.", "authors": ["Naoto Yokoi", "Yuki Tanaka", "Yukito Nonaka", "Shunsuke Daimon", "Junji Haruyama", "Eiji Saitoh"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:15:24.879224", "title_zh": "用於分析電導波動中量子指紋的擴散模型", "summary_zh": "本研究開發了一種條件擴散模型，用於分析量子傳輸現象中複雜的電導波動，又稱通用電導波動或量子指紋。該模型利用磁電導數據重建奈米金屬中的雜質排列和量子干涉圖樣，為基於機器學習的複雜數據分析提供了一種新穎方法。此外，我們可視化了模型中的注意力權重，有效地提取了電子波函數的非局部相關信息，以及表示波函數空間中力場的得分函數。", "applications": ["想像一下，我們可以利用這個技術來分析演唱會場地的音響效果。透過測量不同位置的聲音波動，就能重建場地內部的聲波干涉模式，從而優化音響設備的擺放，讓每個角落的聽眾都能享受最佳音質。", "這項技術也能應用於醫療領域，分析腦電圖（EEG）數據。透過重建大腦中的神經活動模式，醫生可以更精準地診斷癲癇等神經系統疾病，並制定更有效的治療方案。", "在材料科學領域，我們可以利用這個模型分析材料表面的電導波動，從而了解材料內部的微觀結構和缺陷分布。這有助於開發更高性能的半導體材料和更耐用的複合材料。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能解鎖微觀世界的秘密，並將其轉化為巨大的商業價值！我們的擴散模型不僅能分析量子指紋，更像是一把萬能鑰匙，可以解開複雜系統中隱藏的模式。想像一下，我們可以利用它來開發新一代的材料，其性能超越現有材料的極限，例如超導材料、更高效的太陽能電池等。在醫療領域，這項技術將帶來精準醫療的革命，幫助我們更早、更準確地診斷疾病。更重要的是，這項技術具有無限的潛力，可以應用於金融、氣象預測等各個領域。現在投資，您將成為這場科技革命的先驅，共同開創一個全新的時代！我們預計在五年內，這項技術將帶來數十億美元的市場價值，並改變我們生活的方方面面。", "audio": "docs/data/audios/2506.08617v1.wav"}
{"query": "AI", "id": "2506.09002v2", "url": "http://arxiv.org/abs/2506.09002v2", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-12T00:56:04.097589", "title_zh": "透過混合程式分析與大型語言模型提升Rust單元測試覆蓋率", "summary_zh": "本研究提出PALM，一種利用大型語言模型（LLMs）提升單元測試覆蓋率的方法。PALM透過程式分析找出函數中的分支條件，將其轉化為路徑約束，並結合相關上下文資訊，引導LLMs生成單元測試。實驗結果顯示，PALM在短時間內能顯著提升測試覆蓋率，在某些情況下，整體專案覆蓋率提高超過50%，平均覆蓋率達到75.77%，與人工測試相當。此方法展現了LLMs在自動化測試生成方面的潛力，並已成功向開源專案提交多個測試案例，大幅提升程式碼品質與可靠性。", "applications": ["自動駕駛系統：確保自動駕駛程式碼在各種路況下的安全性，降低事故風險。", "醫療設備軟體：驗證醫療設備軟體的準確性，避免因程式錯誤導致的誤診或治療失誤。", "金融交易系統：測試金融交易系統的穩定性，防止因程式漏洞造成的財務損失。"], "pitch": "各位投資人，我們正在打造軟體測試的未來！現今軟體開發週期中，測試往往是耗時且昂貴的環節。PALM，我們的創新技術，結合了程式分析和大型語言模型，能夠以前所未有的效率和準確性自動生成單元測試。想像一下，開發者不再需要花費大量時間編寫測試程式，而是可以將精力集中在核心功能的開發上。這不僅能大幅縮短開發週期，降低成本，更能確保軟體的品質和安全性。未來，PALM將成為所有軟體開發團隊不可或缺的工具，從小型新創公司到大型企業，都將受益於PALM帶來的效率提升和風險降低。我們預計，隨著AI技術的持續發展，PALM的應用範圍將不斷擴大，甚至可以應用於程式碼自動修復，成為真正的AI程式碼醫生。現在加入我們，共同開創軟體測試的新紀元！", "audio": "docs/data/audios/2506.09002v2.wav"}
{"query": "Foundation Model", "id": "2506.09022v2", "url": "http://arxiv.org/abs/2506.09022v2", "title": "Do Multiple Instance Learning Models Transfer?", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "published_date": "2025-06-10", "timestamp": "2025-06-12T00:57:13.228162", "title_zh": "多重實例學習模型可以轉移嗎？", "summary_zh": "多重實例學習（MIL）是計算病理學中，從大型組織圖像生成有意義的切片層級嵌入的基石方法。然而，MIL常在小型、弱監督的臨床數據集上遇到困難。本研究系統性地評估了預訓練MIL模型的遷移學習能力，結果顯示，即使在與目標任務不同的器官上訓練，預訓練模型仍始終優於從頭開始訓練的模型。在泛癌數據集上進行預訓練，更能實現跨器官和任務的強大泛化，勝過切片基礎模型，同時使用更少的預訓練數據。這突顯了MIL模型的強大適應性，並展示了利用遷移學習提高計算病理學性能的好處。我們提供了一個資源，標準化了MIL模型的實現和常用計算病理學任務的預訓練模型權重集合。", "applications": ["**癌症診斷輔助：** 想像一下，醫生可以透過AI快速分析病理切片，找出潛在癌細胞，提高診斷準確性，減少誤判，讓病人及早接受治療。", "**新藥開發加速：** 藥廠可以利用AI分析大量病理圖像，找出對特定疾病有效的藥物標靶，加速新藥開發流程，讓更多病人受益。", "**個人化醫療：** 透過AI分析病人的病理切片，預測疾病發展趨勢，制定更精準的治療方案，實現個人化醫療，提高治療效果。"], "pitch": "各位創投夥伴，我們正在開發一項革命性的AI技術，它能大幅提升癌症診斷的準確性和效率。我們的多重實例學習模型，就像一位經驗豐富的病理學家，能從海量的病理圖像中快速找出關鍵資訊，協助醫生做出更明智的決策。這項技術不僅能降低醫療成本，更能挽救無數生命。想像一下，未來每個醫院都能配備這樣一套AI系統，讓癌症不再是絕症，而是可以有效控制的疾病。我們相信，這項技術將顛覆醫療產業，創造巨大的商業價值。現在正是投資的絕佳時機，讓我們一起攜手打造更健康的未來！", "audio": "docs/data/audios/2506.09022v2.wav"}
{"query": "AI", "id": "2506.09988v1", "url": "http://arxiv.org/abs/2506.09988v1", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:44:10.759850", "title_zh": "EditInspector：一個用於評估文本引導圖像編輯的基準", "summary_zh": "隨著生成式AI的快速發展，文本引導的圖像編輯越來越普及。然而，如何驗證這些編輯的品質成為一個重要問題。我們推出EditInspector，一個基於人類標注的新基準，用於全面評估文本引導的圖像編輯。EditInspector可以評估編輯的準確性、偽影檢測、視覺品質、與場景的融合度、常識一致性以及描述編輯變化的能力。研究發現，現有模型在全面評估編輯方面存在困難，並且在描述變化時容易產生幻覺。為了解決這些問題，我們提出了兩種新方法，在偽影檢測和差異字幕生成方面優於現有模型。", "applications": ["想像一下，你可以用文字描述想要修改的照片，例如『把天空變成夕陽』，這個技術就能自動完成，讓你的照片更具藝術感，輕鬆在社群媒體上獲得更多讚。", "室內設計師可以利用這個技術，快速預覽不同風格的家具擺設在客戶家中的效果，例如『把沙發換成L型皮沙發』，讓客戶更直觀地了解設計方案，提高成交率。", "新聞媒體可以利用這個技術，快速生成示意圖，例如『在街道上增加人群』，更生動地呈現新聞事件，提升報導的吸引力。"], "pitch": "各位投資人，我們團隊開發的EditInspector技術，正迎合了生成式AI圖像編輯爆發性成長的需求。現有圖像編輯工具缺乏有效的品質驗證機制，導致使用者體驗不佳，甚至產生誤導。EditInspector提供了一個客觀、全面的評估標準，能有效提升圖像編輯的可靠性和可用性。我們的技術不僅能應用於現有的圖像編輯軟體，更可整合至廣告設計、電商產品展示、遊戲開發等領域，市場潛力巨大。想像一下，未來使用者可以透過簡單的文字指令，精準控制圖像內容，創造出無限的商業價值。我們預計，EditInspector將成為圖像編輯領域的黃金標準，為投資者帶來豐厚的回報。我們誠摯邀請您加入，共同開創圖像編輯的新紀元！", "audio": "docs/data/audios/2506.09988v1.wav"}
{"query": "Foundation Model", "id": "2506.09982v1", "url": "http://arxiv.org/abs/2506.09982v1", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:45:31.224163", "title_zh": "AnimateAnyMesh：用於文本驅動通用網格動畫的前饋式4D基礎模型", "summary_zh": "AnimateAnyMesh是一個創新的前饋式框架，能根據文字描述快速生成任意3D模型的動畫。它利用獨特的DyMeshVAE架構，有效壓縮和重建動態網格序列，同時分離空間和時間特徵。透過修正流訓練策略，模型能產生高品質、符合文字描述的動畫。研究團隊更釋出包含超過400萬個動態網格序列的DyMesh資料集。實驗證明，AnimateAnyMesh在幾秒內就能生成語義準確、時間連貫的網格動畫，在品質和效率上都超越現有方法，使4D內容創作更易於使用。", "applications": ["遊戲開發：遊戲設計師只需輸入文字描述，即可快速生成角色動畫，例如「殭屍緩慢行走」、「英雄快速奔跑」等，大幅縮短開發時間。", "虛擬實境（VR/AR）：使用者可以透過文字指令，讓虛擬化身做出各種動作，例如「開心跳舞」、「悲傷哭泣」，增加互動的真實感和趣味性。", "教育應用：學生可以輸入文字描述，讓3D模型展示特定動作或物理現象，例如「心臟跳動」、「行星繞太陽運轉」，幫助理解抽象概念。"], "pitch": "各位投資人，想像一下，未來不再需要昂貴的動畫工作室和專業動畫師，只需輸入簡單的文字描述，就能創造出逼真的3D動畫！AnimateAnyMesh正是實現這一願景的關鍵技術。它不僅能大幅降低動畫製作成本，更能開啟無限的商業可能性。我們可以將它應用於遊戲、VR/AR、教育、廣告等各個領域。例如，與電商平台合作，讓消費者透過文字指令，看到商品在不同情境下的動態展示；與影視公司合作，快速生成特效動畫，降低製作成本。更進一步，我們可以將這項技術與AI助手整合，讓使用者透過語音指令，輕鬆創造個性化的3D動畫內容。這不僅僅是一個動畫工具，更是一個革命性的4D內容創作平台，具有巨大的市場潛力。現在投資，您將成為這場變革的領跑者，共同開創4D內容的黃金時代！", "audio": "docs/data/audios/2506.09982v1.wav"}
{"query": "AI", "id": "2506.09985v1", "url": "http://arxiv.org/abs/2506.09985v1", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:19:17.943286", "title_zh": "V-JEPA 2：自我監督式影片模型實現理解、預測與規劃", "summary_zh": "V-JEPA 2 是一種透過觀看大量網路影片和少量機器人互動資料，來學習理解世界並採取行動的自我監督式模型。它在動作理解和人類行為預測方面表現出色，甚至超越了專門為這些任務設計的模型。透過與大型語言模型的結合，V-JEPA 2 在影片問答方面也達到了領先水平。更重要的是，它能利用從機器人收集的少量無標籤影片，進行機器人路徑規劃，並在不同實驗室的機器人手臂上實現物件的拾取和放置，無需額外訓練或獎勵。這證明了透過自我監督學習，模型能夠理解物理世界並制定相應的行動計畫。", "applications": ["智慧家庭：想像一下，你的智慧冰箱能透過觀察你打開冰箱的動作，預測你接下來想做什麼，並提前準備好食材，甚至幫你規劃更健康的飲食。", "自動駕駛：讓自動駕駛系統不僅能識別紅綠燈和行人，還能預測其他車輛或行人的意圖，例如判斷行人是否打算穿越馬路，從而做出更安全、更流暢的駕駛決策。", "遠程醫療：醫生可以透過觀看病患在家中的影片，更準確地判斷病患的病情，並提供更個性化的治療建議，尤其對於行動不便或居住偏遠地區的病患來說，這將極大地提升醫療服務的可及性。"], "pitch": "各位投資人，我們相信 V-JEPA 2 代表了人工智慧發展的下一個重大突破。它不僅僅是一個影片分析工具，而是一個能夠像人類一樣理解世界、預測未來並制定行動計畫的通用智能模型。試想一下，將 V-JEPA 2 應用於工業自動化，它可以讓機器人自主完成複雜的組裝和維修任務，大幅降低生產成本，提高效率。在娛樂產業，它可以生成逼真且互動性強的虛擬世界，為用戶帶來前所未有的沉浸式體驗。更令人興奮的是，V-JEPA 2 有望成為通用人工智慧的基石，催生出我們今天難以想像的全新應用。我們正在尋找具有遠見卓識的合作夥伴，共同將 V-JEPA 2 的潛力轉化為現實，引領下一波人工智慧浪潮。現在投資，您將站在這場革命的最前沿！", "audio": "docs/data/audios/2506.09985v1.wav"}
{"query": "Foundation Model", "id": "2506.09883v1", "url": "http://arxiv.org/abs/2506.09883v1", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:20:43.147567", "title_zh": "透過幾何蒸餾微調具備3D感知能力的視覺語言模型", "summary_zh": "本研究提出幾何蒸餾，一種輕量級、無需標註的微調框架，將人類啟發的幾何線索注入預訓練的視覺語言模型（VLMs），無需修改其架構。透過從現成的3D基礎模型（如MASt3R、VGGT）中提取稀疏對應關係、相對深度關係和密集成本體積，我們的模型塑造出具備幾何感知能力的表示，同時保持與自然圖像文本輸入的相容性。在3D視覺語言推理和3D感知基準測試中，我們的模型始終優於先前的方法，以顯著降低的計算成本實現了改進的3D空間推理。這為2D訓練的VLMs與3D理解的橋樑提供了一個可擴展且高效的路徑，開闢了在空間定位多模態任務中更廣泛的應用。", "applications": ["**智慧導航：**想像一下，你的手機不只告訴你該往哪裡走，還能真正『看懂』周圍環境，避開障礙物、判斷路面狀況，甚至在你快要撞到東西時發出警告。這就像有個超強的3D導航員，讓你走路、開車更安全。", "**虛擬試穿/試妝：**網購衣服或化妝品時，總是擔心不合適？有了這項技術，你可以用手機鏡頭即時『試穿』衣服、『試用』口紅，3D感知能力讓效果更逼真，就像真的穿在你身上一樣，大幅降低退貨率。", "**智慧家庭：**掃地機器人不再笨手笨腳，它能真正理解家裡的空間佈局，精準避開障礙物、清掃死角。你甚至可以用語音指令，讓它去特定位置（例如『客廳沙發底下』）清掃，讓生活更便利。"], "pitch": "各位投資人，我們正在打造下一代AI的『空間智慧』！現有的視覺語言模型雖然強大，但缺乏對3D世界的理解，就像一個色盲的畫家。我們的幾何蒸餾技術，能以極低的成本賦予這些模型『3D視覺』，讓它們真正『看懂』周圍的世界。想像一下，這項技術能應用在自動駕駛、機器人、AR/VR等領域，徹底改變人機互動的方式。隨著元宇宙的發展，對3D空間理解的需求將會爆發式增長，我們的技術將成為這個新世界的基石。現在投資我們，就是投資AI的未來，一個充滿『空間智慧』的未來！我們預期在五年內，搭載我們技術的產品將佔據智慧導航、虛擬試穿和智慧家庭市場的領先地位，帶來數十億美元的營收。", "audio": "docs/data/audios/2506.09883v1.wav"}
{"query": "AI", "id": "2506.09977v1", "url": "http://arxiv.org/abs/2506.09977v1", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:14:46.718441", "title_zh": "人們如何修正不一致的信念？透過使用者研究檢驗人類的信念修正", "summary_zh": "本研究探討人類在接收到新資訊時如何修正自身信念，這對於開發能有效模擬人類推理的AI系統至關重要。研究透過三項使用者研究發現，人們傾向於基於解釋進行信念修正，即使這可能導致與傳統信念變更理論不符的結果。無論解釋是由系統提供還是由使用者自行產生，人們都展現出對看似非最小修正的強烈偏好。這些發現暗示，AI系統在模擬人類推理或與人類互動時，應考慮基於解釋的、可能非最小的信念修正操作，以更好地與人類認知過程對齊。", "applications": ["AI心理諮商：當AI偵測到使用者信念與事實不符時，不是直接更正，而是提供解釋，引導使用者自行修正，減少反彈。", "個人化教育：根據學生的既有知識和信念，提供客製化的學習內容和解釋，幫助他們更有效地理解新概念，並修正錯誤觀念。", "智能客服：在處理客戶投訴時，智能客服不僅提供解決方案，更重要的是解釋問題發生的原因，以及解決方案的合理性，讓客戶更容易接受。"], "pitch": "各位投資人，想像一下，我們正在打造的不僅僅是AI，而是真正理解人類思考方式的AI！傳統AI在面對矛盾資訊時，往往依賴邏輯運算，忽略了人類更看重『解釋』的心理。我們的技術，基於使用者研究，讓AI能夠像人一樣，透過提供或引導產生解釋，來修正信念。這意味著什麼？更人性化的AI互動、更有效的知識傳遞，以及更強大的決策支持系統！試想，未來的AI醫療診斷，不僅告訴你得了什麼病，還會解釋病因和治療方案的依據，讓你安心接受治療。在金融投資領域，AI不僅提供投資建議，還會解釋背後的邏輯，讓你更放心地參與市場。這是一個百億美元級別的市場，我們擁有領先的技術和使用者數據，現在加入我們，一起打造真正懂你的AI，引領AI的下一個時代！", "audio": "docs/data/audios/2506.09977v1.wav"}
{"query": "Foundation Model", "id": "2506.09881v1", "url": "http://arxiv.org/abs/2506.09881v1", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:16:21.178865", "title_zh": "利用深度與語言實現開放詞彙領域泛化語義分割", "summary_zh": "本研究提出一個名為Vireo的全新單階段框架，旨在解決開放詞彙領域泛化語義分割（OV-DGSS）問題。OV-DGSS的目標是在未見過的領域中，為未見過的類別生成像素級別的遮罩，這對於在惡劣條件下的自動駕駛等真實場景至關重要。Vireo基於凍結的視覺基礎模型（VFMs），並通過深度VFMs整合場景幾何信息，以提取領域不變的結構特徵。為了彌合領域轉移下視覺和文本模態之間的差距，Vireo提出了GeoText提示、粗略遮罩先驗嵌入（CMPE）和領域-開放詞彙向量嵌入頭（DOV-VEH）等關鍵組件。實驗結果表明，Vireo在領域泛化和開放詞彙識別方面均優於現有方法，為多樣化和動態環境中的穩健視覺理解提供了一個統一且可擴展的解決方案。", "applications": ["自動駕駛系統：讓汽車即使在雨天、霧天或不同國家/地區，也能準確辨識行人、車輛、交通號誌等，提升行車安全。", "智慧城市監控：即使在光線不足或有遮蔽物的情況下，也能辨識異常事件（例如：跌倒、打架），及時發出警報。", "醫療影像分析：協助醫生辨識X光、CT掃描等影像中未曾見過的病灶或組織結構，提高診斷準確性。"], "pitch": "各位投資人，我們正處於AI視覺革命的前沿！Vireo不僅僅是一個語義分割模型，它代表著AI理解世界方式的根本性突破。想像一下，一個AI能夠像人類一樣，在任何環境下、看到任何東西都能理解。這就是Vireo的潛力！\n\n自動駕駛只是起點。我們的技術可以應用於智慧城市、醫療診斷、工業自動化，甚至太空探索！Vireo讓機器人能夠在完全陌生的環境中工作，讓AI能夠分析前所未有的數據，帶來無限可能。\n\n現有的AI在面對新環境或新物體時往往束手無策，但Vireo克服了這個瓶頸。我們相信，Vireo將成為未來AI視覺領域的基石，帶來巨大的商業價值。現在投資Vireo，就是投資AI的未來！我們預計在五年內，Vireo將成為各行業AI視覺解決方案的標準，為投資者帶來豐厚的回報。讓我們一起開創AI視覺的新時代！", "audio": "docs/data/audios/2506.09881v1.wav"}
{"query": "Diffusion Model", "id": "2506.09932v1", "url": "http://arxiv.org/abs/2506.09932v1", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:17:45.365176", "title_zh": "HadaNorm：透過中心化轉換的擴散轉換器量化", "summary_zh": "擴散模型在圖像生成領域表現出色，但其高記憶體和計算需求限制了在資源有限設備上的部署。後訓練量化(PTQ)通過降低矩陣運算的位寬提供了解決方案。然而，標準PTQ方法難以處理異常值，並且實現更高的壓縮通常需要在量化之前轉換模型權重和激活。本研究提出了一種新穎的線性轉換HadaNorm，它通過在應用Hadamard轉換之前對激活特徵通道進行歸一化，有效地緩解異常值，從而實現更激進的激活量化。實驗證明，HadaNorm始終如一地減少了轉換器模塊各個組件的量化誤差，與最先進的方法相比，實現了卓越的效率-性能權衡。", "applications": ["手機修圖App：讓低階手機也能流暢使用AI修圖功能，快速生成高品質圖片，不再卡頓。", "智慧監控系統：在攝影機端直接進行AI分析，即時辨識異常狀況，無需將大量影像傳回雲端，節省頻寬和電力。", "AI藝術創作：讓藝術家在平板電腦上就能進行複雜的AI繪圖和生成，隨時隨地激發創作靈感，不受硬體限制。"], "pitch": "各位創投大家好，我們正在打造AI普及化的未來！HadaNorm技術，讓原本只有高階伺服器才能運行的AI圖像生成模型，現在可以在手機、物聯網裝置上流暢運行。想像一下，未來每個人都能用手機輕鬆生成高品質的客製化圖像，智慧監控系統可以更快速、更節能地保障安全，AI藝術創作不再受限於昂貴的硬體設備。這不僅僅是技術突破，更是AI應用的大爆發！我們將授權HadaNorm技術給各大晶片廠、手機廠商、安防企業，打造AI生態系統。初期鎖定手機修圖、智慧監控、AI藝術創作三大市場，預計三年內達到數億美元的營收。現在投資我們，您將參與AI普及化的浪潮，共同打造一個更智慧、更便捷的未來！", "audio": "docs/data/audios/2506.09932v1.wav"}
{"query": "AI", "id": "2506.09975v1", "url": "http://arxiv.org/abs/2506.09975v1", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:31:55.244924", "title_zh": "當偵測失效時：微調模型生成如人類般的社群媒體文本的力量", "summary_zh": "本研究探討偵測AI生成的社群媒體文本的難題。由於社群媒體文本短小、非正式且充滿獨特性，使得偵測更加困難。然而，這項研究至關重要，因為社群媒體是網路影響力活動的重要攻擊媒介。研究團隊創建了一個包含超過50萬個AI生成社群媒體貼文的數據集，涵蓋11個爭議性議題。結果顯示，在攻擊者不公開其微調模型的情況下，偵測效果大幅下降。人類研究也證實了這一點。這意味著現有的偵測演算法很容易受到微調LLM的攻擊。這項研究結果對所有偵測領域都具有重要意義，因為微調是一種普遍適用且實際的LLM使用案例。", "applications": ["想像一下，新聞媒體可以用AI來分析大量社群媒體上的輿情，快速掌握民眾對特定議題的看法，並即時調整報導方向，更貼近民意。", "公司可以利用這項技術，辨識出競爭對手是否使用AI生成內容來進行惡意行銷或散播不實訊息，及早採取應對措施，維護品牌聲譽。", "政府單位可以監測網路上的極端言論或煽動性內容，預防潛在的社會動亂，並採取相應的維穩措施，確保社會安定。"], "pitch": "各位投資人，我們正站在一個AI軍備競賽的風口浪尖！AI生成內容的能力日益強大，但我們偵測這些內容的能力卻遠遠落後。這項研究揭示了微調模型在規避偵測方面的驚人力量，這意味著網路上的假訊息和輿論操縱將變得更加難以辨識。我們的解決方案不僅僅是一個偵測工具，更是一個預警系統，能夠幫助企業、政府和個人在資訊戰中保持領先。想像一下，我們可以利用這項技術打造一個『AI內容真實性評估平台』，為新聞媒體、社群平台和廣告商提供可信賴的內容驗證服務。隨著AI生成內容的普及，對真實性驗證的需求將會爆炸性增長，這將是一個數十億美元的市場！現在投資我們，就是投資於未來的資訊安全，成為這場AI軍備競賽的領跑者！我們甚至可以將這項技術應用於Deepfake影片的檢測，保護公眾人物和企業免受惡意攻擊。未來，我們還可以開發更先進的AI防禦系統，主動干擾和瓦解AI生成的假訊息攻勢，建立一個更健康、更真實的網路環境。", "audio": "docs/data/audios/2506.09975v1.wav"}
{"query": "Foundation Model", "id": "2506.09855v1", "url": "http://arxiv.org/abs/2506.09855v1", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:33:08.220647", "title_zh": "基於基礎模型的深度強化學習於 RIS 輔助無線通訊", "summary_zh": "本研究提出一個創新的框架，利用預訓練的開源基礎模型（FM），即大型無線模型（LWM），來處理無線通道並產生多功能且上下文相關的通道嵌入。這些嵌入用於聯合優化基地台波束成形和 RIS 配置。具體來說，我們設計了一個深度強化學習（DRL）模型來自動選擇基地台波束成形向量和 RIS 相移矩陣，旨在最大化頻譜效率（SE）。結果表明，用於無線電訊號理解的預訓練 FM 可以進行微調，並與 DRL 集成，從而在無線網路中做出有效的決策。模擬結果顯示，所提出的方法優於基於 DRL 的方法和基於波束掃描的方法，分別實現了 9.89% 和 43.66% 的頻譜效率提升。", "applications": ["想像一下，在人潮擁擠的體育場或演唱會現場，手機訊號不再卡頓。這項技術就像是個聰明的交通警察，能動態調整無線訊號的方向，確保每個人都能順暢地直播、傳照片或打卡。", "在智慧家居中，這項技術可以優化 Wi-Fi 訊號，讓家裡的每個角落都有穩定的網路連線，無論你在哪個房間，都能享受流暢的影音體驗，再也不用煩惱訊號死角。", "在自動駕駛汽車領域，這項技術能確保車輛之間、車輛與基礎設施之間的通訊暢通無阻。即使在複雜的城市環境中，也能提供可靠的導航和安全警示，提升行車安全。"], "pitch": "各位投資人，我們正在開發一種革命性的無線通訊技術，它能大幅提升網路效率，降低營運成本，並開啟全新的商業模式。想像一下，透過我們的技術，電信營運商能以更少的基地台覆蓋更大的範圍，提供更穩定的網路服務，從而節省數十億美元的基礎建設成本。更重要的是，這項技術為5G、6G甚至未來的無線通訊技術奠定了基礎，它將是智慧城市、物聯網、自動駕駛等領域的關鍵推動力量。我們預計，在未來五年內，這項技術的市場規模將達到數百億美元，而我們將成為這個市場的領導者。現在加入我們，共同打造無線通訊的未來！", "audio": "docs/data/audios/2506.09855v1.wav"}
{"query": "Diffusion Model", "id": "2506.09740v1", "url": "http://arxiv.org/abs/2506.09740v1", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:34:17.753186", "title_zh": "ELBO-T2IAlign：一種基於ELBO的通用方法，用於校準擴散模型中的像素級文字-圖像對齊", "summary_zh": "本研究發現，現有擴散模型在處理小型、遮擋或罕見物體時，文字與圖像的像素級對齊存在偏差。為了解決這個問題，我們提出了一種名為ELBO-T2IAlign的創新方法。它基於證據下界(ELBO)，無需訓練，即可有效校準擴散模型中的文字-圖像對齊。此方法通用性強，適用於各種擴散模型架構，並已在圖像分割和生成等基準數據集上驗證了其有效性。這項技術有助於提升圖像編輯、合成圖像生成等下游任務的性能。", "applications": ["想像一下，你可以用文字精準地編輯照片，比如把照片裡的小狗換成指定品種，而且細節完美無瑕，就像真的在那裡一樣。", "以後設計師可以透過輸入文字描述，快速生成各種風格的產品原型圖，省去大量手繪和建模的時間，加速產品開發流程。", "醫療影像分析領域，醫生可以透過文字引導，更精確地標記和分析X光片或MRI圖像中的病灶，提高診斷準確性。"], "pitch": "各位投資人，我們帶來的是一項革命性的圖像生成技術——ELBO-T2IAlign。它能精準校準文字與圖像的像素級對應關係，解決了AI圖像生成領域長期存在的對齊問題。這意味著，我們能創造出前所未有、高度精確且符合使用者意圖的圖像。想像一下，未來的廣告行銷、遊戲開發、甚至是虛擬實境內容，都將因為這項技術而產生質的飛躍。更重要的是，這項技術是免訓練且通用的，能快速整合到現有的AI圖像生成平台中，具有極高的商業價值和市場潛力。我們預期，這項技術將引領下一代AI圖像革命，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.09740v1.wav"}
{"query": "AI", "id": "2506.09968v1", "url": "http://arxiv.org/abs/2506.09968v1", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:14:32.336141", "title_zh": "SRLAgent：透過遊戲化與大型語言模型輔助強化自我調節學習技能", "summary_zh": "大學生面臨日益增加的學術壓力與獨立性，自我調節學習(SRL)至關重要。SRLAgent是一個由大型語言模型(LLM)輔助的系統，透過遊戲化和適應性支援來培養SRL技能。基於Zimmerman的三階段SRL框架，SRLAgent讓學生在互動遊戲環境中進行目標設定、策略執行和自我反思。系統提供由LLM驅動的即時回饋和鷹架式輔助，以支持學生的獨立學習。實驗結果顯示，使用SRLAgent的學生在SRL技能方面有顯著提升，且參與度更高。這項研究突顯了在遊戲化環境中嵌入SRL鷹架和即時AI支援的價值，為旨在促進更深層次的學習和後設認知技能發展的教育科技提供了設計啟示。", "applications": ["想像一下，國中生小明總是拖延作業，有了SRLAgent，系統會像個貼心的教練，透過遊戲化的方式引導他設定目標、規劃時間，並在完成任務後給予即時回饋，幫助他擺脫拖延症，成為時間管理高手。", "大學生小華準備期末考，SRLAgent能根據他的學習進度，推薦最有效的複習策略，並提供客製化的練習題，就像一位24小時隨時待命的AI家教，幫助他更有信心地迎接考試。", "社會新鮮人小美剛進入職場，SRLAgent可以幫助她分析自己的優勢和劣勢，設定職業目標，並提供學習資源和建議，讓她快速適應職場環境，提升工作效率。"], "pitch": "各位投資人，我們正站在教育科技革命的風口浪尖！SRLAgent不僅僅是一個學習工具，它是一個個性化的AI學習夥伴，能有效提升學生的自我調節學習能力，解決長期以來教育界難以攻克的難題。試想，如果每個學生都能擁有像SRLAgent一樣的AI教練，學習效率將大幅提升，升學率、就業率都將水漲船高！這意味著更優秀的人才，更強大的國家競爭力！SRLAgent的潛在市場極其龐大，從K12教育到高等教育，再到企業培訓，無處不在。我們可以與學校、教育機構、企業合作，提供訂閱服務、客製化課程，甚至開發針對特定領域的專業學習模組。更進一步，我們可以將SRLAgent與VR/AR技術結合，打造沉浸式學習體驗，讓學習變得更加有趣和有效。我們預計，SRLAgent將在未來五年內成為教育科技領域的獨角獸，為投資者帶來豐厚的回報！現在加入我們，共同打造教育的未來！", "audio": "docs/data/audios/2506.09968v1.wav"}
{"query": "Foundation Model", "id": "2506.09784v1", "url": "http://arxiv.org/abs/2506.09784v1", "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:16:12.814824", "title_zh": "基於凍結基礎模型的精準且高效的零樣本6D姿態估計", "summary_zh": "本研究提出FreeZeV2，一種無需訓練的方法，利用幾何和視覺基礎模型，實現對未見物體的精準6D姿態估計。FreeZeV2透過稀疏特徵提取、特徵感知評分機制和模組化設計，顯著提升了準確性和效率。相較於前代FreeZe，FreeZeV2在準確度提升5%的同時，速度提升了8倍。結合多個分割模型後，準確度更提升8%，速度仍快2.5倍。FreeZeV2在BOP Challenge 2024中榮獲最佳整體方法獎。這項技術為機器人學和擴增實境等領域帶來了革命性的突破，降低了對大量訓練資料的依賴，實現了更快速、更準確的物體識別和操作。", "applications": ["**智慧家庭助手：** 想像一下，你的機器人吸塵器能精準辨識地上的玩具、襪子，甚至能分辨不同品牌的清潔劑，並針對性地進行清潔，而不需要事先學習每一樣物品的形狀。", "**自動駕駛汽車：** 未來，自動駕駛汽車能更準確地識別道路上的行人、腳踏車騎士，甚至是散落在地上的障礙物，即使這些物體是汽車從未見過的，也能迅速做出反應，提高行車安全。", "**醫療手術輔助：** 醫生可以利用AR眼鏡，在手術過程中即時看到手術器械在病人體內的精確位置和角度，即使是新型的手術器械，也能立即上手，提高手術的精準度和成功率。"], "pitch": "各位投資人，想像一下，我們正在打造的是一個「零學習」的視覺AI引擎！傳統的AI需要海量資料訓練，耗時耗力，但FreeZeV2徹底顛覆了這個模式。它就像一個天生的偵探，即使面對完全陌生的物體，也能瞬間掌握其空間姿態。這意味著什麼？意味著我們將大幅降低AI部署的成本和門檻，加速AI在各行各業的應用。從智慧製造到智慧醫療，從自動駕駛到AR/VR，FreeZeV2將成為這些領域的關鍵賦能者。更重要的是，我們已經在國際頂級競賽中證明了FreeZeV2的領先地位。現在投資我們，您將擁抱一個潛力無限的未來，共同開創一個「萬物皆可識」的AI新時代！我們的目標是讓每一台機器、每一個設備，都擁有像人類一樣的視覺感知能力，而這一切，都將從FreeZeV2開始！", "audio": "docs/data/audios/2506.09784v1.wav"}
{"query": "Diffusion Model", "id": "2506.09681v1", "url": "http://arxiv.org/abs/2506.09681v1", "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:17:39.832275", "title_zh": "以瓦瑟斯坦距離評估去噪擴散模型的品質：雜訊分數與最佳界限", "summary_zh": "本研究探討去噪擴散機率模型（DDPMs）在生成新樣本方面的能力。DDPMs透過估計分數函數，將布朗運動映射到擴散過程，進而產生新的樣本。研究首先證實DDPMs對於分數評估中的常數變異數雜訊具有穩健性。接著，在瓦瑟斯坦-2距離中建立了有限樣本保證，展現了DDPMs對於雜訊分數估計的穩健性，並實現了比先前已知結果更快的收斂速度。此外，觀察到所得速率與高斯情況下的速率相符，意味著它們具有最佳性。簡單來說，就是這個模型在有雜訊干擾的情況下，依然可以準確且快速地生成高品質的資料。", "applications": ["AI繪圖軟體：即使使用者輸入的草稿或提示不夠清晰，也能生成精美的圖像，提升使用者體驗。", "醫療影像處理：在X光、MRI等影像中，去除雜訊干擾，幫助醫生更準確地診斷疾病，提高醫療效率。", "語音辨識系統：在嘈雜環境中，提高語音辨識的準確性，例如在工廠、車間等環境中，讓機器人能夠聽懂人類的指令。"], "pitch": "想像一下，一個AI模型可以像經驗豐富的藝術家一樣，即使在光線不足或畫布粗糙的情況下，也能創作出令人驚豔的作品。這就是我們技術的核心價值：在充滿雜訊的現實世界中，依然能精準地生成高品質的數據。這項技術的應用範圍極其廣泛，從AI藝術創作、醫療影像分析到自動駕駛等領域，都能帶來革命性的提升。我們預計，未來五年內，這項技術將成為AI領域的基礎設施，賦能各行各業。現在投資我們，您將成為這場AI革命的早期參與者，共同塑造AI的未來！我們不僅僅是在開發一個模型，我們是在打造一個平台，一個可以無限擴展的AI生態系統，一個可以讓AI真正理解並適應現實世界的引擎。這不僅僅是一筆投資，這是一個改變世界的機會！", "audio": "docs/data/audios/2506.09681v1.wav"}
{"query": "AI", "id": "2506.09947v1", "url": "http://arxiv.org/abs/2506.09947v1", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:12:46.478204", "title_zh": "KI4Demokratie：一個基於人工智慧的平台，用於監測和促進民主論述", "summary_zh": "社群媒體日益助長極端主義，特別是右翼極端主義，並加速反民主敘事的傳播。雖然人工智慧和資料科學常被用於操縱政治觀點，但我們迫切需要能夠有效監測、且不侵犯言論自由的工具。KI4Demokratie 是一個基於人工智慧的平台，旨在協助記者、研究人員和政策制定者，監測可能破壞民主價值的右翼言論。它運用機器學習模型分析每日收集的大量德國線上數據，提供德國數位領域趨勢的全面視角。初步分析顯示，追蹤有組織的極端主義行為既複雜又充滿希望，尤其是在關鍵事件期間。", "applications": ["新聞媒體可以使用這個平台來追蹤網路上仇恨言論的趨勢，並報導相關新聞，提醒大眾注意潛在的社會風險。", "教育機構可以利用這個平台來分析學生在社群媒體上的言論，了解他們的價值觀和意識形態，並提供適當的引導和教育。", "政府部門可以透過這個平台來監測網路上的不實訊息，並及時澄清，避免謠言擴散，維護社會穩定。"], "pitch": "各位創投先進，想像一下，一個能夠即時洞察社會輿論、預測潛在危機的人工智慧引擎。KI4Demokratie 不僅僅是一個監測平台，它更是一個社會風險預警系統。在假新聞氾濫、社群媒體影響力日增的時代，它能幫助我們捍衛民主價值，確保資訊的真實性。我們預見，未來企業可以利用它來監測品牌聲譽，政府可以利用它來應對突發事件，甚至個人也能用它來過濾不實資訊。這是一個潛力無限的市場，讓我們一起投資 KI4Demokratie，打造一個更透明、更健康的數位社會！", "audio": "docs/data/audios/2506.09947v1.wav"}
{"query": "Foundation Model", "id": "2506.09748v1", "url": "http://arxiv.org/abs/2506.09748v1", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:14:11.168015", "title_zh": "基於語義與結構約束的無人機絕對視覺定位分層圖像匹配", "summary_zh": "本研究提出一種用於無人機絕對視覺定位的分層跨源圖像匹配方法，旨在解決在無全球導航衛星系統（GNSS）信號下無人機定位的挑戰。該方法整合了語義感知和結構約束的粗略匹配模塊以及輕量級的精細匹配模塊。粗略匹配利用視覺基礎模型提取的語義特徵，在語義和結構約束下建立區域級的對應關係。精細匹配則提取精細特徵，建立像素級的對應關係。實驗結果表明，該方法在多種挑戰性條件下均表現出卓越的準確性和魯棒性，驗證了其有效性。此技術無需依賴相對定位技術，僅通過圖像檢索模塊和提出的分層圖像匹配模塊即可實現。", "applications": ["無人機快遞：在GNSS信號不佳的城市峽谷或室內環境中，無人機可以精確定位並完成包裹投遞。", "災難救援：在地震或洪水等災難發生後，無人機可以利用該技術繪製受災地圖，協助救援人員快速找到受困者，即使GNSS信號中斷。", "智慧農業：無人機可以精確監測農作物生長狀況，例如判斷哪一區域的作物需要額外施肥或灌溉，從而提高農業生產效率。"], "pitch": "各位投資人，我們帶來的是一項革命性的無人機定位技術，它將徹底改變無人機的應用場景。想像一下，在GNSS信號受限的環境中，我們的無人機依然能夠精準定位、自主導航，這意味著什麼？更高效的物流配送、更安全的災難救援、更精準的農業管理，這僅僅是開始！我們的分層圖像匹配技術，克服了傳統方法在跨源圖像差異和時間變化上的局限性，實現了前所未有的準確性和魯棒性。這項技術不僅能應用於現有的無人機市場，更將開啟全新的商業模式。例如，我們可以與保險公司合作，提供災後快速評估服務；與電信公司合作，進行基站信號覆蓋檢測；甚至與軍方合作，提供戰場偵察和情報收集。未來，隨著無人機技術的普及，我們的定位技術將成為各行各業的剛需。現在投資，您將搶佔市場先機，共同打造一個由精準定位技術賦能的無人機新時代！", "audio": "docs/data/audios/2506.09748v1.wav"}
{"query": "Diffusion Model", "id": "2506.09644v1", "url": "http://arxiv.org/abs/2506.09644v1", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:15:19.631579", "title_zh": "DGAE：擴散引導自編碼器，用於高效潛在表示學習", "summary_zh": "DGAE是一種新型自編碼器，旨在提升圖像和影片生成模型的效率。它透過擴散模型引導解碼器，即使在高壓縮比下也能有效恢復資訊，解決了傳統自編碼器在高壓縮時的效能下降問題。DGAE不僅提升了空間壓縮能力，更將潛在空間的維度縮小了2倍，實現了更高效、更精簡的表示。實驗證明，DGAE在ImageNet-1K圖像生成上表現出色，並且能加速擴散模型的收斂，為生成模型帶來了顯著的性能提升。", "applications": ["1. 手機修圖App：使用者可以快速壓縮照片大小，節省儲存空間，同時確保照片品質不受太大影響，上傳分享更方便。", "2. 視訊會議軟體：在網路不佳的情況下，DGAE可以幫助壓縮視訊資料，確保視訊通話的流暢性，減少延遲和卡頓。", "3. 遊戲開發：遊戲開發者可以使用DGAE來壓縮遊戲素材，例如角色模型和場景貼圖，降低遊戲的檔案大小，讓玩家更容易下載和安裝。"], "pitch": "各位投資人，我們正在開發一項革命性的圖像壓縮技術——DGAE，它能以更小的資料量，產生更高品質的圖像。想像一下，未來的AI繪圖只需要極少的運算資源，就能生成逼真的影像，這將徹底顛覆遊戲、影視、廣告等產業。DGAE不僅能節省儲存空間和傳輸頻寬，還能加速AI模型的訓練速度，降低成本。更令人興奮的是，DGAE的潛力遠不止於此，它甚至能應用於醫療影像分析，協助醫生更快更準確地診斷疾病。我們相信，DGAE將成為未來AI時代的基礎設施，擁有巨大的商業價值和社會影響力。現在加入我們，一起開創圖像處理的新紀元！", "audio": "docs/data/audios/2506.09644v1.wav"}
{"query": "AI", "id": "2506.09940v1", "url": "http://arxiv.org/abs/2506.09940v1", "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:56:26.588556", "title_zh": "資訊不對稱與知識可轉移性下線上策略決策的樣本複雜度", "summary_zh": "本研究探討在資訊不對稱和知識轉移挑戰下，如何有效學習策略決策。在多代理系統中，代理人基於私有資訊採取行動以最大化獎勵，這導致了複雜的混淆變數。同時，由於難以在目標環境中進行實驗，知識轉移也構成重大挑戰。我們提出了一種樣本高效的演算法，能準確識別資訊不對稱下的系統動態，並有效應對強化學習中的知識轉移。該方法在線上策略互動模型下，證明能以O(1/epsilon^2)的樣本複雜度學習到epsilon-最佳策略。", "applications": ["股票市場預測：利用過去的交易數據（知識轉移）來預測未來股價，即使不同投資者擁有不同的資訊（資訊不對稱），也能幫助散戶做出更明智的投資決策。", "醫療診斷：醫生根據病人的病歷、檢查報告等資訊（資訊不對稱）來判斷病情。透過學習大量病例數據（知識轉移），可以輔助醫生做出更精準的診斷，減少誤診率。", "交通流量控制：在不同路段的車流量資訊不對稱的情況下，透過分析歷史交通數據（知識轉移），可以即時調整紅綠燈時間，優化整體交通流量，減少交通堵塞。"], "pitch": "各位創投先進，我們正在開發一項突破性的AI技術，能解決資訊不對稱和知識轉移這兩大難題，這在現實世界中無所不在。想像一下，在金融市場，我們的演算法能夠比傳統模型更精準地預測股價，為投資者帶來超額回報。在醫療領域，我們的技術可以協助醫生做出更準確的診斷，挽救更多生命。更令人興奮的是，我們的技術具有高度的可擴展性，可以應用於智慧城市、供應鏈管理、甚至國防安全等領域。我們已經證明了我們的演算法具有極高的樣本效率，這意味著我們可以用更少的數據更快地學習，降低開發成本。我們相信，這項技術將徹底改變決策方式，創造巨大的商業價值。現在加入我們，一起塑造AI的未來！", "audio": "docs/data/audios/2506.09940v1.wav"}
{"query": "Foundation Model", "id": "2506.09638v1", "url": "http://arxiv.org/abs/2506.09638v1", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:57:44.158129", "title_zh": "FedVLMBench：視覺語言模型聯邦式微調基準測試", "summary_zh": "本研究推出FedVLMBench，首個針對視覺語言模型（VLM）聯邦式微調的系統性基準測試。它整合了兩種主流VLM架構、四種微調策略、五種聯邦學習算法，以及六個涵蓋多領域單任務和多任務場景的多模態數據集。實驗結果揭示了VLM架構、微調策略、數據異質性以及多任務聯邦優化之間的相互作用。研究發現，在聯邦學習中，對於基於編碼器的VLM，具有並行連接器和LLM調整的雙層多層感知器連接器是最佳配置。此外，當前的聯邦學習方法在以視覺為中心的任務中，比以文本為中心的任務對數據異質性更敏感。FedVLMBench為研究社群提供必要的工具、數據集和經驗指導，為推進多模態基礎模型的隱私保護聯邦式訓練提供標準化平台。", "applications": ["遠距醫療影像分析：在不洩漏病人隱私的情況下，讓不同醫院的醫療影像數據協同訓練AI模型，提升疾病診斷的準確性。", "個性化教育內容推薦：整合不同地區學生的學習數據，在保護學生隱私的前提下，為每位學生提供更精準的學習資源推薦。", "智慧城市交通流量預測：結合不同來源的交通數據（例如：監視器影像、感測器數據），在保護數據提供者隱私的情況下，提升交通流量預測的準確性，優化交通管理。"], "pitch": "各位投資人，我們帶來的是FedVLMBench，一個革命性的平台，它將引領視覺語言模型（VLM）進入聯邦學習的新時代！想像一下，一個能夠在保護隱私的前提下，整合全球醫療影像數據，開發出更精準的疾病診斷AI的未來；一個能夠匯集所有線上學習平台數據，打造真正個性化教育體驗的未來；一個能夠即時分析全球交通數據，實現智慧交通調度的未來。FedVLMBench正是實現這些願景的關鍵。它不僅是一個基準測試，更是一個創新的催化劑，將加速VLM在各個垂直領域的應用。隨著數據隱私意識的日益增強，聯邦學習的需求將呈指數級增長。FedVLMBench將成為這個市場的領導者，為我們帶來巨大的商業價值和社會影響力。現在加入我們，一起塑造這個隱私保護的AI驅動的未來！", "audio": "docs/data/audios/2506.09638v1.wav"}
{"query": "Diffusion Model", "id": "2506.09538v1", "url": "http://arxiv.org/abs/2506.09538v1", "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:59:22.934110", "title_zh": "AngleRoCL：適用於物理視角不變之T2I對抗性貼片的角度穩健概念學習", "summary_zh": "本研究探討了文字生成圖像(T2I)模型所產生的對抗性貼片在不同視角下的攻擊效果，發現這些貼片在角度穩健性方面存在問題。研究表明，文字提示顯著影響貼片的角度穩健性，且特定任務的語言指令無法有效提升。為此，我們提出AngleRoCL，一種簡單且靈活的方法，學習一個可泛化的概念，代表生成角度穩健貼片的能力。此概念可融入文字提示中，引導T2I模型生成對視角變化具有抵抗力的貼片。實驗結果顯示，AngleRoCL顯著提升了T2I對抗性貼片的角度穩健性，在多個角度下仍保持高攻擊成功率，攻擊效果平均相對提升超過50%。本研究推進了對物理角度穩健貼片的理解，並深入探討了文字概念與T2I生成內容中物理屬性之間的關係。", "applications": ["智慧交通：想像一下，在自動駕駛汽車的攝影機上貼上這種對抗性貼片，可以讓汽車誤判交通號誌，例如將停止標誌誤認為是限速標誌，從而導致交通意外。AngleRoCL可以幫助我們測試並增強自動駕駛系統的安全性，使其更能抵抗這種惡意攻擊。", "安全監控：在機場或重要設施的監控攝影機上貼上這種貼片，可以讓系統無法正確辨識特定人物或物體，從而影響安全預警。AngleRoCL技術有助於評估和改善監控系統的漏洞，確保公共安全。", "廣告干擾：在戶外廣告看板上貼上這種貼片，可以讓手機掃描時顯示錯誤的廣告內容，或者導向惡意網站。AngleRoCL的研究可以幫助我們開發更強大的圖像辨識系統，避免用戶受到欺騙或損害。"], "pitch": "各位創投先進，我們團隊帶來的是AngleRoCL技術，一項革命性的AI安全防禦方案。當前AI圖像辨識系統，特別是自動駕駛、安全監控等領域，面臨著嚴峻的對抗性攻擊威脅。駭客只需透過精心設計的貼片，就能輕易欺騙AI系統，造成難以估計的損失。AngleRoCL的獨特之處在於，它能讓AI系統在各種角度、光線下，都能精準辨識物體，有效抵禦這些惡意攻擊。想像一下，未來無人機送貨普及，AngleRoCL能確保無人機不會被惡意貼片誤導，安全抵達目的地；智慧工廠中，AngleRoCL能保護生產線上的機器手臂，避免因圖像辨識錯誤而發生事故。這不僅僅是一項技術，更是一份安全保障，一個潛力無限的市場。我們預計，隨著AI應用的普及，AngleRoCL將成為AI安全領域的關鍵基礎設施，市場規模將達到數十億美元。現在加入，您將成為AI安全革命的領航者！", "audio": "docs/data/audios/2506.09538v1.wav"}
{"query": "AI", "id": "2506.10975v1", "url": "http://arxiv.org/abs/2506.10975v1", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:44:41.947637", "title_zh": "GenWorld：邁向偵測AI生成的真實世界模擬影片", "summary_zh": "隨著影片生成技術蓬勃發展，AI生成影片的偵測需求日益增加。然而，缺乏高品質的真實世界數據集阻礙了可靠偵測器的發展。為此，我們提出GenWorld，一個大規模、高品質的真實世界模擬數據集，用於AI生成影片偵測。GenWorld專注於模擬真實世界場景的影片，這些影片因其真實感和潛在影響力而意義重大。我們利用多種先進的影片生成模型提供逼真且高品質的偽造影片，並包含來自不同生成器和各種提示模態（例如，文本、圖像、影片）的影片，從而學習更具泛化性的特徵。我們也提出SpannDetector模型，利用多視角一致性作為真實世界AI生成影片偵測的強大標準，實驗證明其效果優異。GenWorld將推動AI生成影片偵測領域的發展。", "applications": ["新聞媒體可以使用這項技術來驗證收到的影片素材是否為真實拍攝，避免誤報或傳播假新聞，維護新聞的可信度。", "保險公司可以利用AI判斷車禍或意外事故的影片是否經過篡改，防止詐保案件發生，減少不必要的損失。", "教育機構可以運用此技術，檢測學生提交的影片作業是否為AI生成，確保學術誠信，鼓勵學生獨立思考和創作。"], "pitch": "各位創投先進，想像一下，在AI影片真假難辨的時代，誰能掌握辨識真偽的鑰匙，誰就能掌握資訊安全的主導權！GenWorld不僅是一個數據集，更是一座金礦！我們開發的SpannDetector模型，能有效辨識由世界模型（如Cosmos）生成的高品質偽造影片，這代表我們能領先市場，提供最可靠的AI影片驗證服務。試想，未來所有的新聞媒體、社群平台、政府機關，甚至是個人，都需要我們的技術來驗證影片的真實性。這是一個數十億美元的潛在市場！更進一步，我們可以將此技術應用於國防安全、金融詐欺防範等領域，其價值難以估量。現在投資GenWorld，就是投資未來，讓我們一起打造一個更真實、更安全的數位世界！", "audio": "docs/data/audios/2506.10975v1.wav"}
{"query": "AI", "id": "2506.10953v1", "url": "http://arxiv.org/abs/2506.10953v1", "title": "Build the web for agents, not agents for the web", "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:18:48.315754", "title_zh": "為代理人打造網路，而非為網路打造代理人", "summary_zh": "近年來，大型語言模型（LLM）和多模態模型的發展，激發了開發網路代理人的濃厚興趣。這些AI系統能在網路環境中自主導航和完成任務。然而，由於人類設計的介面與LLM能力之間存在根本不匹配，現有方法面臨巨大挑戰。本論文倡導網路代理人研究的範式轉移：我們應該開發一種專為代理能力優化的新型互動範式，而不是強迫網路代理人適應為人類設計的介面。為此，我們引入了「代理人網路介面」（AWI）的概念，並確立了六項設計原則，強調安全性、效率和標準化。這種重新定義旨在克服現有介面的根本局限性，為更高效、可靠和透明的網路代理人設計鋪平道路。", "applications": ["智能客服：網路代理人可以自動處理線上客服請求，解答常見問題、協助用戶查找資料，甚至完成簡單的交易，大幅降低企業的人力成本。", "自動化購物：代理人可以根據使用者設定的條件（例如價格、品牌、規格）自動搜尋、比較和購買商品，省去使用者瀏覽大量網頁的時間。", "行程規劃：代理人可以整合各種網站資訊，例如航班、飯店、景點等，自動規劃最佳旅遊行程，並協助預訂機票和住宿。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！想像一下，一個由AI代理人驅動的網路世界，在這個世界裡，複雜的網路互動變得簡單、高效、自動化。我們提出的「代理人網路介面」（AWI）正是實現這個願景的關鍵。現有的網路介面是為人類設計的，AI代理人要適應這些介面，效率低下、錯誤頻發。AWI則反其道而行，為AI代理人量身打造，讓它們能夠更安全、更高效地完成各種網路任務。這不僅僅是技術上的突破，更是一場商業模式的變革。試想，未來的電商平台、旅遊網站、金融服務，都將基於AWI構建，AI代理人將成為使用者與網路世界互動的主要介面。我們預計，AWI將催生一個數十億美元的市場，而我們正是這個市場的開拓者。現在加入我們，一起打造AI驅動的未來網路！", "audio": "docs/data/audios/2506.10953v1.wav"}
{"query": "AI", "id": "2506.10934v1", "url": "http://arxiv.org/abs/2506.10934v1", "title": "Dynamic Epistemic Friction in Dialogue", "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:13:43.255960", "title_zh": "[翻譯失敗] Dynamic Epistemic Friction in Dialogue", "summary_zh": "摘要翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "applications": ["應用場景1：翻譯失敗，請參考原文", "應用場景2：翻譯失敗，請參考原文", "應用場景3：翻譯失敗，請參考原文"], "pitch": "推銷內容翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "audio": "docs/data/audios/2506.10934v1.wav"}
{"query": "Foundation Model", "id": "2506.10914v1", "url": "http://arxiv.org/abs/2506.10914v1", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:14:54.990913", "title_zh": "基於先驗數據擬合網路的因果推論基礎模型", "summary_zh": "本研究提出CausalFM，一個基於先驗數據擬合網路(PFN)的因果推論基礎模型框架。PFN是一種Transformer模型，先使用來自預先指定的先驗分佈的合成數據進行預訓練，然後通過上下文學習實現貝氏推論。CausalFM基於結構因果模型(SCM)建立貝氏先驗，並使用因果啟發的貝氏神經網路，在後門、前門和工具變數調整等不同情境下執行貝氏因果推論。我們訓練了一個用於估計條件平均處理效應(CATE)的基礎模型，並證明CausalFM在合成和半合成基準測試中具有競爭力。CausalFM為因果推論提供了一種新範式，有望從根本上改變醫學、經濟學等領域的因果推論方式。", "applications": ["**個人化醫療建議：** 根據你的生活習慣、基因數據和病史，CausalFM能更準確地預測特定治療方案對你的效果，幫助醫生制定更有效的個人化治療計畫。", "**精準行銷：** 商家可以利用CausalFM分析不同行銷活動對顧客購買行為的影響，找出真正有效的策略，避免浪費資源在無效的廣告上。", "**政策影響評估：** 政府可以利用CausalFM預測政策實施後對社會各個層面的影響，例如教育改革對學生學習成果的影響，從而制定更完善的政策。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻！CausalFM不僅僅是一個模型，它代表著因果推論領域的革命性突破。想像一下，在醫學上，我們不再僅僅依賴相關性，而是能真正理解藥物對病人的因果效應，從而實現精準醫療，大幅提高治療成功率，降低醫療成本。在金融領域，CausalFM能幫助我們更準確地預測市場走向，降低投資風險。在政策制定上，它能幫助政府預測政策的真實影響，避免決策失誤。CausalFM的潛力是無限的，它能應用於任何需要理解因果關係的領域。我們相信，CausalFM將成為未來AI發展的基石，引領我們進入一個更智慧、更可預測的時代。現在加入我們，共同打造這個劃時代的產品，抓住這波AI浪潮，實現巨大的商業價值！", "audio": "docs/data/audios/2506.10914v1.wav"}
{"query": "Diffusion Model", "id": "2506.10971v1", "url": "http://arxiv.org/abs/2506.10971v1", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:16:27.169513", "title_zh": "遮罩離散擴散模型中，引導究竟做了什麼？", "summary_zh": "本研究深入探討了帶有Classifier-Free Guidance (CFG) 的遮罩離散擴散模型。在理想條件下，我們推導出引導反向動態的明確解，精確地描述引導如何影響採樣行為。當目標是從特定類別採樣時，引導會放大特定類別區域，同時抑制與其他類別共享的區域。這種效應取決於引導強度，並在採樣分佈中產生不同的協方差結構。值得注意的是，我們觀察到一維和二維空間中存在定量差異。研究還表明，對於較大的引導強度，總變異的衰減率在反向動態中呈雙指數形式。這些發現突顯了引導的作用，不僅在於塑造輸出分佈，還在於控制採樣軌跡的動態。", "applications": ["AI繪圖助手：像Midjourney或Stable Diffusion一樣，透過調整引導強度，讓使用者更精準地控制AI生成的圖片風格和內容，例如指定畫作更偏向印象派或寫實主義。", "醫療影像分析：協助醫生更準確地辨識X光片或MRI中的微小病灶，透過增強特定組織或器官的特徵，減少誤診率。", "語音合成：讓AI語音更具情感，例如，在生成悲傷語氣時，引導模型更強調低沉、緩慢的音調，使語音聽起來更自然、更符合情境。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓生成式AI的控制力提升到前所未有的水平。我們的研究揭示了「引導」在遮罩離散擴散模型中的核心作用，讓AI不再只是隨機生成，而是能精準地按照我們的意願創造內容。想像一下，一個能完全理解你需求的AI藝術家，或者一個能精確診斷疾病的AI醫生，這就是我們技術的潛力。我們相信，這項技術將徹底改變內容創作、醫療診斷、以及更多領域。未來，我們甚至可以將這項技術應用於新藥開發，透過引導模型生成具有特定藥理特性的分子結構。現在加入我們，一起打造AI驅動的未來！", "audio": "docs/data/audios/2506.10971v1.wav"}
{"query": "AI", "id": "2506.10927v1", "url": "http://arxiv.org/abs/2506.10927v1", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:23:33.537698", "title_zh": "生成式AI在促進社交互動中的作用：範圍界定性回顧", "summary_zh": "社交連結減少對心理健康、壽命和整體福祉構成威脅。生成式AI技術，如大型語言模型和圖像生成工具，正被廣泛應用於增強人類社交體驗。本研究回顧了30篇2020年以來發表的文獻，探討了基於生成式AI的應用程式如何促進社交互動，以及它們針對的社交參與形式。研究發現，這些應用程式廣泛應用於故事敘述、社交情緒技能訓練、懷舊、協作學習、音樂創作和一般對話。研究強調了參與式和共同設計方法在促進有效技術使用和社交參與中的作用，同時也審視了文化偏見和可及性等社會倫理問題。生成式AI有潛力支持動態和個人化的互動，但需要更加關注公平設計實踐和包容性評估策略。", "applications": ["爺爺奶奶常常感到孤單？我們可以利用AI生成他們年輕時的照片，並與他們聊天，回憶過去的美好時光，讓他們的生活不再孤單。", "小朋友不擅長表達自己的情緒？透過AI互動遊戲，讓他們學習如何識別和表達情緒，提升社交能力，成為EQ高手。", "想學新樂器卻找不到人一起練習？AI可以化身為你的專屬樂團，隨時隨地和你一起Jam，激發你的音樂潛能。"], "pitch": "各位投資人，想像一下，一個不再有孤獨的世界！我們的技術正是通往這個世界的鑰匙。我們利用生成式AI，不僅能創造個人化的社交體驗，更能解決高齡化社會的孤獨問題，提升年輕世代的社交技能。這不僅是一個技術革新，更是一項社會責任投資！市場規模將會隨著AI技術的成熟與普及，呈現爆炸性成長。我們預計在三年內，成為社交AI領域的領導者，五年內將技術應用於醫療、教育、娛樂等更廣泛的領域。現在投資，您將成為這場社交革命的先驅，共同打造一個更連結、更溫暖的未來！我們的目標是讓每個人都能享受高品質的社交生活，讓AI成為促進人與人之間連結的橋樑，而非阻礙。這是一個千載難逢的投資機會，讓我們一起改變世界！", "audio": "docs/data/audios/2506.10927v1.wav"}
{"query": "Foundation Model", "id": "2506.10579v1", "url": "http://arxiv.org/abs/2506.10579v1", "title": "Equations of state and stability condition of mixed p-spin glass model", "summary": "The Sherrington-Kirkpatrick (SK) is a foundational model for understanding\nspin glass systems. It is based on the pairwise interaction between each two\nspins in a fully connected lattice with quenched disordered interactions. The\nnature of long-range interaction among spins in the (SK) model simplifies the\nstudy of this system by eliminating fluctuations. An advanced (SK) model, known\nas the p-spin model, introduces higher-order interactions that involve the\ninteraction of P spins. This research focuses on the general Hamiltonian of the\nspin glass model with long-range interaction, referred to as the mixed p-spin\nglass model, which consists of adding all p-spin interaction terms. This\nresearch aims to derive the equation of states for this Hamiltonian, formulate\nthe equation of state within the framework of the first replica symmetry\nbreaking, and determine both the stability condition of the replica symmetric\nsolution and the stability of the replicas belonging to the same group in the\nfirst step of replica symmetry breaking.", "authors": ["Ali Talebi"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:24:52.066766", "title_zh": "混合p-自旋玻璃模型的狀態方程式與穩定性條件", "summary_zh": "本研究深入探討混合p-自旋玻璃模型，這是一個更複雜的自旋玻璃模型，它擴展了傳統的Sherrington-Kirkpatrick (SK) 模型，引入了涉及多個自旋之間的高階交互作用。我們的目標是推導出此模型的狀態方程式，並在第一步複製對稱性破壞的框架下建立方程式。此外，我們還致力於確定複製對稱解的穩定性條件，以及在複製對稱性破壞的第一步中，屬於同一群組的複製體的穩定性。這項研究有助於更深入理解複雜系統的行為，並為材料科學和信息科學等領域的應用奠定基礎。", "applications": ["想像一下，我們可以利用這種模型來設計更穩定的記憶體。就像自旋玻璃一樣，記憶體中的數據也需要保持在一個穩定的狀態。這個模型可以幫助我們找到最佳的材料組合和結構，讓數據不會輕易丟失或損壞。", "在金融市場上，股票價格的波動就像自旋一樣，互相影響。這個模型可以幫助我們分析市場的複雜關係，預測風險，並設計出更有效的投資策略。", "在生物學上，蛋白質的摺疊方式決定了它的功能。這個模型可以幫助我們理解蛋白質是如何找到正確的摺疊方式，並設計出新的藥物來治療疾病。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，基於對混合p-自旋玻璃模型的深入研究，這項技術將徹底改變材料科學、金融和生物技術等領域。想像一下，我們能夠設計出永不失效的記憶體、預測金融市場的崩盤，甚至解開蛋白質摺疊的終極奧秘。我們的模型不僅僅是一個理論框架，更是一個強大的工具，可以幫助我們解決現實世界中的複雜問題。我們相信，這項技術的潛在商業價值是無限的，現在投資，您將站在下一次科技革命的最前沿！未來，我們將把這個模型應用於量子計算領域，開發出更強大、更穩定的量子位元，引領下一個世代的科技發展。不要錯過這個機會，加入我們，一起創造未來！", "audio": "docs/data/audios/2506.10579v1.wav"}
{"query": "Diffusion Model", "id": "2506.10963v1", "url": "http://arxiv.org/abs/2506.10963v1", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:26:23.188296", "title_zh": "MMMG：一個大規模、跨領域、多層級的文本到圖像推理生成基準", "summary_zh": "本研究提出知識圖像生成的新任務，並創建了大規模跨領域多層級知識圖像生成基準（MMMG），旨在評估圖像生成模型的推理能力。知識圖像在人類文明和學習中至關重要。MMMG包含4456個專家驗證的圖像-提示對，涵蓋10個學科、6個教育程度和多種知識格式。為了簡化評估，採用統一的知識圖譜（KG）表示。我們還引入MMMG-Score來評估生成的知識圖像，結合了事實準確性（通過KG的圖編輯距離衡量）和視覺清晰度評估。評估結果顯示，現有模型在推理方面存在嚴重缺陷，為此，我們發布了FLUX-Reason，作為一個有效的開源基準。", "applications": ["**教育輔助：** 想像一下，學生在學習複雜的歷史事件時，只需輸入文字描述，就能自動生成相關的事件時間軸圖或因果關係圖，幫助他們更直觀地理解知識。", "**醫療健康：** 醫生可以利用這項技術，將病患的病歷資料轉換成易於理解的圖表或示意圖，方便與病患溝通病情，提升醫囑遵從性。", "**新聞報導：** 新聞媒體可以快速生成新聞事件的相關圖表或地圖，例如地震災情分布圖、經濟數據走勢圖等，讓讀者更快速地掌握新聞重點。"], "pitch": "各位投資人，我們正在開發一項劃時代的技術：基於MMMG基準的知識圖像生成。想像一下，未來的人工智慧不只能生成逼真的圖像，還能理解複雜的知識並將其視覺化呈現。這將徹底改變教育、醫療、新聞、行銷等各個領域。我們的技術不僅能提升學習效率、改善溝通效果，還能創造全新的商業模式。例如，我們可以為企業客製化生成數據分析報告的視覺化圖表，或為遊戲開發者提供快速生成遊戲素材的工具。隨著AI技術的不斷發展，知識圖像生成將成為一個巨大的市場。現在投資我們，您將站在AI革命的最前沿，共同開創一個充滿無限可能的未來！我們相信，這項技術的潛力遠遠超出我們的想像，它將成為下一代AI的核心引擎。", "audio": "docs/data/audios/2506.10963v1.wav"}
{"query": "AI", "id": "2506.10916v1", "url": "http://arxiv.org/abs/2506.10916v1", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:13:05.843699", "title_zh": "[翻譯失敗] Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary_zh": "摘要翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "applications": ["應用場景1：翻譯失敗，請參考原文", "應用場景2：翻譯失敗，請參考原文", "應用場景3：翻譯失敗，請參考原文"], "pitch": "推銷內容翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "audio": "docs/data/audios/2506.10916v1.wav"}
{"query": "Foundation Model", "id": "2506.10395v1", "url": "http://arxiv.org/abs/2506.10395v1", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:14:32.966572", "title_zh": "雙魚座：用於圖像理解與生成的自迴歸基礎模型", "summary_zh": "本研究提出名為「雙魚座」的自迴歸多模態基礎模型，旨在解決圖像理解與生成在統一模型中表現不佳的問題。雙魚座採用獨特的解耦視覺編碼架構，並針對多模態生成進行優化訓練。透過精心的數據管理、預訓練與微調，雙魚座在圖像理解與生成任務上均展現了卓越的競爭力。實驗結果顯示，雙魚座在超過20個圖像理解公開基準測試中表現出色，並在GenEval圖像生成基準測試中展現了強大的生成能力。研究揭示了圖像理解與生成之間的協同效應，以及使用獨立視覺編碼器的優勢，推動了統一多模態模型的發展。", "applications": ["**智慧相簿自動分類：** 想像一下，你的手機相簿可以自動辨識照片內容，例如風景、人物、食物等，並進行智慧分類，讓你輕鬆找到想看的照片，省去手動整理的麻煩。", "**AI繪圖助手：** 只要簡單描述你想要的畫面，例如「夕陽下的海灘，有幾個小孩在玩沙」，AI就能根據你的描述生成栩栩如生的圖像，讓每個人都能成為藝術家。", "**醫療影像輔助診斷：** 醫生可以利用AI快速分析X光片、CT掃描等醫療影像，協助早期發現病灶，提升診斷效率和準確性，造福更多患者。"], "pitch": "各位投資人，我們正站在AI圖像技術革命的浪潮之巔！「雙魚座」不僅僅是一個模型，它是一把開啟圖像理解與生成無限可能的鑰匙。試想一下，未來的電商平台，消費者只需上傳一張舊沙發的照片，AI就能立即生成數十種不同風格、材質的新沙發設計方案，並直接連結到供應商，實現個性化定制。在遊戲產業，開發者可以利用「雙魚座」快速生成各種逼真的人物、場景，大幅降低開發成本，提升遊戲品質。更重要的是，我們正在積極探索將「雙魚座」應用於自動駕駛、智慧城市等領域，讓AI成為真正的視覺大腦，賦能各行各業。現在投資「雙魚座」，就是投資AI的未來，我們有信心在短時間內實現指數級增長，為各位帶來豐厚的回報！", "audio": "docs/data/audios/2506.10395v1.wav"}
{"query": "Diffusion Model", "id": "2506.10962v1", "url": "http://arxiv.org/abs/2506.10962v1", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:15:49.844564", "title_zh": "SpectralAR：頻譜自迴歸視覺生成", "summary_zh": "本研究提出一種名為SpectralAR的視覺生成框架，有別於傳統將圖像分割成空間圖塊的方式，SpectralAR從頻譜角度實現視覺序列的因果關係。它首先使用嵌套頻譜標記將圖像轉換為有序的頻譜標記，代表由低到高的頻率分量。然後，以由粗到細的方式，使用頻譜標記序列執行自迴歸生成。這種方法兼顧了序列因果關係和標記效率，在ImageNet-1K圖像重建和自迴歸生成實驗中，僅使用64個標記和3.1億個參數，就達到了3.02 gFID的優異成果。", "applications": ["AI修復老照片：可以將模糊不清的老照片，透過AI運算，還原成清晰、色彩鮮明的影像，讓珍貴的回憶重現。", "線上遊戲材質生成：遊戲開發者可以利用這項技術，快速生成各種遊戲場景的材質，例如岩石、木紋、金屬等等，大幅縮短開發時間。", "醫療影像增強：醫生可以利用這項技術，提升X光片、斷層掃描等醫療影像的清晰度，幫助診斷疾病。"], "pitch": "各位投資人，我們相信SpectralAR將徹底改變視覺內容生成領域。想像一下，一個AI能夠以驚人的效率和逼真度生成各種圖像，從高解析度的產品渲染圖到個性化的藝術作品，再到複雜的醫療影像。 SpectralAR的頻譜自迴歸方法，不僅在技術上領先，更具有巨大的商業潛力。我們預計，這項技術將廣泛應用於遊戲、廣告、醫療、電商等行業，成為AI圖像生成領域的領頭羊。我們的團隊擁有深厚的技術積累和豐富的市場經驗，我們有信心將SpectralAR打造成一個獨角獸企業，為各位投資人帶來豐厚的回報。現在投資，您將成為這場視覺革命的早期參與者，共同開創AI圖像生成的新時代！未來，我們甚至可以將其應用於元宇宙的建設，快速生成各種虛擬場景和人物，打造一個高度真實且豐富多彩的數位世界。", "audio": "docs/data/audios/2506.10962v1.wav"}
{"query": "AI", "id": "2506.10908v1", "url": "http://arxiv.org/abs/2506.10908v1", "title": "Probably Approximately Correct Labels", "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:17:10.288601", "title_zh": "可能近似正確標籤", "summary_zh": "為了降低獲取高品質標註資料集的成本，本研究提出一種結合專家標籤與預訓練AI模型預測的方法。此方法能以較低的成本建構標註資料集，並確保標籤的整體錯誤率在可接受範圍內，即「可能近似正確」。透過結合人工智慧模型，我們得以更嚴謹且高效地管理資料集。我們已透過大型語言模型的文本標註、預訓練視覺模型的圖像標註，以及AlphaFold的蛋白質摺疊分析，驗證了此方法的優勢。", "applications": ["AI醫生助理：AI可以分析病歷和醫學影像，初步判斷病情。醫生再審核AI的建議，大幅提升診斷效率，並減少誤診率。", "智慧客服：AI客服可以根據用戶問題，快速從龐大的知識庫中找到答案。客服人員則處理較複雜或AI無法解決的問題，降低客服成本，提升客戶滿意度。", "自動駕駛訓練：AI可以模擬各種駕駛場景，產生大量標註數據。工程師只需驗證AI生成的數據，即可加速自動駕駛系統的開發，提高安全性。"], "pitch": "想像一下，我們正處於一個數據爆炸的時代，AI的發展高度依賴於大量的標註數據。然而，傳統的人工標註成本高昂、耗時費力，嚴重阻礙了AI的發展速度。我們的「可能近似正確標籤」技術，就像是AI數據領域的「煉金術」，它能以極低的成本，將預訓練AI模型的初步預測，結合少量專家驗證，快速生成高質量的標註數據。這意味著，AI模型的訓練成本將大幅降低，開發週期將顯著縮短。從醫療診斷、金融風控，到自動駕駛、智慧製造，各行各業都將因此受益。我們預計，未來五年內，隨著AI技術的普及，對高質量標註數據的需求將呈指數級增長。我們的技術將成為AI產業的基石，擁有巨大的市場潛力。現在投資我們，您將與我們一起，引領AI數據革命，共同分享AI時代的巨大紅利！", "audio": "docs/data/audios/2506.10908v1.wav"}
{"query": "Foundation Model", "id": "2506.10386v1", "url": "http://arxiv.org/abs/2506.10386v1", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:18:18.533362", "title_zh": "利用六自由度姿態基礎模型繪製海洋沉積物覆蓋圖", "summary_zh": "本研究提出一個名為PoseIDON的電腦視覺流程，結合深度基礎模型特徵與多視角攝影測量技術，從水下遙控載具（ROV）影片中估算海底物體（如桶子和彈藥）的六自由度姿態以及周圍海床的方位。透過將物體的CAD模型與觀測影像對齊，並擬合海床的局部平面近似，推斷出掩埋深度。實驗結果顯示，該模型能以約10公分的平均誤差估算掩埋深度，並解析反映底層沉積物傳輸過程的空間掩埋模式。此方法實現了海底掩埋的可擴展、非侵入式繪圖，並支持受污染地點的環境評估。", "applications": ["海底電纜或管線巡檢：精確判斷電纜或管線是否被泥沙覆蓋，及時維護，避免損壞。", "尋找失落的飛機或船隻殘骸：透過分析殘骸的姿態和掩埋情況，推斷沉沒時間和原因。", "海洋考古：協助考古學家繪製海底文物的三維地圖，研究古代文明。"], "pitch": "各位創投先進，我們團隊開發的PoseIDON技術，是水下環境監測領域的革命性突破！想像一下，全球海洋污染日益嚴重，各國對海底廢棄物處理的需求迫在眉睫。我們的技術能精準、高效地繪製海底掩埋物地圖，幫助政府和企業評估污染風險、制定清理策略，甚至回收有價值的資源。除了環保應用，PoseIDON還能用於海底基礎設施巡檢、海洋考古等領域，市場潛力巨大。更重要的是，我們的技術具有高度的可擴展性，未來可以整合AI技術，實現自動化的海底環境監測。我們相信，PoseIDON將成為水下機器人視覺領域的領頭羊，為創投帶來豐厚的回報！現在投資，您將站在海洋科技的最前沿，共同開創藍色經濟的無限可能！", "audio": "docs/data/audios/2506.10386v1.wav"}
{"query": "Diffusion Model", "id": "2506.10955v1", "url": "http://arxiv.org/abs/2506.10955v1", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:19:29.309616", "title_zh": "ReGuidance：一個簡潔的擴散模型封裝器，用於提升困難反問題的樣本品質", "summary_zh": "本研究提出一個名為ReGuidance的簡潔封裝器，旨在提升預訓練擴散模型在解決反問題時的樣本真實性和品質。針對訊號雜訊比低的困難反問題，現有方法容易偏離數據流形，產生不真實的結果。ReGuidance透過逆轉候選解，並將其潛在向量作為擴散後驗抽樣(DPS)的初始化，來改善此問題。實驗證明，在大型圖像修復和高倍率超解析度等任務中，ReGuidance能顯著提升樣本品質和測量一致性，甚至在多模態數據分佈上，ReGuidance能同時提升獎勵值並使候選解更接近數據流形。本研究也為DPS提供了首個嚴格的演算法保證。", "applications": ["老照片修復：將模糊或損壞的老照片變得清晰，重現珍貴回憶。", "醫療影像增強：提升X光片、MRI等醫療影像的清晰度，幫助醫生更準確地診斷疾病。", "犯罪現場重建：根據模糊的監控錄影或目擊者描述，重建清晰的犯罪現場圖像，協助警方破案。"], "pitch": "各位投資人，我們正處於AI生成內容的黃金時代，但現有技術在處理複雜、模糊的反問題時仍力不從心。ReGuidance的出現，正是解決這一痛點的關鍵！想像一下，我們能將模糊的衛星圖像轉化為清晰的城市地圖，為自動駕駛提供更精確的環境資訊；或是將微弱的生物訊號放大，早期發現癌症等疾病。ReGuidance不僅能提升圖像品質，更能應用於工業檢測、科學研究等多個領域，市場潛力巨大。我們相信，ReGuidance將成為AI圖像處理領域的基礎設施，引領新一輪的技術革命，為投資者帶來豐厚的回報！", "audio": "docs/data/audios/2506.10955v1.wav"}
{"query": "AI", "id": "2506.10897v1", "url": "http://arxiv.org/abs/2506.10897v1", "title": "GenPlanX. Generation of Plans and Execution", "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tomás de la Rosa", "Alfredo Garrachón", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:11:48.504597", "title_zh": "GenPlanX：計畫生成與執行", "summary_zh": "GenPlanX結合大型語言模型（LLM）與傳統AI規劃引擎，讓使用者能用自然語言描述複雜任務，系統即可自動生成行動方案並執行。它能理解人類意圖，簡化工作流程，提升生產力。想像一下，只要口頭指示，GenPlanX就能自動安排會議、預訂差旅、甚至管理日常家務。這項技術讓人機協作更無縫，釋放更多時間與精力，專注於更重要的事務。GenPlanX不僅是工具，更是個人助理，開啟智慧生活新篇章。", "applications": ["**會議安排小幫手：** 只要告訴GenPlanX『下週安排與客戶的會議，討論合約細節』，它會自動檢查雙方行程、發送邀請、預訂會議室，甚至準備相關文件。", "**智能家居管家：** 說聲『我到家了』，GenPlanX會自動開燈、調整室溫、播放音樂，讓你一回到家就能享受舒適的環境。如果說『準備晚餐』，它會根據你的飲食偏好，推薦菜單並開始準備食材清單。", "**差旅規劃專家：** 告訴GenPlanX『我要去台北出差三天』，它會自動搜尋機票、酒店、安排交通，並根據你的預算和偏好，提供最佳方案。"], "pitch": "各位投資人，想像一下，一個能聽懂人話、自動完成複雜任務的AI助理，將徹底顛覆工作與生活方式。GenPlanX正是實現這個願景的關鍵。它結合了LLM的理解能力與傳統AI的執行效率，讓使用者能用自然語言控制一切。這不僅僅是技術創新，更是一場效率革命。試想，企業可以大幅降低人力成本，個人可以擺脫繁瑣事務，專注於創造性工作。GenPlanX的應用場景無限廣闊，從智能辦公、智能家居到智能城市，都將迎來指數級的增長。我們預計，GenPlanX將成為未來人機協作的基礎設施，市場規模將達到數千億美元。現在投資GenPlanX，就是投資未來，讓我們一起引領這場AI革命！", "audio": "docs/data/audios/2506.10897v1.wav"}
{"query": "Foundation Model", "id": "2506.10335v1", "url": "http://arxiv.org/abs/2506.10335v1", "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that\nsurpasses the neural radiance field (NeRF) in both rendering speed and visual\nquality by leveraging an explicit 3D scene representation. Existing 3DGS\napproaches require a large number of calibrated views to generate a consistent\nand complete scene representation. When input views are limited, 3DGS tends to\noverfit the training views, leading to noticeable degradation in rendering\nquality. To address this limitation, we propose a Point-wise Feature-Aware\nGaussian Splatting framework that enables real-time, high-quality rendering\nfrom sparse training views. Specifically, we first employ the latest stereo\nfoundation model to estimate accurate camera poses and reconstruct a dense\npoint cloud for Gaussian initialization. We then encode the colour attributes\nof each 3D Gaussian by sampling and aggregating multiscale 2D appearance\nfeatures from sparse inputs. To enhance point-wise appearance representation,\nwe design a point interaction network based on a self-attention mechanism,\nallowing each Gaussian point to interact with its nearest neighbors. These\nenriched features are subsequently decoded into Gaussian parameters through two\nlightweight multi-layer perceptrons (MLPs) for final rendering. Extensive\nexperiments on diverse benchmarks demonstrate that our method significantly\noutperforms NeRF-based approaches and achieves competitive performance under\nfew-shot settings compared to the state-of-the-art 3DGS methods.", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:13:07.644754", "title_zh": "PointGS：基於點注意力感知與高斯濺射的稀疏視角合成", "summary_zh": "本研究提出一個名為PointGS的新框架，旨在解決3D高斯濺射(3DGS)技術在視角稀疏情況下容易過擬合的問題。PointGS首先利用立體視覺模型估算精確的相機位姿，並重建稠密點雲以初始化高斯分佈。接著，透過採樣和聚合稀疏輸入中的多尺度2D外觀特徵，來編碼每個3D高斯的顏色屬性。此外，設計基於自注意力機制的點交互網絡，增強點級別的外觀表示。實驗證明，PointGS在稀疏視角下，能實現高品質、即時的渲染效果，顯著優於傳統NeRF方法，並與最先進的3DGS方法競爭。", "applications": ["線上購物：想像一下，你可以在網路上用手機隨意繞著商品拍攝幾張照片，就能立即產生商品的3D模型，讓你從各個角度仔細觀察，就像在實體店面一樣。", "室內設計：設計師只要用手機掃描房間的幾個角落，就能快速建立房間的3D模型，方便擺放虛擬家具，讓客戶在裝修前就能預覽實際效果。", "遊戲開發：遊戲開發者可以使用少量照片快速建立遊戲場景的3D模型，大幅縮短開發時間，並且能創造出更逼真的遊戲世界。"], "pitch": "各位投資人，我們正處於視覺技術的黃金時代！PointGS技術突破了傳統3D建模的限制，僅需少量圖片即可生成高品質3D模型，這意味著更低的成本、更快的速度和更廣泛的應用。想像一下，未來AR/VR的普及，人們需要大量3D內容，而PointGS將成為內容生成的關鍵引擎。從電商的沉浸式購物體驗、到遊戲的逼真場景、再到工業設計的快速原型製作，PointGS的應用潛力無限。我們預計，隨著技術的成熟，PointGS將成為元宇宙基礎建設的重要一環，市場規模將達到數十億美元。現在投資PointGS，就是投資未來！", "audio": "docs/data/audios/2506.10335v1.wav"}
{"query": "Diffusion Model", "id": "2506.10892v1", "url": "http://arxiv.org/abs/2506.10892v1", "title": "The Diffusion Duality", "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:14:15.989052", "title_zh": "擴散二元性", "summary_zh": "本研究提出一種名為Duo的方法，旨在提升均勻狀態離散擴散模型在文字生成方面的效能。Duo的核心洞見在於，均勻狀態擴散過程實際上源自底層的高斯擴散。透過將高斯擴散的強大技術轉移到離散擴散模型，Duo在訓練和採樣方面都獲得顯著改善。首先，引入了受高斯過程引導的課程學習策略，透過降低方差，使訓練速度翻倍。其次，提出了離散一致性蒸餾算法，將連續一致性蒸餾適應於離散環境，實現了擴散語言模型中的少步生成，將採樣速度提高了兩個數量級。實驗結果顯示，Duo在多個基準測試中超越了自迴歸模型。", "applications": ["**AI寫作助手：**想像一下，你可以用更短的時間，產出更高品質的文章。無論是撰寫行銷文案、新聞稿，甚至是小說情節，AI都能幫你快速生成初稿，節省大量的時間和精力。", "**即時翻譯校正：**出國旅遊或工作時，透過App即時翻譯對話，AI不僅能快速翻譯，還能自動校正語法和用詞，讓溝通更加流暢自然，避免誤解。", "**遊戲對話生成：**在遊戲中，AI能根據玩家的行為和情境，快速生成豐富多樣的NPC對話，讓遊戲世界更加生動有趣，提升玩家的沉浸感。"], "pitch": "各位投資人，我們正處於AI內容生成的黃金時代！Duo技術不僅解決了現有擴散模型速度慢的問題，更在品質上超越了傳統自迴歸模型。想像一下，未來所有的內容創作都將由AI驅動，Duo將成為這場革命的核心引擎！從自動生成劇本、程式碼，到客製化教育內容，Duo的應用潛力無窮。我們預計，Duo將在三年內佔據AI內容生成市場的領先地位，為投資者帶來數十倍甚至數百倍的回報。這不僅僅是一項技術，更是一個改變世界的機會！現在加入，一起塑造AI驅動的未來！", "audio": "docs/data/audios/2506.10892v1.wav"}
{"query": "AI", "id": "2506.10890v1", "url": "http://arxiv.org/abs/2506.10890v1", "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation", "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter", "authors": ["Zhao Zhang", "Yutao Cheng", "Dexiang Hong", "Maoke Yang", "Gonglei Shi", "Lei Ma", "Hui Zhang", "Jie Shao", "Xinglong Wu"], "published_date": "2025-06-12", "timestamp": "2025-06-14T00:54:12.732312", "title_zh": "CreatiPoster：邁向可編輯和可控的多層圖形設計生成", "summary_zh": "CreatiPoster是一個能生成可編輯、多圖層設計的框架，使用者可以透過文字指令或素材來引導生成過程。它採用了RGBA大型多模態模型，先產生一個JSON規格，詳細描述每個圖層（文字或素材）的佈局、層級、內容和樣式，再加上簡潔的背景提示。接著，條件背景模型會根據這些前景圖層合成一個連貫的背景。CreatiPoster在圖形設計生成基準測試中，超越了領先的開源方法和商業系統。我們還釋出了一個包含10萬個多圖層設計的免版權語料庫，以促進進一步的研究。CreatiPoster支援多種應用，例如畫布編輯、文字疊加、響應式調整大小、多語言適配和動畫海報，從而推動AI輔助圖形設計的普及。", "applications": ["小明想在臉書上分享旅遊照片，但排版苦手。有了CreatiPoster，他只要輸入幾個關鍵字，就能自動生成吸睛的海報，讓他的照片更專業。", "社團需要製作活動宣傳海報，但預算有限。透過CreatiPoster，他們可以輕鬆設計出高品質的海報，省下找設計師的費用，還能隨時修改內容。", "電商老闆娘想為商品製作促銷圖片，但缺乏設計經驗。利用CreatiPoster，她可以快速生成各種風格的圖片，提升商品的吸引力，增加銷售額。"], "pitch": "各位投資人，我們正處於一個視覺內容爆炸的時代，但高品質的圖形設計仍然耗時且昂貴。想像一下，如果每個人都能輕鬆創造出專業級的設計，那將釋放出多麼巨大的潛力！CreatiPoster正是為了解決這個問題而生。它不僅超越了現有的開源和商業系統，更重要的是，它具備高度的擴展性和商業價值。我們可以將其應用於廣告行銷、電商、教育、社交媒體等各個領域。未來，我們計劃將CreatiPoster整合到現有的設計平台，甚至開發出獨立的AI設計App，讓CreatiPoster成為AI設計領域的領導者。我們相信，CreatiPoster將徹底改變圖形設計的產業，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.10890v1.wav"}
{"query": "AI", "id": "2506.10862v1", "url": "http://arxiv.org/abs/2506.10862v1", "title": "OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics", "summary": "High-fidelity and efficient simulation of fluid dynamics drive progress in\nvarious scientific and engineering applications. Traditional computational\nfluid dynamics methods offer strong interpretability and guaranteed\nconvergence, but rely on fine spatial and temporal meshes, incurring\nprohibitive computational costs. Physics-informed neural networks (PINNs) and\nneural operators aim to accelerate PDE solvers using deep learning techniques.\nHowever, PINNs require extensive retraining and careful tuning, and purely\ndata-driven operators demand large labeled datasets. Hybrid physics-aware\nmethods embed numerical discretizations into network architectures or loss\nfunctions, but achieve marginal speed gains and become unstable when balancing\ncoarse priors against high-fidelity measurements. To this end, we introduce\nOmniFluids, a unified physics pre-trained operator learning framework that\nintegrates physics-only pre-training, coarse-grid operator distillation, and\nfew-shot fine-tuning, which enables fast inference and accurate prediction\nunder limited or zero data supervision. For architectural design, the key\ncomponents of OmniFluids include a mixture of operators, a multi-frame decoder,\nand factorized Fourier layers, which enable efficient and scalable modeling of\ndiverse physical tasks while maintaining seamless integration with\nphysics-based supervision. Across a broad range of two- and three-dimensional\nbenchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven\nmethods in flow field reconstruction and turbulence statistics accuracy,\ndelivering 10-100x speedups compared to classical solvers, and accurately\nrecovers unknown physical parameters from sparse, noisy data. This work\nestablishes a new paradigm for efficient and generalizable surrogate modeling\nin complex fluid systems under limited data availability.", "authors": ["Rui Zhang", "Qi Meng", "Han Wan", "Yang Liu", "Zhi-Ming Ma", "Hao Sun"], "published_date": "2025-06-12", "timestamp": "2025-06-14T03:40:29.976184", "title_zh": "OmniFluids：流體動力學的統一物理預訓練模型", "summary_zh": "OmniFluids是一個創新的流體動力學模擬框架，它結合了物理預訓練、粗網格算子提煉和少量樣本微調，從而在有限或零數據監督下實現快速推斷和準確預測。OmniFluids利用混合算子、多幀解碼器和分解傅立葉層等關鍵組件，高效且可擴展地模擬各種物理任務，同時與基於物理的監督無縫集成。在廣泛的二維和三維基準測試中，OmniFluids在流場重建和湍流統計精度方面顯著優於現有AI方法，與傳統求解器相比，速度提高了10-100倍，並能從稀疏、嘈雜的數據中準確恢復未知的物理參數。這項工作為複雜流體系統中高效且可泛化的代理建模建立了一個新範例。", "applications": ["天氣預報：更準確地預測天氣變化，提前預警極端天氣事件，例如颱風路徑和降雨量，減少災害損失。", "汽車設計：在虛擬環境中模擬汽車周圍的氣流，優化車身外形，降低風阻，提高燃油效率，減少碳排放。", "醫療器材：設計更好的心血管支架或人工瓣膜，透過模擬血液流動，減少血栓形成的風險，延長器材使用壽命。"], "pitch": "各位投資人，想像一下，如果我們能以閃電般的速度，精準預測任何流體行為，會帶來什麼樣的變革？OmniFluids正是這樣一款顛覆性技術！它不僅比傳統方法快10-100倍，還能在數據稀缺的情況下，準確模擬複雜流體系統。這意味著，我們可以在新藥研發中，快速模擬藥物在血液中的擴散；在航空航天領域，設計出更節能、更安全的飛行器；甚至在金融市場，預測資金流動，抓住投資機會。OmniFluids的潛力遠不止於此。隨著AI和算力的不斷發展，我們有信心將其應用於更廣泛的領域，例如氣候建模、能源開發、甚至宇宙探索。現在投資OmniFluids，就是投資未來，讓我們一起引領流體模擬的新時代！", "audio": "docs/data/audios/2506.10862v1.wav"}
{"query": "AI", "id": "2506.10857v1", "url": "http://arxiv.org/abs/2506.10857v1", "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos", "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.", "authors": ["Jiashuo Yu", "Yue Wu", "Meng Chu", "Zhifei Ren", "Zizheng Huang", "Pei Chu", "Ruijie Zhang", "Yinan He", "Qirui Li", "Songze Li", "Zhenxiang Li", "Zhongying Tu", "Conghui He", "Yu Qiao", "Yali Wang", "Yi Wang", "Limin Wang"], "published_date": "2025-06-12", "timestamp": "2025-06-14T06:16:38.401424", "title_zh": "VRBench：長篇敘事影片中多步驟推理的基準測試", "summary_zh": "VRBench是一個專為評估大型模型在長篇敘事影片中多步驟推理能力而設計的基準測試。它包含1010個平均時長1.6小時的長影片，以及超過9000個人工標記的多步驟問答對和3萬多個帶時間戳的推理步驟。這些影片經過嚴格篩選，確保情節連貫性。VRBench設計了一個多階段評估流程，從結果和過程層面評估模型，並使用LLM引導的評分指標來全面評估推理鏈的質量。通過對多個LLM和VLM的廣泛評估，VRBench為多步驟推理領域的研究提供了寶貴的見解。", "applications": ["想像一下，你正在用VR學歷史。VRBench可以幫助系統理解歷史事件的因果關係，例如，『因為奧匈帝國皇儲被刺殺，所以引發了第一次世界大戰』，讓學習體驗更深入。", "如果有一個AI能理解複雜的電影情節，並根據你的喜好推薦你可能喜歡的電影，那會怎麼樣？VRBench就能協助AI做到這一點，它能理解影片中人物的動機和事件的發展。", "在自動駕駛領域，VRBench可以幫助AI理解駕駛環境中的複雜場景，例如，『因為前方有行人穿越馬路，所以需要減速』，提高駕駛的安全性。"], "pitch": "各位投資人，想像一下，我們正在打造的是AI界的『福爾摩斯』！VRBench不僅是一個基準測試，更是開啟AI理解複雜世界的一把鑰匙。現有的AI在處理長篇影片和複雜推理方面能力不足，而VRBench的出現，正是為了填補這個巨大的缺口。試想，未來AI能夠像人類一樣理解電影情節、分析歷史事件、甚至預測市場趨勢，這將帶來多大的商業價值？從智慧教育到自動駕駛，再到金融分析和娛樂產業，VRBench的應用前景無限廣闊。我們相信，投資VRBench，就是投資AI的未來，一個充滿無限可能的未來！早期加入，您將有機會成為這個AI推理革命的領航者！", "audio": "docs/data/audios/2506.10857v1.wav"}
{"query": "AI", "id": "2506.10829v1", "url": "http://arxiv.org/abs/2506.10829v1", "title": "LLM-Driven Personalized Answer Generation and Evaluation", "summary": "Online learning has experienced rapid growth due to its flexibility and\naccessibility. Personalization, adapted to the needs of individual learners, is\ncrucial for enhancing the learning experience, particularly in online settings.\nA key aspect of personalization is providing learners with answers customized\nto their specific questions. This paper therefore explores the potential of\nLarge Language Models (LLMs) to generate personalized answers to learners'\nquestions, thereby enhancing engagement and reducing the workload on educators.\nTo evaluate the effectiveness of LLMs in this context, we conducted a\ncomprehensive study using the StackExchange platform in two distinct areas:\nlanguage learning and programming. We developed a framework and a dataset for\nvalidating automatically generated personalized answers. Subsequently, we\ngenerated personalized answers using different strategies, including 0-shot,\n1-shot, and few-shot scenarios. The generated answers were evaluated using\nthree methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our\nfindings indicated that providing LLMs with examples of desired answers (from\nthe learner or similar learners) can significantly enhance the LLMs' ability to\ntailor responses to individual learners' needs.", "authors": ["Mohammadreza Molavi", "Mohammadreza Tavakoli", "Mohammad Moein", "Abdolali Faraji", "Gábor Kismihók"], "published_date": "2025-06-12", "timestamp": "2025-06-14T09:12:49.057091", "title_zh": "基於大型語言模型的個人化答案生成與評估", "summary_zh": "本研究探索大型語言模型（LLM）在生成個人化答案方面的潛力，旨在提升線上學習體驗並減輕教育者的負擔。我們利用StackExchange平台，針對語言學習和程式設計兩個領域進行了全面的研究，開發了驗證自動生成個人化答案的框架和數據集。實驗結果表明，向LLM提供範例答案，無論是來自學習者本身還是類似學習者，都能顯著提升LLM根據個人需求客製化答案的能力。此技術有助於打造更有效率、更具吸引力的線上學習環境。", "applications": ["想像一下，你正在學英文，遇到一個文法問題百思不得其解。有了這個技術，AI就能根據你過去的學習紀錄和程度，用你最容易理解的方式解釋給你聽，就像一位超級耐心的家教。", "如果你是個程式設計新手，卡在一個Bug上很久。傳統的論壇或教學影片可能無法針對你的特定程式碼提供協助。但有了這個技術，AI可以分析你的程式碼，並提供客製化的解決方案，就像一位經驗豐富的工程師在你身邊指導。", "假設你的孩子在準備考試，遇到不懂的數學題目。AI可以根據孩子的學習風格和進度，提供逐步的解題指導，而不是直接給答案，幫助孩子真正理解概念。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，將徹底改變線上教育的面貌。基於大型語言模型的個人化答案生成，能夠根據每個學習者的獨特需求，提供客製化的學習體驗，大幅提升學習效率和參與度。想像一下，一個不再有學習障礙的世界，每個人都能以最適合自己的方式學習。這項技術不僅能應用於線上教育平台，還能整合到企業培訓、技能提升等各個領域，市場潛力巨大。我們的初步研究已經證明了技術的可行性，並在StackExchange平台上取得了令人鼓舞的成果。我們相信，透過您的資金支持，我們可以將這項技術推向市場，打造一個更智能、更個人化的學習未來，成為教育科技領域的獨角獸！未來，我們更可以將這項技術應用於心理諮詢、健康建議等領域，打造一個全方位的個人化服務平台。", "audio": "docs/data/audios/2506.10829v1.wav"}
{"query": "Foundation Model", "id": "2506.10157v1", "url": "http://arxiv.org/abs/2506.10157v1", "title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "authors": ["Michelle M. Li", "Ben Y. Reis", "Adam Rodman", "Tianxi Cai", "Noa Dagan", "Ran D. Balicer", "Joseph Loscalzo", "Isaac S. Kohane", "Marinka Zitnik"], "published_date": "2025-06-11", "timestamp": "2025-06-14T09:14:10.864707", "title_zh": "一位病人，多重情境：透過情境智能擴展醫療人工智慧", "summary_zh": "現有的醫療AI模型，例如用臨床筆記訓練的語言模型，或醫療影像的視覺語言模型，雖然能總結病歷、回答問題、輔助決策，但要適應新的族群、專科或環境，往往需要微調或提示工程，非常不便。這些模型難以理解不熟悉的輸入，也無法根據訓練中未出現的臨床情境調整。因此，模型容易出現情境錯誤，預測看似合理，卻忽略了關鍵的病人特定或情境資訊。本研究提出一個醫療AI情境切換的願景：模型無需重新訓練，就能動態調整其推理，適應新的專科、族群、工作流程和臨床角色，從而診斷、管理和治療各種疾病，並擴大醫療服務的可及性。", "applications": ["想像一下，你去看醫生，醫生可以使用AI模型快速瞭解你的病史和當前的狀況，即使你換了醫院或科別，AI都能無縫接軌，提供醫生最完整的資訊，就像一位超級助理一樣。", "如果住在偏鄉地區，醫療資源不足，情境切換AI可以幫助當地的醫生或護理人員做出更準確的判斷，甚至可以透過遠距醫療，讓大醫院的專家也能即時提供協助。", "當發生大型災難或傳染病疫情時，情境切換AI可以迅速適應新的醫療需求，例如快速分析大量病患資料，找出高風險族群，並協助調配醫療資源，提高整體應變能力。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！現有的醫療AI模型就像單一功能的機器人，只能在特定情境下工作。而我們的情境切換AI，就像一位能夠隨時學習和適應的超級醫生，無論面對什麼樣的病人、疾病或環境，都能提供最佳的醫療建議。這項技術的商業價值巨大，不僅可以提高醫療效率、降低醫療成本，還能擴大醫療服務的可及性，尤其是在偏遠地區和發展中國家。想像一下，未來每個人都能擁有一個AI醫生，隨時隨地提供個性化的醫療服務。我們相信，情境切換AI將徹底改變醫療產業，成為下一個十年的投資熱點。現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.10157v1.wav"}
{"query": "Diffusion Model", "id": "2506.10639v1", "url": "http://arxiv.org/abs/2506.10639v1", "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "summary": "Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.", "authors": ["Xiaoyi Bao", "Jindi Lv", "Xiaofeng Wang", "Zheng Zhu", "Xinze Chen", "YuKun Zhou", "Jiancheng Lv", "Xingang Wang", "Guan Huang"], "published_date": "2025-06-12", "timestamp": "2025-06-14T09:15:32.696367", "title_zh": "GigaVideo-1：透過自動回饋和4 GPU-小時微調推進影片生成", "summary_zh": "GigaVideo-1 是一個高效的影片生成微調框架，無需額外的人工監督。它不依賴大量外部高品質數據，而是透過自動回饋釋放預訓練影片擴散模型的潛力。GigaVideo-1 專注於微調過程的兩個關鍵方面：數據和最佳化。它設計了一個提示驅動的數據引擎，構建多樣化、針對弱點的訓練樣本。在最佳化方面，引入了獎勵引導的訓練策略，利用預訓練的視覺語言模型的回饋，自適應地權衡樣本，並施加真實性約束。實驗表明，GigaVideo-1 僅使用 4 GPU-小時，就能在幾乎所有維度上持續提高性能，平均增益約為 4%，展現了高效性。", "applications": ["想像一下，你可以用手機App，輸入幾個關鍵字，就能自動生成一段逼真的短影片。例如，輸入「貓咪」、「彈鋼琴」，就能看到一隻可愛的貓咪優雅地演奏鋼琴的影片，分享到社群媒體上。", "未來，遊戲開發者不再需要花費大量時間和金錢製作遊戲動畫。透過這項技術，他們可以快速生成各種遊戲場景和角色動作，大幅降低開發成本，加速遊戲上市。", "教育工作者可以利用這項技術，將抽象的概念轉化為生動的影片，讓學生更容易理解。例如，講解物理學原理時，可以生成模擬實驗的影片，讓學生更直觀地學習。"], "pitch": "各位投資人，我們正處於一個影片內容爆炸性增長的時代，但高品質影片的製作成本仍然居高不下。GigaVideo-1 的出現，徹底顛覆了這一現狀。它就像影片生成的「煉金術」，僅需少量資源就能創造出令人驚豔的影片。想像一下，未來每個人都能成為影片創作者，市場潛力無可限量！從短影音平台、遊戲開發、教育娛樂到廣告行銷，GigaVideo-1 的應用範圍極其廣泛。我們預計，隨著技術的成熟，GigaVideo-1 將成為影片生成領域的基礎設施，引領下一代內容創作革命。現在投資 GigaVideo-1，就是投資影片的未來！", "audio": "docs/data/audios/2506.10639v1.wav"}
{"query": "AI", "id": "2506.10825v1", "url": "http://arxiv.org/abs/2506.10825v1", "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches", "summary": "Following the successful paradigm shift of large language models, leveraging\npre-training on a massive corpus of data and fine-tuning on different\ndownstream tasks, generalist models have made their foray into computer vision.\nThe introduction of Segment Anything Model (SAM) set a milestone on\nsegmentation of natural images, inspiring the design of a multitude of\narchitectures for medical image segmentation. In this survey we offer a\ncomprehensive and in-depth investigation on generalist models for medical image\nsegmentation. We start with an introduction on the fundamentals concepts\nunderpinning their development. Then, we provide a taxonomy on the different\ndeclinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on\nthe recent SAM 2, on other innovative models trained on images alone, and\nothers trained on both text and images. We thoroughly analyze their\nperformances at the level of both primary research and best-in-literature,\nfollowed by a rigorous comparison with the state-of-the-art task-specific\nmodels. We emphasize the need to address challenges in terms of compliance with\nregulatory frameworks, privacy and security laws, budget, and trustworthy\nartificial intelligence (AI). Finally, we share our perspective on future\ndirections concerning synthetic data, early fusion, lessons learnt from\ngeneralist models in natural language processing, agentic AI and physical AI,\nand clinical translation.", "authors": ["Andrea Moglia", "Matteo Leccardi", "Matteo Cavicchioli", "Alice Maccarini", "Marco Marcon", "Luca Mainardi", "Pietro Cerveri"], "published_date": "2025-06-12", "timestamp": "2025-06-14T12:21:02.938235", "title_zh": "醫學影像分割中的通用模型：與任務特定方法之比較研究與效能評估", "summary_zh": "大型語言模型的成功啟發了電腦視覺領域，通用模型開始嶄露頭角。Segment Anything Model (SAM) 的出現為自然影像分割樹立了里程碑，也激發了醫學影像分割架構的設計。本研究深入探討醫學影像分割的通用模型，介紹其基本概念，並對 SAM 的不同變體進行分類，包括零樣本、少樣本、微調、適配器、SAM 2，以及其他僅使用圖像或同時使用文本和圖像訓練的創新模型。我們分析了它們在主要研究和文獻中的表現，並與最先進的任務特定模型進行了嚴格比較。同時，我們強調了在法規、隱私、安全、預算和可信賴AI方面需要應對的挑戰，並分享了對合成數據、早期融合、自然語言處理通用模型經驗、代理AI、物理AI和臨床轉化的未來方向的看法。", "applications": ["**智慧醫療影像判讀：** 想像一下，醫生不用再花大量時間仔細檢查X光片、CT掃描或MRI。透過這個技術，AI能自動標記出影像中的可疑區域，例如腫瘤或骨折，讓醫生能更快、更準確地做出診斷，節省寶貴的時間，也能減少誤判的可能性。", "**手術導航與精準治療：** 手術過程中，醫生可以利用這個技術來即時定位病灶，並在複雜的器官組織中規劃出最安全的手術路徑。這就像是擁有了內建GPS的手術刀，能幫助醫生更精準地切除病灶，減少對周圍健康組織的傷害，提高手術成功率。", "**遠程醫療與居家照護：** 在偏遠地區或醫療資源不足的地方，我們可以利用這個技術來分析遠程傳輸的醫療影像，為病人提供初步的診斷建議。這也能應用於居家照護，讓AI協助監測病人的健康狀況，例如追蹤傷口癒合情況或檢測早期疾病跡象，及早發現問題並採取相應措施。"], "pitch": "各位投資人，我們帶來的是醫學影像領域的革命性技術——基於通用模型的醫學影像分割AI。想像一下，一個AI模型能夠處理各種不同的醫學影像，從X光到MRI，都能精準分割和分析，不再需要針對不同任務訓練不同的模型！\n\n這不僅大幅降低了開發成本，更提高了效率和準確性。我們的技術已經超越了傳統的任務特定模型，並具備持續學習和進化的能力。未來，我們可以將這項技術應用於遠程醫療、智慧醫院、精準手術等領域，打造一個更高效、更便捷的醫療生態系統。\n\n更令人興奮的是，隨著AI代理和物理AI的發展，我們的技術有潛力與機器人結合，實現自動化的診斷和治療。例如，AI控制的機器人可以根據醫學影像自動進行微創手術，大幅提高手術的精準度和效率。我們相信，這項技術將引領醫療產業進入一個全新的時代，為投資者帶來豐厚的回報！現在加入我們，一起開創醫學影像AI的未來吧！", "audio": "docs/data/audios/2506.10825v1.wav"}
{"query": "Foundation Model", "id": "2506.10055v1", "url": "http://arxiv.org/abs/2506.10055v1", "title": "TaskCraft: Automated Generation of Agentic Tasks", "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "published_date": "2025-06-11", "timestamp": "2025-06-14T12:22:26.952993", "title_zh": "TaskCraft：自動生成自主代理任務", "summary_zh": "TaskCraft 是一個自動化工作流程，旨在生成可擴展難度、涉及多工具且可驗證的自主代理任務，並提供執行軌跡。它透過深度和廣度擴展來擴展原子任務，創造結構和層次上複雜的挑戰。實驗結果表明，這些任務改進了生成工作流程中的提示優化，並增強了自主代理基礎模型的監督微調。我們提供了一個約 36,000 個具有不同難度的任務的大型合成數據集，以支持未來對代理調整和評估的研究。", "applications": ["智能家居控制：讓AI代理根據用戶需求，自動規劃並執行複雜的家居任務，例如根據天氣預報和用戶日程安排，自動調整室內溫度、濕度、照明和安防系統。", "個人助理：AI代理可以協助用戶處理更複雜的任務，例如規劃包含多個地點和活動的旅行行程，自動預訂機票、酒店和餐廳，並根據實時交通狀況調整行程。", "客戶服務：AI代理可以處理更複雜的客戶諮詢，例如診斷產品故障，並提供個性化的解決方案，甚至自動安排技術人員上門維修。"], "pitch": "各位投資人，我們正在開發 TaskCraft，一個革命性的AI任務生成平台。想像一下，AI不再只是執行簡單指令，而是能夠自主規劃、利用工具，解決複雜問題，就像一位超級助理！TaskCraft 能自動生成各種難度的任務，大幅降低訓練AI代理的成本，加速AI在各行各業的應用。試想，未來的工廠，AI能自主調配生產線，優化流程；未來的醫療，AI能協助醫生診斷病情，制定治療方案。TaskCraft 的潛力無限，我們正在打造AI的未來，邀請您一同參與這場變革！我們預計在三年內，TaskCraft將成為AI代理訓練領域的領先平台，並透過訂閱服務和定制化解決方案，創造巨大的商業價值。現在加入，您將成為AI革命的先鋒！", "audio": "docs/data/audios/2506.10055v1.wav"}
{"query": "Diffusion Model", "id": "2506.10633v1", "url": "http://arxiv.org/abs/2506.10633v1", "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models", "summary": "Latent Diffusion Models have shown remarkable results in text-guided image\nsynthesis in recent years. In the domain of natural (RGB) images, recent works\nhave shown that such models can be adapted to various vision-language\ndownstream tasks with little to no supervision involved. On the contrary,\ntext-to-image Latent Diffusion Models remain relatively underexplored in the\nfield of medical imaging, primarily due to limited data availability (e.g., due\nto privacy concerns). In this work, focusing on the chest X-ray modality, we\nfirst demonstrate that a standard text-conditioned Latent Diffusion Model has\nnot learned to align clinically relevant information in free-text radiology\nreports with the corresponding areas of the given scan. Then, to alleviate this\nissue, we propose a fine-tuning framework to improve multi-modal alignment in a\npre-trained model such that it can be efficiently repurposed for downstream\ntasks such as phrase grounding. Our method sets a new state-of-the-art on a\nstandard benchmark dataset (MS-CXR), while also exhibiting robust performance\non out-of-distribution data (VinDr-CXR). Our code will be made publicly\navailable.", "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "published_date": "2025-06-12", "timestamp": "2025-06-14T12:23:41.529766", "title_zh": "基於解剖學的弱監督提示調整胸腔X光潛在擴散模型", "summary_zh": "本研究針對胸腔X光影像，探討如何利用潛在擴散模型，將放射科報告中的文字資訊與影像特定區域對應起來。現有的模型在這方面表現不佳，因此我們提出了一種微調框架，能有效提升模型的多模態對齊能力。透過解剖學知識的引導，模型能更準確地將文字描述與X光片上的器官或病灶連結，進而改善疾病診斷和影像判讀的準確性。實驗結果顯示，我們的方法在標準數據集上取得了領先成果，並在分布外數據上表現出穩健性。這項技術有助於開發更精準的醫療影像分析工具。", "applications": ["想像一下，未來醫生可以對著X光片說：「標記出所有肋骨骨折的地方」，AI就能自動且精準地標示出來，節省醫生時間，減少誤判。", "如果開發出一款App，讓病人可以上傳自己的X光片，AI初步分析後提供建議，例如「疑似有肺炎跡象，建議諮詢醫生」，這能幫助民眾更了解自己的健康狀況。", "醫療教學上，學生可以利用這項技術，輸入特定疾病的描述，AI就能生成對應的X光影像，幫助學生更直觀地學習各種疾病的影像特徵。"], "pitch": "各位投資人，我們正在打造醫療影像AI的未來！現有的醫療影像分析技術往往需要大量的人工標註數據，成本高昂且效率低下。我們的技術利用創新的弱監督學習方法，結合解剖學知識，大幅降低了對人工標註的依賴，同時提升了模型的準確性和泛化能力。想像一下，一個AI醫生，能夠快速準確地分析X光片，輔助醫生診斷，甚至在偏遠地區提供遠程醫療服務。這不僅能降低醫療成本，提高診斷效率，更能拯救無數生命！我們預計，未來這項技術將廣泛應用於疾病篩查、診斷輔助、藥物研發等領域，市場潛力巨大。現在加入我們，您將成為醫療AI革命的領跑者！", "audio": "docs/data/audios/2506.10633v1.wav"}
{"query": "AI", "id": "2506.10785v1", "url": "http://arxiv.org/abs/2506.10785v1", "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "published_date": "2025-06-12", "timestamp": "2025-06-14T15:11:55.172883", "title_zh": "使用者重視與批評之處：AI行動應用使用者回饋的大規模分析", "summary_zh": "本研究針對Google Play上292款AI行動應用，分析近90萬則使用者評論，旨在了解使用者對AI功能的看法。研究團隊開發一套多階段分析流程，運用大型語言模型，精準萃取評論中的觀點與情感。分析結果顯示，使用者正面評價集中在生產力、可靠性和個人化協助，負面評價則多為技術故障、價格問題和語言支援不足。這套流程能同時捕捉同一評論中的不同情感，提供更細緻的使用者體驗分析，揭示各類應用程式的通用滿意度和特定領域的痛點，為開發者提供寶貴的參考。", "applications": ["語音轉文字App：可以分析使用者對於語音辨識準確度的回饋，快速找出哪些口音或語速的辨識效果不佳，進而改善演算法。", "AI繪圖App：透過分析使用者評論，了解使用者喜歡的繪圖風格、對生成圖片的滿意度，以及是否有出現不符預期的結果，協助開發者調整模型，生成更符合使用者需求的圖片。", "AI健身App：分析使用者對於運動計畫、追蹤功能的評價，找出使用者覺得太難、太簡單或不夠個人化的部分，進而優化運動建議，提升使用者黏著度。"], "pitch": "各位創投先進，想像一下，現在每個人手機裡都有好幾個AI App，但開發者卻像在盲人摸象，不知道使用者真正想要什麼！我們的技術就像一台精密的『使用者情緒探測器』，能深入挖掘海量評論，精準定位使用者痛點與喜好。這不僅能大幅降低開發成本，更能打造出真正『命中紅心』的AI產品。例如，我們可以協助教育App開發者打造出讓學生『愛不釋手』的AI家教，也可以協助電商App開發者打造出讓消費者『忍不住剁手』的AI推薦系統。未來，我們甚至可以將這項技術應用於分析醫療數據、預測金融市場，潛力無限！現在投資我們，就是投資AI的未來，讓我們一起引爆下一波AI革命！", "audio": "docs/data/audios/2506.10785v1.wav"}
{"query": "Foundation Model", "id": "2506.09623v1", "url": "http://arxiv.org/abs/2506.09623v1", "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "published_date": "2025-06-11", "timestamp": "2025-06-14T15:13:10.593168", "title_zh": "解析型任務排程器：基於遞迴最小平方法的具身基礎模型持續學習方法", "summary_zh": "本研究提出解析型任務排程器（ATS），旨在解決具身基礎模型在持續學習中遇到的災難性遺忘問題。ATS透過建立任務專屬模型庫，並利用遞迴最小平方法訓練解析型排程器，學習語言指令與任務模型之間的映射關係。這種架構能精準識別任務，動態選擇模型，並從根本上避免任務間的參數干擾。排程器僅使用統計數據增量更新參數，無需重訪歷史數據即可實現抗遺忘學習。在真實機器人平台上的驗證表明，ATS在抗遺忘和適應任務變化方面表現出色，為具身基礎模型在複雜動態環境中的持續學習提供了一種高效、可擴展且可部署的解決方案。", "applications": ["智慧家庭管家：想像一下，一個機器人管家可以不斷學習新的家務技能，像是煮新的菜色、整理不同的物品，而且不會忘記之前學過的技能，永遠都能幫你把家裡打理得井井有條。", "客製化醫療照護：醫療機器人可以透過學習不同的照護流程，為病人提供更個人化的服務。例如，它可以學習如何協助病人復健、監測生命體徵，並根據病人的需求調整照護方式，而且不會忘記病人的病史和特殊需求。", "工廠自動化彈性生產：工廠裡的機器人可以快速學習執行新的生產任務，例如組裝新的產品、調整生產線配置，而且不會忘記舊的生產流程，讓工廠能更靈活地應對市場變化。"], "pitch": "各位投資人，我們正在打造下一代的AI機器人，它們將擁有像人類一樣的持續學習能力！我們的解析型任務排程器（ATS）技術，能讓機器人在不斷學習新技能的同時，不會忘記舊技能，徹底解決了傳統AI機器人「學了新的，忘了舊的」的痛點。想像一下，未來的工廠、醫院、家庭，都將充滿著能不斷進化、自我完善的AI機器人，它們能執行更複雜、更精確的任務，大幅提高生產效率和生活品質。這不僅僅是一項技術突破，更是一個千億美元級的市場機會！我們相信，ATS將成為具身智能領域的Game Changer，引領AI機器人走向更廣闊的應用前景。現在加入我們，一起開創AI機器人的新紀元！", "audio": "docs/data/audios/2506.09623v1.wav"}
{"query": "Diffusion Model", "id": "2506.10632v1", "url": "http://arxiv.org/abs/2506.10632v1", "title": "Hessian Geometry of Latent Space in Generative Models", "summary": "This paper presents a novel method for analyzing the latent space geometry of\ngenerative models, including statistical physics models and diffusion models,\nby reconstructing the Fisher information metric. The method approximates the\nposterior distribution of latent variables given generated samples and uses\nthis to learn the log-partition function, which defines the Fisher metric for\nexponential families. Theoretical convergence guarantees are provided, and the\nmethod is validated on the Ising and TASEP models, outperforming existing\nbaselines in reconstructing thermodynamic quantities. Applied to diffusion\nmodels, the method reveals a fractal structure of phase transitions in the\nlatent space, characterized by abrupt changes in the Fisher metric. We\ndemonstrate that while geodesic interpolations are approximately linear within\nindividual phases, this linearity breaks down at phase boundaries, where the\ndiffusion model exhibits a divergent Lipschitz constant with respect to the\nlatent space. These findings provide new insights into the complex structure of\ndiffusion model latent spaces and their connection to phenomena like phase\ntransitions. Our source code is available at\nhttps://github.com/alobashev/hessian-geometry-of-diffusion-models.", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "published_date": "2025-06-12", "timestamp": "2025-06-14T15:14:32.206469", "title_zh": "生成模型潛在空間的黑森幾何", "summary_zh": "本研究提出一種新方法，透過重建費雪資訊度量來分析生成模型的潛在空間幾何結構，包含統計物理模型和擴散模型。此方法近似生成樣本的潛在變數後驗分佈，並藉此學習對數配分函數，定義指數族的費雪度量。經驗證，在Ising和TASEP模型上，此方法在重建熱力學量方面優於現有基準。應用於擴散模型時，揭示了潛在空間中相變的分形結構，其特徵是費雪度量的突變。研究表明，測地線插值在各個相中大致呈線性，但在相界處，這種線性關係會崩潰，擴散模型呈現相對於潛在空間的發散Lipschitz常數。這些發現為擴散模型潛在空間的複雜結構及其與相變等現象的聯繫提供了新的見解。", "applications": ["AI藝術創作：透過理解潛在空間的結構，可以更精確地控制AI生成的圖像風格和內容，例如讓AI畫家能更自然地融合不同風格，創造出獨一無二的藝術作品。", "新藥開發：生成模型可以用於設計新的分子結構。了解潛在空間的幾何特性，能幫助科學家更有效地探索潛在的藥物候選分子，加速新藥開發過程。", "材料科學：類似於新藥開發，生成模型也能協助設計具有特定物理或化學性質的新材料。理解潛在空間的相變，有助於發現性能飛躍的新材料組合。"], "pitch": "想像一下，我們能精準操控AI的創造力，就像控制水龍頭一樣！這項技術的核心在於解開生成模型潛在空間的秘密，就像替AI繪製一張詳細的藏寶圖。我們不僅能更精準地生成圖像、音樂，更能應用於新藥開發、材料科學等領域，大幅縮短研發時程，降低成本。更令人興奮的是，我們發現了潛在空間中的『相變』現象，這意味著我們可以透過微小的改變，觸發AI生成結果的巨大變化，就像鍊金術一樣！未來，這項技術將成為各行各業AI應用的基石，從個人化的AI助理到突破性的科學發現，都將因它而加速。我們正在打造的是一個全新的AI時代，而您現在有機會成為這個時代的領航者！", "audio": "docs/data/audios/2506.10632v1.wav"}
{"query": "AI", "id": "2506.10751v1", "url": "http://arxiv.org/abs/2506.10751v1", "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering", "summary": "Automated question answering (QA) over electronic health records (EHRs) can\nbridge critical information gaps for clinicians and patients, yet it demands\nboth precise evidence retrieval and faithful answer generation under limited\nsupervision. In this work, we present Neural, the runner-up in the BioNLP 2025\nArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method\ndecouples the task into (1) sentence-level evidence identification and (2)\nanswer synthesis with explicit citations. For each stage, we automatically\nexplore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning\ninstructions and few-shot demonstrations on the development set. A\nself-consistency voting scheme further improves evidence recall without\nsacrificing precision. On the hidden test set, our method attains an overall\nscore of 51.5, placing second stage while outperforming standard zero-shot and\nfew-shot prompting by over 20 and 10 points, respectively. These results\nindicate that data-driven prompt optimization is a cost-effective alternative\nto model fine-tuning for high-stakes clinical QA, advancing the reliability of\nAI assistants in healthcare.", "authors": ["Sai Prasanna Teja Reddy Bogireddy", "Abrar Majeedi", "Viswanatha Reddy Gajjala", "Zhuoyan Xu", "Siddhant Rai", "Vaishnav Potlapalli"], "published_date": "2025-06-12", "timestamp": "2025-06-14T21:12:04.265649", "title_zh": "Neural於ArchEHR-QA 2025：基於代理的提示優化，用於證據導向的臨床問答", "summary_zh": "本研究提出名為Neural的系統，在BioNLP 2025 ArchEHR-QA競賽中獲得亞軍，專注於電子病歷的自動問答。該系統將任務分解為兩個階段：一是句子層級的證據識別，二是帶有明確引用的答案合成。研究利用DSPy的MIPROv2優化器自動探索提示空間，在開發集上聯合調整指令和少量範例。並採用自洽性投票機制，在不犧牲精確度的前提下，進一步提高證據檢索率。實驗結果顯示，該方法在隱藏測試集上取得了優異成績，證明數據驅動的提示優化是高風險臨床問答中，一種經濟有效的替代方案，能提升醫療保健AI助手的可靠性。", "applications": ["**個人化健康建議：** 當你輸入症狀或疑慮時，系統能快速從你的電子病歷中找到相關資訊，並提供個人化的建議，例如：『根據你的病史，這個藥物可能更適合你，但請諮詢醫生』。", "**醫生診斷輔助：** 醫生在看診時，可以透過這個系統快速查閱病人的完整病歷，找出關鍵資訊，輔助診斷，例如：『這位病人過去有類似的症狀，當時的診斷是…』，減少診斷錯誤。", "**藥物交互作用檢查：** 當醫生開立新藥時，系統能自動檢查該藥物是否會與病人正在服用的其他藥物產生交互作用，並提出警告，例如：『這個藥物可能會與你目前服用的降血壓藥產生交互作用，請告知醫生』。"], "pitch": "各位投資人，想像一下，一個能夠精準解讀電子病歷，並提供醫生和病人可靠建議的AI助手，這不僅僅是技術突破，更是醫療領域的革命！我們的Neural系統，透過創新的提示優化技術，在高風險的臨床問答中展現了卓越的性能，遠超傳統方法。這意味著更準確的診斷、更安全的治療方案，以及更高的醫療效率。試想，將這項技術應用於遠程醫療，讓偏遠地區的居民也能獲得頂尖醫療專家的建議；或者將其整合到智慧手錶等穿戴裝置中，實現24小時的健康監測和預警。這是一個千億美元級別的市場，而Neural正是開啟這扇大門的鑰匙。我們正在尋找有遠見的投資者，共同打造醫療AI的未來，讓人人都能享有更健康、更長壽的生活！", "audio": "docs/data/audios/2506.10751v1.wav"}
{"query": "Foundation Model", "id": "2506.09593v1", "url": "http://arxiv.org/abs/2506.09593v1", "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "published_date": "2025-06-11", "timestamp": "2025-06-14T21:13:32.956263", "title_zh": "超越過度自信：基礎模型重新定義深度神經網路的校準", "summary_zh": "深度神經網路的可靠校準對於高風險應用至關重要。過去的研究表明，深度學習模型通常表現出過度自信，尤其是在資料分布發生變化時。本研究深入探討了ConvNeXt、EVA和BEiT等基礎模型的校準特性，發現它們在原始資料分布中反而傾向於低度自信，但在資料分布變化時校準效果有所提升。此外，這些模型對事後校準技術反應良好，但在極端分布變化下效果會變差。這項研究挑戰了傳統觀念，表明架構和訓練創新對模型校準的影響並非單調遞增。", "applications": ["**醫療診斷輔助：** 想像一下，醫生使用AI診斷X光片。如果AI能準確告知自己判斷的信心程度，醫生就能更有效地判斷哪些病例需要進一步檢查，避免誤診或漏診。", "**自動駕駛系統：** 自動駕駛汽車在遇到突發狀況時，需要準確判斷前方物體的種類與距離。如果AI能明確表達自己對判斷的信心，系統就能根據信心程度採取不同程度的應急措施，例如減速、變道或緊急停車，提高行車安全。", "**金融風險評估：** 銀行在審核貸款申請時，可以使用AI模型評估借款人的信用風險。如果AI能告知自己預測的信心程度，銀行就能更精確地調整貸款利率或額度，降低壞帳風險。"], "pitch": "各位創投，我們正在重新定義AI的信任度！傳統深度學習模型常有過度自信的問題，在高風險領域應用時隱藏巨大風險。我們的研究發現，新一代基礎模型在校準方面展現出獨特優勢，能更準確地表達預測的信心程度。這意味著什麼？更安全、更可靠的AI應用！想像一下，在自動駕駛、醫療診斷、金融風控等領域，因為有了更精準的AI信心評估，能大幅降低事故率、誤診率和壞帳率。這不僅僅是技術提升，更是對社會的巨大貢獻。我們正在開發基於此項技術的校準工具包，能輕鬆整合到現有的AI系統中，提升其可靠性。預計未來AI信任度市場將達到數十億美元規模，而我們將成為這個市場的領頭羊！現在加入我們，一起打造更值得信賴的AI未來！", "audio": "docs/data/audios/2506.09593v1.wav"}
{"query": "Diffusion Model", "id": "2506.10612v1", "url": "http://arxiv.org/abs/2506.10612v1", "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "summary": "We present TexTailor, a novel method for generating consistent object\ntextures from textual descriptions. Existing text-to-texture synthesis\napproaches utilize depth-aware diffusion models to progressively generate\nimages and synthesize textures across predefined multiple viewpoints. However,\nthese approaches lead to a gradual shift in texture properties across\nviewpoints due to (1) insufficient integration of previously synthesized\ntextures at each viewpoint during the diffusion process and (2) the\nautoregressive nature of the texture synthesis process. Moreover, the\npredefined selection of camera positions, which does not account for the\nobject's geometry, limits the effective use of texture information synthesized\nfrom different viewpoints, ultimately degrading overall texture consistency. In\nTexTailor, we address these issues by (1) applying a resampling scheme that\nrepeatedly integrates information from previously synthesized textures within\nthe diffusion process, and (2) fine-tuning a depth-aware diffusion model on\nthese resampled textures. During this process, we observed that using only a\nfew training images restricts the model's original ability to generate\nhigh-fidelity images aligned with the conditioning, and therefore propose an\nperformance preservation loss to mitigate this issue. Additionally, we improve\nthe synthesis of view-consistent textures by adaptively adjusting camera\npositions based on the object's geometry. Experiments on a subset of the\nObjaverse dataset and the ShapeNet car dataset demonstrate that TexTailor\noutperforms state-of-the-art methods in synthesizing view-consistent textures.\nThe source code for TexTailor is available at\nhttps://github.com/Adios42/Textailor", "authors": ["Suin Lee", "Dae-Shik Kim"], "published_date": "2025-06-12", "timestamp": "2025-06-14T21:14:43.673790", "title_zh": "TexTailor：透過有效重採樣實現客製化的文字對齊紋理生成", "summary_zh": "TexTailor是一種創新的方法，能從文字描述生成一致的物件紋理。現有的技術常因視角間紋理屬性的漸進式變化，以及相機位置的預先定義限制，導致紋理一致性不佳。TexTailor透過在擴散過程中重複整合先前合成的紋理資訊，以及基於物件幾何結構調整相機位置來解決這些問題。此外，我們也提出了一種性能保留損失，以緩解模型在高保真圖像生成方面的能力限制。實驗結果表明，TexTailor在合成視角一致的紋理方面優於現有技術。", "applications": ["想像一下，你可以用文字描述你想要的鞋子外觀，例如『紅色皮革、火焰圖案』，TexTailor就能立即生成逼真的鞋子紋理圖案，讓設計師或消費者預覽成品效果。", "遊戲開發者可以使用TexTailor快速為遊戲角色或場景創建獨特的紋理。只要輸入文字描述，例如『古老的石牆、爬滿常春藤』，就能自動生成符合描述的紋理，大幅節省美術設計時間。", "電商平台可以利用TexTailor讓消費者客製化商品。例如，消費者可以輸入『藍色條紋、姓名縮寫』來設計手機殼，並即時預覽效果，提升購物體驗和個性化需求。"], "pitch": "各位投資人，我們正在開發TexTailor，這是一款革命性的AI紋理生成工具，它將徹底改變3D內容創作的方式！想像一下，一個設計師不再需要耗費數小時手動繪製紋理，只需輸入文字描述，TexTailor就能在幾秒鐘內生成高品質、視角一致的紋理。這不僅能大幅降低成本，更能加速產品上市時間。市場潛力巨大！從遊戲、電影、電商，到建築設計、虛擬實境，任何需要3D內容的產業都將是我們的客戶。我們預計，隨著元宇宙的發展，對客製化3D物件的需求將呈指數級增長，而TexTailor將成為這個市場的領頭羊。我們的技術優勢在於獨特的重採樣和幾何感知方法，能確保紋理在不同視角下的一致性，這是其他競爭對手難以企及的。我們正在申請專利，並積極尋求戰略合作夥伴。現在加入我們，一起打造3D內容創作的未來！", "audio": "docs/data/audios/2506.10612v1.wav"}
{"query": "AI", "id": "2506.10674v1", "url": "http://arxiv.org/abs/2506.10674v1", "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "authors": ["Vincenzo Colle", "Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Fadhel Ayed", "Merouane Debbah"], "published_date": "2025-06-12", "timestamp": "2025-06-15T01:02:03.459049", "title_zh": "TeleMath：電信數學問題求解中大型語言模型的基準測試", "summary_zh": "隨著人工智慧在電信領域的應用日益普及，大型語言模型(LLM)解決電信領域特定數學密集型任務的能力備受關注。為評估LLM在此領域的表現，我們推出了TeleMath，這是第一個專門用於評估LLM在解決電信領域數值解數學問題的基準數據集，包含500個問答對，涵蓋電信領域的廣泛主題。通過對各種開源LLM的評估表明，專為數學或邏輯推理設計的最新模型在TeleMath上表現最佳。而通用模型，即使具有大量參數，也常常難以應對這些挑戰。我們已發布數據集和評估代碼，以簡化結果的可重複性並支持未來的研究。", "applications": ["想像一下，你可以用手機App快速計算出最佳訊號接收位置，不再需要到處亂跑找訊號，這就是TeleMath技術的應用。", "電信公司可以利用這個技術，自動優化基地台的配置，確保在人口密集的區域也能提供穩定的網路服務，讓大家追劇、視訊會議不卡頓。", "未來，無人機可以利用TeleMath技術，在災區快速建立臨時通訊網路，協助救援人員進行搜救工作，爭取黃金救援時間。"], "pitch": "各位投資人，我們正處於電信產業AI化的浪潮之巔！TeleMath不僅是一個基準測試，更是一個解鎖電信領域龐大數據價值的鑰匙。想像一下，一個能自動優化網路、預測流量、甚至設計下一代通訊協定的AI大腦。TeleMath讓LLM具備了這種能力，它能協助電信業者節省數十億美元的運營成本，並開創全新的服務模式。例如，針對特定用戶群體提供客製化的網路解決方案，或者在物聯網(IoT)設備激增的情況下，智能分配網路資源。更進一步，我們可以將TeleMath應用於衛星通訊、5G/6G網路的部署，甚至太空探索領域的通訊系統設計。這不僅僅是一個項目，而是一個能重新定義電信產業的未來，潛力無可限量！現在加入我們，一起打造電信AI的黃金時代！", "audio": "docs/data/audios/2506.10674v1.wav"}
{"query": "Foundation Model", "id": "2506.09453v1", "url": "http://arxiv.org/abs/2506.09453v1", "title": "From Partial to Monadic: Combinatory Algebra with Effects", "summary": "Partial Combinatory Algebras (PCAs) provide a foundational model of the\nuntyped $\\lambda$-calculus and serve as the basis for many notions of\ncomputability, such as realizability theory. However, PCAs support a very\nlimited notion of computation by only incorporating non-termination as a\ncomputational effect. To provide a framework that better internalizes a wide\nrange of computational effects, this paper puts forward the notion of Monadic\nCombinatory Algebras (MCAs). MCAs generalize the notion of PCAs by structuring\nthe combinatory algebra over an underlying computational effect, embodied by a\nmonad. We show that MCAs can support various side effects through the\nunderlying monad, such as non-determinism, stateful computation and\ncontinuations. We further obtain a categorical characterization of MCAs within\nFreyd Categories, following a similar connection for PCAs. Moreover, we explore\nthe application of MCAs in realizability theory, presenting constructions of\neffectful realizability triposes and assemblies derived through evidenced\nframes, thereby generalizing traditional PCA-based realizability semantics. The\nmonadic generalization of the foundational notion of PCAs provides a\ncomprehensive and powerful framework for internally reasoning about effectful\ncomputations, paving the path to a more encompassing study of computation and\nits relationship with realizability models and programming languages.", "authors": ["Liron Cohen", "Ariel Grunfeld", "Dominik Kirst", "Étienne Miquey"], "published_date": "2025-06-11", "timestamp": "2025-06-15T01:03:20.984062", "title_zh": "從偏函數到單子：具備副作用的組合代數", "summary_zh": "本研究提出「單子組合代數」(MCAs)，擴展了傳統「偏組合代數」(PCAs) 的概念，使其能更好地處理各種計算副作用，例如不確定性、狀態計算和延續性。MCAs透過單子(monad)來結構化組合代數，從而整合了這些副作用。我們在Freyd範疇中對MCAs進行了範疇刻劃，並展示了其在可實現性理論中的應用，推廣了基於PCA的可實現性語義。這種單子泛化為更全面地研究具副作用的計算、可實現性模型及程式語言之間的關係，奠定了基礎。", "applications": ["在網頁遊戲開發中，可以用MCAs來管理遊戲狀態，例如玩家的生命值、道具欄等，確保遊戲邏輯的正確性和一致性，即使在網路延遲或錯誤發生時也能正常運行。", "在金融交易系統中，MCAs可以處理交易的原子性，確保交易要么完全成功，要么完全失敗，避免因部分成功導致的資金損失或數據不一致。", "在人工智慧的決策系統中，MCAs可以模擬不確定性，例如天氣預報、股票市場波動等，幫助系統做出更穩健的決策。"], "pitch": "各位投資人，我們正在開發一項突破性的技術，名為「單子組合代數」(MCAs)。想像一下，您的軟體可以更安全、更可靠地處理各種複雜的計算，不再因為小小的副作用而崩潰。MCAs就像是程式碼的超級防護罩，能讓您的應用程式在各種意想不到的情況下都能穩定運行。這項技術不僅能大幅降低軟體開發和維護成本，還能應用於金融科技、人工智慧、區塊鏈等高成長領域，孕育出全新的商業模式。例如，我們可以開發出更安全的智能合約，或者更可靠的自動駕駛系統。現在正是投資MCAs的最佳時機，讓我們一起打造更安全、更智能的未來！", "audio": "docs/data/audios/2506.09453v1.wav"}
{"query": "Diffusion Model", "id": "2506.10605v1", "url": "http://arxiv.org/abs/2506.10605v1", "title": "High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model", "summary": "We present LatentCSI, a novel method for generating images of the physical\nenvironment from WiFi CSI measurements that leverages a pretrained latent\ndiffusion model (LDM). Unlike prior approaches that rely on complex and\ncomputationally intensive techniques such as GANs, our method employs a\nlightweight neural network to map CSI amplitudes directly into the latent space\nof an LDM. We then apply the LDM's denoising diffusion model to the latent\nrepresentation with text-based guidance before decoding using the LDM's\npretrained decoder to obtain a high-resolution image. This design bypasses the\nchallenges of pixel-space image generation and avoids the explicit image\nencoding stage typically required in conventional image-to-image pipelines,\nenabling efficient and high-quality image synthesis. We validate our approach\non two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi\ndevices and cameras; and a subset of the publicly available MM-Fi dataset. The\nresults demonstrate that LatentCSI outperforms baselines of comparable\ncomplexity trained directly on ground-truth images in both computational\nefficiency and perceptual quality, while additionally providing practical\nadvantages through its unique capacity for text-guided controllability.", "authors": ["Eshan Ramesh", "Nishio Takayuki"], "published_date": "2025-06-12", "timestamp": "2025-06-15T01:04:31.961963", "title_zh": "基於預訓練潛在擴散模型，從WiFi CSI產生高解析度且高效的圖像", "summary_zh": "LatentCSI是一種創新的方法，利用預訓練的潛在擴散模型（LDM），從WiFi CSI測量數據生成物理環境的圖像。它使用輕量級神經網路將CSI幅度直接映射到LDM的潛在空間，避免了像素空間的圖像生成，也無需傳統圖像到圖像流程中顯式的圖像編碼階段，從而實現高效且高品質的圖像合成。通過文字引導，LatentCSI可以靈活控制圖像生成，並在計算效率和感知品質上優於其他方法。實驗證明，它在我們收集的寬頻CSI數據集和公開的MM-Fi數據集上都表現出色。", "applications": ["想像一下，你走到一個陌生的房間，手機掃描一下WiFi訊號，就能立刻生成房間的3D模型，方便你快速了解環境，再也不用擔心迷路或找不到東西了。", "家裡的WiFi訊號突然變弱，透過這個技術，我們可以分析訊號變化，生成家裡障礙物的圖像，快速找出訊號被遮蔽的原因，輕鬆解決WiFi問題。", "警察在犯罪現場，只要掃描WiFi訊號，就能重建案發現場的圖像，即使沒有監視器也能幫助還原事情經過，提升破案效率。"], "pitch": "各位投資人，LatentCSI是一項顛覆性的技術，它將WiFi訊號轉化為高解析度的環境圖像，開啟了無限可能。想像一下，結合AIoT和智慧城市應用，我們可以打造更安全、更便利的生活。例如，智慧安防系統可以透過WiFi訊號監控異常行為並生成警報；零售業可以利用顧客WiFi數據分析店內人流並優化商品擺放。更重要的是，這項技術具有極高的可擴展性，未來甚至可以應用於自動駕駛、無人機導航等領域。我們擁有領先的技術優勢和龐大的市場潛力，現在正是投資LatentCSI的最佳時機，讓我們一起開創WiFi圖像應用的新時代！", "audio": "docs/data/audios/2506.10605v1.wav"}
{"query": "AI", "id": "2506.10673v1", "url": "http://arxiv.org/abs/2506.10673v1", "title": "Reaching the Ultimate Quantum Precision Limit at Colliders: Conditions and Case Studies", "summary": "We investigate whether collider experiments can reach the quantum limit of\nprecision, defined by the quantum Fisher information (QFI), using only\nclassical observables such as particle momenta. As a case study, we focus on\nthe $\\tau^+\\tau^-$ system and the decay channel $\\tau \\to \\pi \\nu$, which\noffers maximal spin-analyzing power and renders the decay a projective\nmeasurement. We develop a general framework to determine when collider\nmeasurements can, in principle, saturate the QFI in an entangled biparticle\nsystem, and this framework extends naturally to other such systems. Within this\nframework, QFI saturation occurs if and only if the symmetric logarithmic\nderivative (SLD) commutes with a complete set of orthonormal separable\nprojectors associated with collider-accessible measurements. This separability\ncondition, reflecting the independence of decay amplitudes, is highly\nnontrivial. To meet this condition, a key requirement is that the spin density\nmatrix be rank-deficient, allowing the SLD sufficient freedom. We show that the\nclassical Fisher information asymptotically saturates the QFI for magnetic\ndipole moments and CP-violating Higgs interactions in selected phase-space\nregions, but not for electric dipole moments. These results bridge quantum\nmetrology and collider physics, providing a systematic method to identify\nquantum-optimal sensitivity in collider experiments.", "authors": ["Tengyu Ai", "Qi Bi", "Yuxin He", "Jia Liu", "Xiao-Ping Wang"], "published_date": "2025-06-12", "timestamp": "2025-06-15T06:17:23.916384", "title_zh": "在對撞機中達到極限量子精度：條件與案例研究", "summary_zh": "本研究探討對撞機實驗是否能僅使用粒子動量等古典可觀測量，達到量子精度極限，即量子費雪資訊（QFI）。以$\\\\\\tau^+\\\\\\\\\\tau^-$系統和$\\\\\\\\\\tau \\\\\\\\[右箭頭] \\\\\\\\[pi] \\\\\\\\[nu]$衰變通道為例，該通道提供最大的自旋分析能力，並使衰變成為投影測量。我們開發了一個通用框架，以確定對撞機測量在原則上何時可以飽和糾纏雙粒子系統中的QFI。結果表明，自旋密度矩陣的秩虧是關鍵，允許SLD有足夠的自由度。研究發現，在選定的相空間區域中，古典費雪資訊漸近地飽和了磁偶極矩和違反CP對稱性的希格斯相互作用的QFI，但電偶極矩則不然。這些結果連接了量子計量學和對撞機物理學，提供了一種系統的方法來識別對撞機實驗中的量子最佳靈敏度。", "applications": ["醫學影像增強：更精準的量子測量技術可以應用於醫學影像，例如MRI或CT掃描，提高影像的清晰度和解析度，從而更早、更準確地診斷疾病。", "精密導航：利用量子精確度進行導航，可以大幅提升導航系統的準確性，即使在GPS訊號微弱或受干擾的環境下，也能提供可靠的定位資訊，應用於無人機、自動駕駛等領域。", "材料科學：在材料科學中，可以利用這種技術更精確地測量材料的微觀性質，例如磁性、電學特性等，從而開發出性能更優異的新材料。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它將量子計量學的精確性帶入粒子對撞機實驗，並將其應用於更廣泛的領域。想像一下，我們可以以前所未有的精度測量亞原子粒子的特性，這不僅能幫助我們更深入地了解宇宙的奧秘，還將催生一系列顛覆性應用。例如，在醫學診斷方面，我們的技術可以實現超高解析度的醫學影像，早期發現癌症等疾病。在材料科學領域，我們可以加速新材料的開發，例如超導材料或更高效的太陽能電池。最重要的是，我們的方法具有高度的可擴展性，可以應用於各種不同的測量場景。我們相信，通過您的投資，我們可以將這項技術從實驗室推向市場，創造巨大的商業價值，並為人類社會帶來深遠的影響。現在投資，您將成為這場量子技術革命的先驅！未來，我們甚至可能利用此技術開發出基於量子精準度的全新感測器，應用於環境監測、國防安全等領域，市場潛力無限。", "audio": "docs/data/audios/2506.10673v1.wav"}
{"query": "AI", "id": "2506.10627v1", "url": "http://arxiv.org/abs/2506.10627v1", "title": "NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors", "summary": "This paper presents our system for Track 1: Mistake Identification in the BEA\n2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The\ntask involves evaluating whether a tutor's response correctly identifies a\nmistake in a student's mathematical reasoning. We explore four approaches: (1)\nan ensemble of machine learning models over pooled token embeddings from\nmultiple pretrained language models (LMs); (2) a frozen sentence-transformer\nusing [CLS] embeddings with an MLP classifier; (3) a history-aware model with\nmulti-head attention between token-level history and response embeddings; and\n(4) a retrieval-augmented few-shot prompting system with a large language model\n(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,\nconstructs structured prompts, and uses schema-guided output parsing to produce\ninterpretable predictions. It outperforms all baselines, demonstrating the\neffectiveness of combining example-driven prompting with LLM reasoning for\npedagogical feedback assessment. Our code is available at\nhttps://github.com/NaumanNaeem/BEA_2025.", "authors": ["Numaan Naeem", "Sarfraz Ahmad", "Momina Ahsan", "Hasan Iqbal"], "published_date": "2025-06-12", "timestamp": "2025-06-15T09:12:54.914670", "title_zh": "NeuralNexus於BEA 2025共享任務：檢索增強提示在AI家教中錯誤識別的應用", "summary_zh": "本研究介紹NeuralNexus系統，用於評估AI家教在數學推理中識別學生錯誤的能力。我們結合了多種方法，包括集成機器學習模型、凍結句子轉換器、歷史感知模型，以及檢索增強的少量樣本提示系統，採用大型語言模型GPT-4o。我們的系統通過檢索語義相似的範例，構建結構化提示，並使用模式引導的輸出解析，產生可解釋的預測。實驗結果表明，該系統超越了所有基準線，證明了基於範例的提示與LLM推理相結合在教學反饋評估中的有效性。該技術能有效提升AI家教的教學品質與效率。", "applications": ["AI家教：當孩子使用AI家教學習數學時，系統能自動偵測並指出孩子在解題過程中的錯誤，並給予適當的引導，就像一位隨時在旁的專業家教。", "程式碼偵錯：程式設計師在寫程式時，AI能協助找出程式碼中的bug，並提供修改建議，大幅縮短除錯時間，提升開發效率。", "醫療診斷輔助：醫生在診斷病情時，AI能根據病患的症狀，提供可能的疾病診斷方向，並提醒醫生注意潛在的風險，協助醫生做出更精確的判斷。"], "pitch": "各位投資人，想像一下，未來的教育不再只是單向的知識灌輸，而是高度個人化、即時反饋的學習體驗。我們的NeuralNexus系統，正是實現這一願景的關鍵。透過結合檢索增強提示與大型語言模型，我們的技術能精準識別學習過程中的錯誤，並提供客製化的指導，徹底顛覆傳統教育模式。這不僅能提升學習效率，更能激發學生的學習興趣。試想，將這項技術應用於程式教育、醫療培訓等領域，市場潛力無可限量！我們相信，NeuralNexus將成為AI教育領域的領頭羊，為投資者帶來豐厚的回報。現在加入我們，共同開創AI賦能教育的黃金時代！", "audio": "docs/data/audios/2506.10627v1.wav"}
{"query": "Foundation Model", "id": "2506.09440v1", "url": "http://arxiv.org/abs/2506.09440v1", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "published_date": "2025-06-11", "timestamp": "2025-06-15T09:14:06.503322", "title_zh": "GigaChat家族：透過混合專家架構實現高效的俄語語言建模", "summary_zh": "GigaChat家族是一系列針對俄語設計的大型語言模型，包含基礎模型和指令調整版本。由於開發俄語專用模型需要大量計算資源，因此相關研究較少。本研究詳細介紹了GigaChat的模型架構、預訓練過程和實驗結果，並在俄語和英語基準測試中評估了其性能，與多語言模型進行比較。GigaChat提供API、Telegram機器人和Web界面等訪問方式，並已開源部分模型，旨在促進俄語自然語言處理研究和產業應用。", "applications": ["俄語學習App：GigaChat可以作為你的俄語老師，隨時提供語法、詞彙的解釋，甚至可以和你進行角色扮演，模擬真實的俄語對話情境。", "俄語新聞摘要：快速將冗長的俄語新聞文章濃縮成重點摘要，讓你輕鬆掌握俄羅斯和國際時事。", "俄語客服機器人：取代真人客服，24小時不間斷地為俄語使用者提供產品諮詢和技術支援，大幅降低企業的人力成本。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！GigaChat家族不僅僅是一個俄語大型語言模型，更是開啟俄語市場的鑰匙。試想一下，一個能流利、自然地用俄語溝通的AI，它能做什麼？它可以是俄語世界的Siri或Alexa，為數百萬使用者提供個性化服務；它可以是企業進軍俄語市場的得力助手，翻譯文件、撰寫行銷文案、處理客戶服務。更重要的是，GigaChat家族是開源的，這意味著無數開發者可以基於它創造出更多令人驚豔的應用。我們預計，隨著俄語網路內容的爆炸式增長，對高品質俄語語言模型的需求將會持續攀升。投資GigaChat，就是投資俄語AI的未來，您將獲得巨大的商業回報！", "audio": "docs/data/audios/2506.09440v1.wav"}
{"query": "Diffusion Model", "id": "2506.10532v1", "url": "http://arxiv.org/abs/2506.10532v1", "title": "Equivariant Neural Diffusion for Molecule Generation", "summary": "We introduce Equivariant Neural Diffusion (END), a novel diffusion model for\nmolecule generation in 3D that is equivariant to Euclidean transformations.\nCompared to current state-of-the-art equivariant diffusion models, the key\ninnovation in END lies in its learnable forward process for enhanced generative\nmodelling. Rather than pre-specified, the forward process is parameterized\nthrough a time- and data-dependent transformation that is equivariant to rigid\ntransformations. Through a series of experiments on standard molecule\ngeneration benchmarks, we demonstrate the competitive performance of END\ncompared to several strong baselines for both unconditional and conditional\ngeneration.", "authors": ["François Cornet", "Grigory Bartosh", "Mikkel N. Schmidt", "Christian A. Naesseth"], "published_date": "2025-06-12", "timestamp": "2025-06-15T09:15:21.842971", "title_zh": "等變神經擴散模型用於分子生成", "summary_zh": "我們提出了一種名為「等變神經擴散」(END) 的全新擴散模型，用於在3D空間中生成分子，並且對於歐幾里得轉換具有等變性。與目前最先進的等變擴散模型相比，END 的關鍵創新在於其可學習的前向過程，從而增強了生成建模的能力。前向過程並非預先指定，而是通過時間和數據相關的轉換進行參數化，該轉換對於剛性轉換具有等變性。通過一系列在標準分子生成基準上的實驗，我們證明了 END 在無條件和條件生成方面，與幾個強大的基線模型相比，具有競爭性的性能。", "applications": ["藥物設計：加速新藥開發，設計出更有效、副作用更小的藥物分子。", "材料科學：創造具有特定性能的新材料，例如更堅固、更輕便的複合材料。", "化學工程：優化化學反應路徑，提高化學品的生產效率和安全性。"], "pitch": "想像一下，一個AI能以前所未有的速度和精度設計新分子。我們的「等變神經擴散」(END) 技術，正是實現這一願景的關鍵。END 不僅能生成分子結構，更能預測其在三維空間中的行為，這對於藥物發現、材料科學等領域至關重要。傳統的分子設計方法耗時耗力，而 END 可以將這個過程縮短數月甚至數年，大幅降低研發成本。我們預計，END 將徹底改變分子設計領域，為製藥、化工、材料等行業帶來革命性的突破。透過授權、合作開發等方式，END 的商業潛力將是無限的。現在投資 END，就是投資未來，搶佔分子設計AI的領先地位！", "audio": "docs/data/audios/2506.10532v1.wav"}
{"query": "AI", "id": "2506.10624v1", "url": "http://arxiv.org/abs/2506.10624v1", "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "authors": ["Lukas Jünger", "Jan Henrik Weinstock", "Tim Kraus"], "published_date": "2025-06-12", "timestamp": "2025-06-15T12:21:47.441139", "title_zh": "快速虛擬平台中的可擴展軟體測試：利用SystemC、QEMU和容器化技術", "summary_zh": "現今硬軟體系統日益複雜，尤其在汽車等安全至關重要的領域，需要大量的測試。然而，硬體往往供不應求，阻礙了早期軟體開發。本研究提出一種利用容器化技術封裝虛擬平台（VP）的方法，以減少環境依賴性，並實現雲端部署，從而進行快速、並行的測試執行。同時，採用QEMU和VCML等開源VP技術，避免了授權費用。我們以AI加速器VP案例研究，證明了該方法的有效性，為加速硬軟體協同開發提供了一個可靠的解決方案。", "applications": ["想像一下，汽車製造商可以在新車的晶片還沒生產出來前，就先在雲端模擬各種駕駛情境，進行軟體測試，確保自動駕駛系統在各種狀況下都能安全運行，就像玩賽車遊戲一樣，但結果攸關人命。", "智慧家電開發商可以在產品上市前，利用虛擬平台模擬真實的家庭環境，測試家電的軟體是否能順利運作，並與其他智慧設備相容。這就像在自家客廳裡擺滿了虛擬的家電，測試它們的協同工作能力。", "醫療設備公司可以利用虛擬平台，模擬手術室的環境，測試醫療設備的軟體是否穩定可靠，確保手術過程的安全。這就像在虛擬手術室中進行演練，避免真實手術中可能發生的錯誤。"], "pitch": "各位投資人，我們正在打造的是下一代硬軟體協同開發的加速器！傳統的硬體開發週期漫長且昂貴，嚴重阻礙了創新。我們的技術，透過結合SystemC、QEMU和容器化等先進技術，打造出可擴展的虛擬平台，讓軟體工程師在硬體原型出來之前，就能進行大規模的測試和驗證。這不僅大幅縮短了開發時間，降低了成本，更重要的是，它加速了創新！想像一下，未來所有的智慧設備、汽車、醫療設備，甚至太空船，都可以在雲端進行模擬和測試。這是一個巨大的市場，而我們正站在這個市場的風口浪尖上。我們的AI加速器案例研究已經證明了技術的有效性，我們需要您的投資，將這項技術推向市場，引領硬軟體協同開發的未來！未來的硬體創新將不再受限於物理世界的限制，而是可以在雲端無限擴展！", "audio": "docs/data/audios/2506.10624v1.wav"}
{"query": "Foundation Model", "id": "2506.09368v1", "url": "http://arxiv.org/abs/2506.09368v1", "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "published_date": "2025-06-11", "timestamp": "2025-06-15T12:23:09.116938", "title_zh": "基於擴散模型的異常檢測與生成：綜述", "summary_zh": "本研究綜述了利用擴散模型進行異常檢測與生成（ADGDM）的最新進展。異常檢測在網路安全、金融、醫療和工業製造等領域至關重要。擴散模型因其學習複雜數據分佈和生成高保真樣本的能力，為無監督異常檢測提供了一個強大的框架。本研究重點介紹了異常檢測和生成之間內在的協同關係，揭示了擴散模型如何通過生成技術解決異常數據稀缺的問題，同時檢測方法如何提供關鍵反饋以提高生成保真度。我們詳細分類了ADGDM方法，並討論了可擴展性和計算效率等關鍵挑戰，以及與大型語言模型整合等未來方向。本綜述旨在指導研究人員和從業者利用擴散模型，為各種應用提供創新的異常檢測解決方案。", "applications": ["信用卡詐欺偵測：想像一下，我們的技術能即時分析您的信用卡交易紀錄，一旦出現與您平常消費習慣不符的異常交易（例如：突然在國外刷了一筆大額款項），系統會立刻發出警報，避免您的財產損失。", "工廠設備故障預測：在工廠裡，機器每天都在運作。我們的技術可以分析機器運作時產生的數據（例如：溫度、震動），提早發現異常狀況，預測機器可能發生的故障，讓工廠可以及早維修，避免生產線停擺。", "醫療影像異常檢測：醫生可以利用我們的技術，快速分析X光片或核磁共振影像，找出潛在的病灶或異常組織，協助醫生做出更精確的診斷，提高治療的成功率。"], "pitch": "各位投資人，我們帶來的是革命性的異常檢測技術，它基於最先進的擴散模型，能以前所未有的精度和效率，在海量數據中揪出潛在的風險和機會。試想一下，在金融領域，它可以精準預測市場崩盤，讓您提前避險；在醫療領域，它可以早期發現癌症，拯救無數生命；在工業領域，它可以預防設備故障，降低巨額損失。更重要的是，我們的技術還能生成異常數據，解決訓練數據不足的難題，大幅提升檢測效果。未來，隨著數據量的爆炸式增長，異常檢測的需求將會越來越大，而我們的技術將成為各行各業不可或缺的利器。現在投資我們，您將站在AI浪潮的最前端，共同開創一個更安全、更高效的未來！", "audio": "docs/data/audios/2506.09368v1.wav"}
{"query": "Diffusion Model", "id": "2506.10507v1", "url": "http://arxiv.org/abs/2506.10507v1", "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle", "summary": "Recent advances in diffusion models have significantly improved image\ngeneration and editing, but extending these capabilities to 3D assets remains\nchallenging, especially for fine-grained edits that require multi-view\nconsistency. Existing methods typically restrict editing to predetermined\nviewing angles, severely limiting their flexibility and practical applications.\nWe introduce Edit360, a tuning-free framework that extends 2D modifications to\nmulti-view consistent 3D editing. Built upon video diffusion models, Edit360\nenables user-specific editing from arbitrary viewpoints while ensuring\nstructural coherence across all views. The framework selects anchor views for\n2D modifications and propagates edits across the entire 360-degree range. To\nachieve this, Edit360 introduces a novel Anchor-View Editing Propagation\nmechanism, which effectively aligns and merges multi-view information within\nthe latent and attention spaces of diffusion models. The resulting edited\nmulti-view sequences facilitate the reconstruction of high-quality 3D assets,\nenabling customizable 3D content creation.", "authors": ["Junchao Huang", "Xinting Hu", "Zhuotao Tian", "Shaoshuai Shi", "Li Jiang"], "published_date": "2025-06-12", "timestamp": "2025-06-15T12:24:37.153581", "title_zh": "Edit360：從任意角度將 2D 圖像編輯應用於 3D 資產", "summary_zh": "Edit360 是一個免微調的框架，能將 2D 圖像編輯擴展到多視角一致的 3D 編輯。它基於影片擴散模型，讓使用者能從任何角度進行編輯，同時確保所有視角的結構一致性。Edit360 透過創新的「錨點視圖編輯傳播」機制，在擴散模型的潛在和注意力空間中對齊並合併多視角資訊。這產生了編輯過的多視圖序列，方便重建高品質的 3D 資產，實現可自訂的 3D 內容創建。簡單來說，就是能讓你像修照片一樣，輕鬆從各個角度修改3D模型。", "applications": ["**線上購物增強體驗：** 想像一下，你想買一張椅子，透過 Edit360，你可以隨意調整椅子的顏色、材質，甚至改變椅腳的樣式，並從任何角度觀看修改後的樣子，就像真的擺在家裡一樣，買到最滿意的商品。", "**遊戲角色客製化：** 遊戲玩家可以利用 Edit360，從各個角度精細調整遊戲角色的外觀，打造獨一無二的角色形象，不再受限於預設選項，完全展現個人風格。", "**建築設計預覽：** 建築師或設計師可以使用 Edit360，讓客戶在設計階段就能從任何角度預覽建築或室內設計的效果，並即時進行修改，大幅提升溝通效率和客戶滿意度。"], "pitch": "各位投資人，想像一下，未來所有 3D 內容的創作都將變得像修圖一樣簡單！Edit360 打破了 3D 編輯的技術壁壘，讓使用者能以 2D 編輯的直覺方式，輕鬆修改 3D 模型，無需專業技能。這將引爆 3D 內容創作的革命！\n\n從電商、遊戲、建築到元宇宙，Edit360 的應用潛力無窮。我們可以預見，未來每個人都能輕鬆打造個性化的 3D 資產，無論是設計獨一無二的虛擬化身，還是客製化的家居產品。這將是一個數十億美元的市場！\n\n更重要的是，Edit360 的技術領先地位，將為我們帶來巨大的競爭優勢。我們將積極拓展 B2B 和 B2C 市場，與各大平台合作，建立 3D 內容生態系統。現在投資 Edit360，您將成為這場 3D 革命的領航者，共同開創 3D 內容的無限可能！", "audio": "docs/data/audios/2506.10507v1.wav"}
{"query": "AI", "id": "2506.10622v1", "url": "http://arxiv.org/abs/2506.10622v1", "title": "SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis", "summary": "The advancement of conversational AI systems relies on the availability of\nhigh-quality, flexible, and reproducible synthetic dialogues for training,\nevaluation, and benchmarking. SDialog is a modular, extensible Python toolkit\ndesigned to address the challenges of synthetic dialogue generation and\nanalysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog\nprovides abstractions for personas, orchestration, and scenario management,\nenabling the creation of realistic, diverse, and controllable conversational\ndata for research and development. SDialog supports workflows such as\nmulti-agent simulation and scenario-driven generation, and represents a step\nforward in the standardization of tools and frameworks for synthetic data\ngeneration, a crucial advancement for ensuring reproducibility in today's\nfast-evolving research landscape.", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Petr Motlicek"], "published_date": "2025-06-12", "timestamp": "2025-06-15T15:12:03.026487", "title_zh": "SDialog：用於合成對話生成與分析的Python工具包", "summary_zh": "SDialog是一個Python工具包，旨在簡化合成對話的生成與分析。它利用大型語言模型（LLMs），提供角色、流程和情境管理的抽象化功能，幫助研究人員創建逼真、多樣且可控的對話數據。SDialog支援多代理模擬和情境驅動生成等工作流程，有助於提升對話AI系統的訓練、評估和基準測試。此工具包促進了合成數據生成工具和框架的標準化，對於確保快速發展的研究領域中的可重複性至關重要。", "applications": ["**智能客服模擬訓練：** 可以用 SDialog 模擬各種客服情境，讓客服機器人學習如何應對不同的客戶問題和情緒，提升服務品質。", "**語言學習夥伴：** SDialog 可以創建多種角色扮演的語言學習夥伴，讓學習者在模擬的真實對話情境中練習口語，提高語言能力。", "**心理諮商模擬：** 心理諮商師可以使用 SDialog 模擬不同的心理諮商情境，練習諮商技巧，並評估諮商效果。"], "pitch": "各位投資人，我們正在打造對話式AI的未來！SDialog 是一個革命性的工具包，它能以前所未有的方式生成和分析合成對話。想像一下，有了 SDialog，我們就能訓練出更聰明、更人性化的聊天機器人，應用於客戶服務、教育、醫療等各個領域。這不僅僅是技術，更是一個巨大的市場機會。隨著對話式AI的需求不斷增長，SDialog 將成為行業標準，為我們帶來巨大的商業價值。我們相信，透過 SDialog，我們能引領下一代對話式AI的發展，創造一個更智能、更便捷的世界。現在加入我們，一起開創這個令人興奮的未來！", "audio": "docs/data/audios/2506.10622v1.wav"}
{"query": "Foundation Model", "id": "2506.09349v1", "url": "http://arxiv.org/abs/2506.09349v1", "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "published_date": "2025-06-11", "timestamp": "2025-06-15T15:13:29.098926", "title_zh": "OmniDRCA：透過雙重解析度語音表示和對比對齊的平行語音-文本基礎模型", "summary_zh": "OmniDRCA是一種新的平行語音-文本基礎模型，它使用雙重解析度的語音表示和對比跨模態對齊來提升效能。它能同時處理語音和文本，透過對比對齊增強音訊理解。在語音問答基準測試中，OmniDRCA在平行聯合語音-文本模型中達到最先進的效能，甚至能與交錯模型相媲美。這項技術有潛力擴展到全雙工對話情境，為語音和語言處理領域帶來突破。", "applications": ["智慧客服：OmniDRCA可以讓客服機器人更自然地理解客戶的語音提問，並即時生成相應的文字回覆，提升溝通效率和客戶滿意度。", "即時翻譯：使用者可以直接對著手機說話，OmniDRCA可以同步生成翻譯後的文字，方便跨語言交流，例如在國外旅遊或參加國際會議時。", "語音控制智慧家居：透過OmniDRCA，使用者可以用更自然的語音指令控制家中的電燈、空調、電視等設備，無需學習複雜的指令或操作介面。"], "pitch": "各位投資人，想像一下，未來的人機互動將不再受限於文字，而是自然流暢的語音交流！OmniDRCA正是實現這個願景的關鍵技術。它不僅能同步理解語音和文字，更能在兩者之間建立更深層次的關聯，打造真正智能的對話系統。這意味著，我們的智慧助理將變得更聰明、更貼心，能夠理解複雜的語氣和情感，提供更個性化的服務。市場潛力巨大！從智慧客服、教育輔導到醫療照護，OmniDRCA的應用場景無可限量。我們預計，搭載OmniDRCA技術的產品將在未來五年內引領人機互動的新潮流，成為市場上的領導者。現在投資OmniDRCA，就是投資未來人機互動的無限可能！", "audio": "docs/data/audios/2506.09349v1.wav"}
{"query": "Diffusion Model", "id": "2506.10502v1", "url": "http://arxiv.org/abs/2506.10502v1", "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks", "summary": "We present a novel attack specifically designed against Tree-Ring, a\nwatermarking technique for diffusion models known for its high imperceptibility\nand robustness against removal attacks. Unlike previous removal attacks, which\nrely on strong assumptions about attacker capabilities, our attack only\nrequires access to the variational autoencoder that was used to train the\ntarget diffusion model, a component that is often publicly available. By\nleveraging this variational autoencoder, the attacker can approximate the\nmodel's intermediate latent space, enabling more effective surrogate-based\nattacks. Our evaluation shows that this approach leads to a dramatic reduction\nin the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to\n0.153 and from 0.994 to 0.385, respectively, while maintaining high image\nquality. Notably, our attacks outperform existing methods that assume full\naccess to the diffusion model. These findings highlight the risk of reusing\npublic autoencoders to train diffusion models -- a threat not considered by\ncurrent industry practices. Furthermore, the results suggest that the Tree-Ring\ndetector's precision, a metric that has been overlooked by previous\nevaluations, falls short of the requirements for real-world deployment.", "authors": ["Junhua Lin", "Marc Juarez"], "published_date": "2025-06-12", "timestamp": "2025-06-15T15:15:28.570772", "title_zh": "樹皮上的裂縫：利用公開知識移除樹環浮水印", "summary_zh": "本研究揭露一種針對擴散模型浮水印技術「樹環」的新型攻擊方法。「樹環」以其高度隱蔽性和對抗移除攻擊的強韌性著稱。不同於以往需要強烈假設攻擊者能力的移除攻擊，我們的攻擊僅需取得用於訓練目標擴散模型的變分自編碼器，而這個組件通常是公開可用的。透過利用這個變分自編碼器，攻擊者可以近似模型的潛在空間，從而實現更有效的代理攻擊。實驗結果顯示，這種方法大幅降低了「樹環」偵測器的AUC值，同時保持了高圖像品質。我們的攻擊優於假設完全存取擴散模型的現有方法。研究結果突顯了重用公開自編碼器訓練擴散模型的風險，這是一個當前業界實務未曾考慮的威脅。", "applications": ["相片真偽鑑定：新聞媒體或社群平台可以利用此技術，檢測AI生成的圖片是否被植入浮水印，進而判斷圖片的真實性，避免假新聞傳播。", "智慧財產權保護：藝術家或設計師可以利用此技術分析AI生成的圖像，確認自己的作品是否被未經授權地使用在AI訓練資料中，進而維護自身權益。", "內容審核：社群平台或政府機構可以使用此技術，檢測AI生成的內容是否帶有惡意或不當的浮水印，例如宣傳仇恨言論或散播不實資訊的隱藏訊息。"], "pitch": "各位創投夥伴，想像一下，現在AI生成的圖片越來越逼真，真假難辨。我們這項技術，就像是AI圖片的『反偵測雷達』，能找出隱藏在圖片中的浮水印，揭穿AI造假的真相！\n\n目前，AI浮水印技術被視為保護智慧財產權的重要工具，但我們的研究證明，現有的浮水印技術存在漏洞，容易被破解。這代表什麼？代表市場迫切需要更安全、更可靠的AI內容驗證技術！\n\n我們的技術不僅能移除浮水印，更能分析其弱點，為開發更強大的浮水印技術提供寶貴的資訊。未來，我們可以將這項技術應用於：\n\n*   AI內容驗證服務：提供企業或個人驗證AI生成內容真偽的服務，收取訂閱費用。\n*   AI模型安全諮詢：協助企業評估AI模型的安全性，避免被惡意攻擊。\n*   下一代浮水印技術研發：利用我們的研究成果，開發更難以破解的浮水印技術，搶佔市場先機。\n\nAI的時代已經來臨，內容安全將是重中之重。現在投資我們，就是投資AI的未來！讓我們一起打造一個更安全、更可信賴的AI世界！", "audio": "docs/data/audios/2506.10502v1.wav"}
{"query": "AI", "id": "2506.10600v1", "url": "http://arxiv.org/abs/2506.10600v1", "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "published_date": "2025-06-12", "timestamp": "2025-06-15T18:15:58.593923", "title_zh": "EmbodiedGen：邁向具身智慧的生成式3D世界引擎", "summary_zh": "EmbodiedGen 是一個創新的平台，旨在低成本、大規模地生成高品質、可控且逼真的3D資產，這些資產具有精確的物理屬性和真實世界的比例，並採用統一機器人描述格式 (URDF)。它包含六個關鍵模組，包括 Image-to-3D、Text-to-3D 等，能夠生成多樣且互動的3D世界。EmbodiedGen 利用生成式 AI 來應對具身智慧研究中泛化和評估的挑戰，解決了傳統3D資產製作成本高昂和真實感有限的問題，為數據驅動方法的可擴展性提供了基礎。", "applications": ["想像一下，你可以用手機拍一張照片，EmbodiedGen 就能立刻生成一個3D模型，讓你直接在手機上玩遊戲，或者把它放到你的虛擬家居裝修設計中，看看效果如何。", "如果你是一位老師，想讓學生更直觀地學習物理知識，你可以用 EmbodiedGen 生成各種3D物件和場景，讓學生在虛擬實驗室中進行互動和實驗，加深理解。", "對於遊戲開發者來說，EmbodiedGen 可以快速生成大量的3D資產，例如不同的房屋、家具、人物等，節省大量的建模時間和成本，讓他們可以更專注於遊戲的玩法和劇情設計。"], "pitch": "各位投資人，我們正在打造一個革命性的3D世界生成引擎 EmbodiedGen！它就像3D世界的Photoshop，但比Photoshop更強大、更智能。傳統3D建模耗時耗力，限制了VR/AR、遊戲、機器人等產業的發展。EmbodiedGen 利用生成式AI，能以極低的成本、極高的效率，創造出無限可能的3D世界。想像一下，未來每個人都可以輕鬆創造自己的虛擬世界，用於社交、娛樂、教育、設計，甚至商業活動！這將是一個數千億美元級別的市場！我們不僅僅是創造工具，我們是在創造一個全新的3D內容生態系統！現在加入我們，一起成為這個未來世界的締造者！", "audio": "docs/data/audios/2506.10600v1.wav"}
{"query": "Foundation Model", "id": "2506.09284v1", "url": "http://arxiv.org/abs/2506.09284v1", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "published_date": "2025-06-10", "timestamp": "2025-06-15T18:17:20.311615", "title_zh": "UAD：用於機器人操作泛化的無監督可供性知識提煉", "summary_zh": "本研究提出一種名為UAD的無監督可供性知識提煉方法，旨在提升機器人在非結構化環境中操作物體的泛化能力。UAD利用大型視覺模型和視覺-語言模型的互補優勢，自動生成大規模的<指令, 視覺可供性>配對數據集，無需人工標注。透過在凍結特徵之上訓練輕量級的任務條件解碼器，UAD展現了對真實機器人場景和各種人類活動的顯著泛化能力，即使僅在模擬渲染對象上進行訓練。使用UAD提供的可供性作為觀察空間，模仿學習策略在僅需10次示範後，就能展現出對未見過的物體實例、物體類別，甚至是任務指令變化的良好泛化能力。", "applications": ["**智慧家庭助手：** 想像一下，你可以對你的機器人管家說：「把蘋果切成兩半。」機器人就能透過理解蘋果的『可切割性』，精準地完成任務，即使它從未見過這個品種的蘋果。", "**自動化倉儲物流：** 在擁擠的倉庫中，機器人可以根據貨物的『可抓取性』和『可放置性』，靈活地搬運各種形狀和大小的包裹，大幅提升物流效率。", "**輔助醫療照護：** 機器人可以根據病人的需求，例如『可扶握性』，協助病人起身、移動，減輕照護人員的負擔，提升病人的生活品質。"], "pitch": "各位創投先進，我們正站在機器人技術革命的浪潮之巔！UAD技術，如同為機器人裝上了一雙慧眼，讓它們無需大量人工標注，就能像人類一樣理解物體的『可供性』，也就是物體能被如何使用。這項技術打破了機器人應用場景的限制，從智慧工廠到無人商店，再到個人化的家庭服務，UAD賦予機器人前所未有的適應性和自主性。想像一下，未來的機器人不再需要針對每個任務進行編程，而是能夠透過理解環境，自主學習並完成任務。這將釋放巨大的商業潛力，我們預計在未來五年內，基於UAD技術的機器人解決方案，將在全球市場創造數十億美元的價值。現在投資UAD，就是投資機器人技術的未來，讓我們一起引領這場變革！", "audio": "docs/data/audios/2506.09284v1.wav"}
{"query": "Diffusion Model", "id": "2506.10433v1", "url": "http://arxiv.org/abs/2506.10433v1", "title": "Measuring Semantic Information Production in Generative Diffusion Models", "summary": "It is well known that semantic and structural features of the generated\nimages emerge at different times during the reverse dynamics of diffusion, a\nphenomenon that has been connected to physical phase transitions in magnets and\nother materials. In this paper, we introduce a general information-theoretic\napproach to measure when these class-semantic \"decisions\" are made during the\ngenerative process. By using an online formula for the optimal Bayesian\nclassifier, we estimate the conditional entropy of the class label given the\nnoisy state. We then determine the time intervals corresponding to the highest\ninformation transfer between noisy states and class labels using the time\nderivative of the conditional entropy. We demonstrate our method on\none-dimensional Gaussian mixture models and on DDPM models trained on the\nCIFAR10 dataset. As expected, we find that the semantic information transfer is\nhighest in the intermediate stages of diffusion while vanishing during the\nfinal stages. However, we found sizable differences between the entropy rate\nprofiles of different classes, suggesting that different \"semantic decisions\"\nare located at different intermediate times.", "authors": ["Florian Handke", "Félix Koulischer", "Gabriel Raya", "Luca Ambrogioni"], "published_date": "2025-06-12", "timestamp": "2025-06-15T18:18:47.748164", "title_zh": "生成擴散模型中語義資訊產生的測量", "summary_zh": "本研究提出一種資訊理論方法，量化生成式擴散模型在生成圖像過程中，不同類別語義資訊產生的時間點。透過估算帶雜訊狀態下類別標籤的條件熵，並分析其隨時間的變化，我們能找出語義資訊傳輸最活躍的階段。實驗結果顯示，語義資訊主要在擴散過程的中間階段產生，且不同類別的語義資訊產生的時間點存在差異。這項研究有助於我們更深入了解生成模型的運作機制，並可應用於提升圖像生成品質與控制。", "applications": ["智慧相簿：相簿自動分類照片，例如自動辨識風景照、人像照、食物照等，並依據語義資訊產生的時間點，更精準地進行分類，甚至能區分不同種類的風景（山景、海景）。", "AI藝術創作：讓使用者能更精細地控制AI生成圖像的風格和內容。例如，指定先生成天空的顏色和雲朵的形狀，再加入建築物的細節，創造出獨一無二的藝術作品。", "醫療影像分析：協助醫生更快速、準確地診斷疾病。例如，AI可以先辨識出腫瘤的大概位置，再逐步分析腫瘤的細節特徵，協助醫生判斷腫瘤的良性或惡性。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能精準掌握AI生成圖像的關鍵時間點，如同掌握了煉金術的配方！想像一下，未來我們可以打造出能高度客製化的AI圖像引擎，讓使用者能像導演一樣，精準控制AI生成內容的每一個細節。這不僅能應用於遊戲、電影等娛樂產業，更能深入醫療、教育等領域，帶來顛覆性的創新。我們的技術能讓AI生成內容更可控、更高效，大幅降低生產成本，並創造出前所未有的商業價值。我們預期在五年內，這項技術將成為AI圖像生成領域的黃金標準，市場規模將達到數十億美元。現在加入我們，一起開創AI圖像生成的全新時代！", "audio": "docs/data/audios/2506.10433v1.wav"}
{"query": "AI", "id": "2506.10585v1", "url": "http://arxiv.org/abs/2506.10585v1", "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning", "summary": "This paper introduces the Primender sequence, a novel integer sequence\ndefined by a hybrid rule that combines classical primality with modular\ndigit-based conditions. Specifically, a number n is included in the sequence if\nit is prime or ends with a prime number of unit digit or any length. In other\nwords, numbers which are primes or have at least one prime suffix. The\nresulting sequence exhibits a deterministic yet non-trivial structure, blending\nnumber-theoretic properties with symbolic patterning. We propose the Primender\nsequence as a benchmark for evaluating the symbolic reasoning capabilities of\nLarge Language Models (LLMs). The study is motivated by the need for\ninterpretable, rule-based testbeds that can assess an LLM's ability to infer\nhidden rules, validate mathematical hypotheses, and generalize symbolic logic\nat scale. A key hypothesis explored is: Whenever a number in the Primender\nsequence is exactly one more than the largest prime less than or equal to it,\nthe difference between it and the previous number in the sequence is also 1. We\ndesign a structured prompt and evaluation framework to test this hypothesis\nacross multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,\nGemini, Grok, and LLaMA. The models are tasked with identifying the underlying\nrule, validating the hypothesis, and generating the next 100,000 terms of the\nsequence. Comparative metrics such as rule inference accuracy, hypothesis\nevaluation, sequence validity, and symbolic explanation quality are used to\nassess model performance. This work contributes a novel mathematical construct\nand a reproducible methodology for benchmarking LLMs in symbolic reasoning,\nhypothesis testing, and scalable pattern generalization - bridging the domains\nof number theory, artificial intelligence, and software engineering.", "authors": ["Mohd Anwar Jamal Faiz"], "published_date": "2025-06-12", "timestamp": "2025-06-15T21:12:11.801501", "title_zh": "質數尾數序列：一種用於測試符號推論與人工智慧推理的新型數學結構", "summary_zh": "本論文介紹了「質數尾數序列」，這是一種結合了質數特性和模數數位條件的全新整數序列。序列中的數字若為質數，或其尾數包含質數，即被納入。此序列呈現出確定性但非平凡的結構，融合了數論性質與符號模式。我們建議將此序列作為評估大型語言模型（LLM）符號推理能力的基準。研究旨在建立可解釋、基於規則的測試平台，評估LLM推斷隱藏規則、驗證數學假設，以及大規模推廣符號邏輯的能力。我們設計了結構化提示和評估框架，測試多個LLM，並評估其規則推斷準確性、假設驗證、序列有效性和符號解釋品質。這項研究貢獻了一種新的數學結構和可重現的方法，用於評估LLM在符號推理、假設檢定和可擴展模式泛化方面的能力，連結了數論、人工智慧和軟體工程領域。", "applications": ["**密碼學應用：** 質數尾數序列的獨特性質可用於生成更安全的密碼或加密金鑰。想像一下，你的銀行密碼不再是簡單的數字組合，而是基於複雜的質數尾數序列生成的，破解難度大大提升。", "**藝術與音樂創作：** 藝術家或音樂家可以將質數尾數序列作為靈感來源，創造出獨特的視覺或聽覺作品。例如，根據序列中的數字來決定音符的長短或顏色的搭配，產生意想不到的藝術效果。", "**數據壓縮與檢索：** 質數尾數序列的模式可用於數據壓縮，減少儲存空間。想像一下，你的手機照片或影片可以透過這種方式進行更高效的壓縮，節省大量儲存空間，而且檢索速度更快。"], "pitch": "各位創投先進，想像一下，我們正在打造的是AI界的「智力測驗黃金標準」！傳統的AI評估方法往往是黑箱作業，難以理解AI的思考邏輯。但我們的「質數尾數序列」就像一把手術刀，精準剖析AI的符號推理能力，讓AI的「思考過程」無所遁形！這不僅能幫助我們開發更聰明、更可靠的AI，還能催生全新的AI應用，例如：更安全的金融交易系統、更精準的醫療診斷、更高效的程式碼生成工具。未來，我們將建立一個龐大的「AI推理能力評估平台」，為各行各業提供AI智力評估服務，成為AI時代的「標準普爾」。現在加入我們，您將成為AI革命的領航者，共同開創一個AI賦能的無限未來！", "audio": "docs/data/audios/2506.10585v1.wav"}
{"query": "Foundation Model", "id": "2506.09167v1", "url": "http://arxiv.org/abs/2506.09167v1", "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "published_date": "2025-06-10", "timestamp": "2025-06-15T21:13:25.356366", "title_zh": "從手腕穿戴式加速計估算內臟脂肪", "summary_zh": "本研究利用2011-2014年美國國家健康與營養調查(NHANES)的數據，探討身體活動(PA)與內臟脂肪(VAT)之間的關聯。內臟脂肪是代謝健康的重要指標，過多內臟脂肪與第二型糖尿病和胰島素阻抗密切相關。研究團隊開發兩種方法，分別是基於步態和睡眠動作的工程特徵，以及基於深度神經網路的連續加速計數據，來估算內臟脂肪。結果顯示，結合這兩種方法，並加入受試者的人口統計學和身體測量數據，可以高度準確地估算內臟脂肪(r=0.86)。這項研究證明了身體活動與內臟脂肪之間存在強烈的關聯，進而也與代謝健康風險息息相關。", "applications": ["「健康手環進階功能：以後你的健康手環不只能算步數，還能估算你的內臟脂肪，提醒你注意飲食和運動，降低罹患糖尿病的風險。」", "「個人化運動建議：App根據你的活動數據，提供更精準的運動建議，幫助你燃燒內臟脂肪，改善代謝健康。」", "「企業員工健康管理：公司可以利用這項技術，追蹤員工的內臟脂肪變化，提供客製化的健康促進方案，降低醫療成本。」"], "pitch": "各位投資人，我們正在開發一項革命性的技術，能從手腕穿戴裝置精準估算內臟脂肪，這項技術不僅能讓使用者更了解自己的代謝健康風險，還能提供個人化的健康建議。想像一下，未來每個人都能透過簡單的穿戴裝置，隨時監控自己的內臟脂肪，及早預防糖尿病等慢性疾病。這項技術的應用範圍非常廣泛，從個人健康管理到企業員工健康促進，甚至是保險公司的風險評估，都有巨大的潛力。我們相信，這項技術將徹底改變健康產業，為投資者帶來豐厚的回報。現在投資，您將成為這場健康革命的先驅！", "audio": "docs/data/audios/2506.09167v1.wav"}
{"query": "Diffusion Model", "id": "2506.10391v1", "url": "http://arxiv.org/abs/2506.10391v1", "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion", "summary": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.", "authors": ["Yuanyi Song", "Pumeng Lyu", "Ben Fei", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "published_date": "2025-06-12", "timestamp": "2025-06-15T21:14:54.106965", "title_zh": "ReconMOST：基於觀測引導擴散的多層海洋溫度重建", "summary_zh": "本研究提出ReconMOST，一個數據驅動的引導擴散模型框架，用於多層海洋溫度重建。該模型首先利用大量歷史數值模擬數據預訓練一個無條件擴散模型，使其能夠學習海洋溫度場的物理一致性分佈模式。在生成階段，利用稀疏但高精度的現場觀測數據作為反向擴散過程的引導點，生成準確的重建結果。即使在缺乏直接觀測數據的區域，預訓練階段學習到的物理一致性空間分佈模式也能實現隱式引導和物理上合理的重建。該方法將基於機器學習的海面溫度重建擴展到全球多層設置，處理超過92.5%的缺失數據，同時保持重建精度、空間分辨率和卓越的泛化能力。實驗結果表明，該框架有效且穩健。", "applications": ["漁業資源管理：漁民可以利用更精確的海洋溫度資訊，預測魚群的移動路徑，提高捕撈效率，同時避免過度捕撈，維持海洋生態平衡。", "航運安全：船隻可以根據重建的海洋溫度資訊，避開可能結冰的海域或異常海流，保障航行安全，減少事故發生。", "氣候變遷研究：科學家可以利用ReconMOST重建過去的海洋溫度資料，更深入地了解氣候變遷對海洋環境的影響，並預測未來的氣候變化趨勢。"], "pitch": "各位投資人，想像一下，如果我們能像預測天氣一樣，精準掌握全球海洋各個深度的溫度變化，會帶來多大的變革？ReconMOST正是實現這個願景的關鍵技術！傳統方法受限於數據稀疏和計算成本，而ReconMOST利用AI，能有效重建高達92.5%的缺失數據，提供前所未有的海洋溫度資訊。這不僅僅是學術研究的突破，更是潛力無限的商業機會！\n\n試想，我們可以將這些數據應用於智慧漁業，幫助漁民精準定位魚群，提高產量；可以為航運業提供最佳航線規劃，降低燃料消耗和事故風險；甚至可以開發海洋能源，利用溫差發電。更重要的是，面對全球暖化，ReconMOST能幫助我們更了解海洋在氣候變遷中的角色，為制定有效的應對策略提供科學依據。我們預期未來海洋數據市場將呈現指數級增長，ReconMOST將成為這個市場的領導者！現在投資ReconMOST，就是投資海洋的未來，投資一個充滿無限可能的藍海市場！", "audio": "docs/data/audios/2506.10391v1.wav"}
{"query": "AI", "id": "2506.10570v1", "url": "http://arxiv.org/abs/2506.10570v1", "title": "6G Infrastructures for Edge AI: An Analytical Perspective", "summary": "The convergence of Artificial Intelligence (AI) and the Internet of Things\nhas accelerated the development of distributed, network-sensitive applications,\nnecessitating ultra-low latency, high throughput, and real-time processing\ncapabilities. While 5G networks represent a significant technological\nmilestone, their ability to support AI-driven edge applications remains\nconstrained by performance gaps observed in real-world deployments. This paper\naddresses these limitations and highlights critical advancements needed to\nrealize a robust and scalable 6G ecosystem optimized for AI applications.\nFurthermore, we conduct an empirical evaluation of 5G network infrastructure in\ncentral Europe, with latency measurements ranging from 61 ms to 110 ms across\ndifferent close geographical areas. These values exceed the requirements of\nlatency-critical AI applications by approximately 270%, revealing significant\nshortcomings in current deployments. Building on these findings, we propose a\nset of recommendations to bridge the gap between existing 5G performance and\nthe requirements of next-generation AI applications.", "authors": ["Kurt Horvath", "Shpresa Tuda", "Blerta Idrizi", "Stojan Kitanov", "Fisnik Doko", "Dragi Kimovski"], "published_date": "2025-06-12", "timestamp": "2025-06-16T00:58:16.596601", "title_zh": "邊緣人工智慧的6G基礎設施：分析視角", "summary_zh": "人工智慧與物聯網的融合加速了分散式、網路敏感型應用程式的發展，對超低延遲、高吞吐量和即時處理能力提出了更高的要求。雖然5G網路取得了顯著的技術進展，但在實際部署中，其支援人工智慧驅動的邊緣應用程式的能力仍然受到性能差距的限制。本文探討了這些限制，並強調了實現針對人工智慧應用優化的強大且可擴展的6G生態系統所需的關鍵進展。我們在歐洲中部對5G網路基礎設施進行了實證評估，發現延遲遠遠超出延遲敏感型人工智慧應用程式的需求。基於這些發現，我們提出了一系列建議，以彌合現有5G性能與下一代人工智慧應用程式需求之間的差距。", "applications": ["智慧交通：想像一下，未來的自駕車可以透過6G網路即時分析路況，比現在的5G更快更準確地做出反應，避免事故，讓交通更順暢、更安全。", "遠程醫療：醫生可以利用6G網路進行遠端手術，幾乎沒有延遲，就像親臨現場一樣。這讓偏遠地區的居民也能享受到頂尖的醫療服務。", "智慧工廠：工廠裡的機器人透過6G網路協同工作，精準度更高、反應速度更快，大幅提升生產效率，降低成本。"], "pitch": "各位投資人，我們正站在一個劃時代的轉捩點！5G雖然令人興奮，但它無法滿足未來人工智慧爆炸性成長的需求。我們的研究揭示了5G的瓶頸，並指出了通往6G時代的關鍵路徑。6G不僅僅是5G的升級，它將是人工智慧邊緣運算的基石，賦予無數應用前所未有的能力。試想一下，一個由無人機送貨、智慧城市管理、以及完全自動化產業組成的世界，這一切都將由6G驅動。我們的團隊擁有領先的技術優勢和對市場的深刻理解，我們正在打造下一代網路基礎設施，它將重新定義各行各業。現在投資，您將成為6G革命的先驅，共享萬億美元級的市場紅利！", "audio": "docs/data/audios/2506.10570v1.wav"}
{"query": "AI", "id": "2506.12008v1", "url": "http://arxiv.org/abs/2506.12008v1", "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI", "summary": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations.", "authors": ["Olga Vechtomova", "Jeff Bos"], "published_date": "2025-06-13", "timestamp": "2025-06-16T03:49:39.586023", "title_zh": "重塑舞蹈：舞者與AI之間的即時音樂共創", "summary_zh": "傳統舞蹈表演中，舞者的動作往往跟隨音樂。我們開發了一套系統，讓舞者能透過肢體動作即時塑造音樂環境。這個系統運用多模態架構，根據舞者的動作智能地組合預錄音樂片段，創造出連貫的音樂作品，建立舞者與AI之間的雙向創作夥伴關係。舞者既是表演者，也是作曲者。數據分析顯示，舞者的動作品質與音訊特徵之間存在關聯性。這種方法重新定義了AI在表演藝術中的角色，使其成為一個反應靈敏的合作者，擴展了專業舞蹈表演和即興藝術表達的可能性。", "applications": ["想像一下，在家運動時，AI能根據你的動作強度和節奏，自動調整播放的音樂，讓你更有動力，運動效果更好。", "在特殊教育領域，這套系統可以幫助自閉症兒童透過舞蹈和音樂表達情感，促進他們的社交互動和溝通能力。", "未來，演唱會上，歌手不再只是唱歌，而是能透過肢體動作與AI共同創作音樂，帶給觀眾前所未有的視聽體驗。"], "pitch": "各位投資人，我們正在打造一個革命性的音樂創作平台，它將重新定義舞蹈和音樂的關係。想像一下，一個舞者不再只是表演者，而是音樂的創造者，透過肢體動作就能即時生成獨一無二的音樂作品。這項技術不僅能應用於專業表演藝術，更能走入大眾生活，例如健身、教育、娛樂等領域，市場潛力巨大。我們的AI音樂共創系統，將成為下一個音樂產業的顛覆者，打造一個全新的音樂生態系統。現在加入我們，一起開創音樂與舞蹈的未來！我們預計，未來五年內，此技術將被廣泛應用於虛擬演唱會、互動遊戲、以及個人化的音樂創作工具，市場規模可達數十億美元。", "audio": "docs/data/audios/2506.12008v1.wav"}
{"query": "Foundation Model", "id": "2506.12015v1", "url": "http://arxiv.org/abs/2506.12015v1", "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction", "summary": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.", "authors": ["Hsi-Che Lin", "Yu-Chu Yu", "Kai-Po Chang", "Yu-Chiang Frank Wang"], "published_date": "2025-06-13", "timestamp": "2025-06-16T03:51:07.816653", "title_zh": "EMLoC：基於模擬器的記憶體效率型微調，結合LoRA修正", "summary_zh": "EMLoC是一個創新的框架，旨在解決大型模型微調時記憶體需求過高的問題。它利用下游校準集，透過activation-aware奇異值分解(SVD)構建輕量級的任務特定模擬器。接著，透過LoRA在模擬器上進行微調。為了修正原始模型和壓縮模擬器之間的不對齊，EMLoC提出了一種新穎的補償算法，校正微調後的LoRA模組，使其能夠合併回原始模型進行推論。EMLoC支援靈活的壓縮比例和標準訓練流程，適用於廣泛的應用。實驗證明，EMLoC在多個數據集和模態上優於其他基準模型，甚至可以在單張24GB消費級GPU上微調380億參數的模型。", "applications": ["AI個人助理客製化：使用者可以根據自己的需求和偏好，在本地端微調AI助理，使其更了解使用者的習慣，提供更精準的建議和服務，例如：行程安排、郵件回覆、新聞推薦等，無需擔心隱私外洩。", "醫療影像分析模型優化：醫生可以使用EMLoC在有限的資源下，針對特定疾病的醫療影像數據集微調模型，提高診斷的準確性和效率，例如：癌症篩檢、眼底病變檢測等，加速醫療診斷流程。", "遊戲AI強化：遊戲開發者可以利用EMLoC快速微調遊戲中的AI角色，使其更具挑戰性和互動性，提升玩家的遊戲體驗，例如：調整敵人的行為模式、設計更智能的NPC等，而無需耗費大量資源。"], "pitch": "各位投資人，我們正在顛覆AI模型微調的遊戲規則！想像一下，過去需要昂貴伺服器才能完成的模型客製化，現在可以在一台普通電腦上實現。EMLoC技術大幅降低了AI應用的門檻，讓每個人都能擁有專屬的AI模型。這不僅僅是技術突破，更是一個巨大的市場機會。未來，我們可以將EMLoC技術授權給各行各業，從個人助理、醫療診斷到遊戲開發，甚至可以應用於自動駕駛、金融風控等領域。隨著AI應用的普及，EMLoC的需求將呈現指數級增長。我們預計，在未來五年內，EMLoC將成為AI模型微調的行業標準，為投資者帶來豐厚的回報。現在加入我們，一起開創AI平民化的新時代！", "audio": "docs/data/audios/2506.12015v1.wav"}
{"query": "AI", "id": "2506.12003v1", "url": "http://arxiv.org/abs/2506.12003v1", "title": "Upgrade or Switch: Do We Need a New Registry Architecture for the Internet of AI Agents?", "summary": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built registry\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband\ntransitions, we find that agent requirements constitute qualitative, and not\nincremental, changes. While upgrades offer compatibility and faster deployment,\nclean-slate solutions provide better performance but require longer for\nadoption. Our analysis suggests hybrid approaches will emerge, with centralized\nregistries for critical agents and federated meshes for specialized use cases.", "authors": ["Ramesh Raskar", "Pradyumna Chari", "Jared James Grogan", "Mahesh Lambe", "Robert Lincourt", "Raghu Bala", "Abhishek Singh", "Ayush Chopra", "Rajesh Ranjan", "Shailja Gupta", "Dimitris Stripelis", "Maria Gorskikh", "Sichao Wang"], "published_date": "2025-06-13", "timestamp": "2025-06-16T06:20:13.872570", "title_zh": "升級還是轉換：我們是否需要為人工智慧代理網路設計全新的註冊架構？", "summary_zh": "隨著AI代理網路的興起，現有的網路基礎設施已難以應付其需求。AI代理會主動發起行動、保持狀態、衍生子代理，並直接與其他代理協商，這對搜尋速度、憑證撤銷和行為驗證提出了遠超現有DNS/PKI系統的要求。論文分析了升級現有基礎設施或建立專用註冊架構的可行性，指出DNS傳播延遲、憑證撤銷擴展性和IP位址不足是關鍵問題。研究評估了升級、轉換和混合註冊等方案，認為AI代理的需求是質變而非量變。最終建議採用混合方法，針對關鍵代理使用集中式註冊，針對特定用例使用聯合網格。", "applications": ["想像一下，未來家裡的掃地機器人、智慧冰箱，甚至你的數位分身，都需要快速、安全地互相溝通。這項技術就像是為它們打造一個專屬的『高速公路』，讓它們能即時協作，提供更智能、更便捷的服務。", "在無人駕駛汽車領域，這項技術能讓車輛之間瞬間確認彼此身份、共享路況資訊，就像是建立了車聯網的『身分證』系統，大幅提升行車安全和效率。", "在醫療領域，AI醫生可以透過這個技術安全地共享病患數據、協同診斷，而不用擔心資料洩露或被竄改，就像是建立了一個安全可靠的『醫療數據高速通道』，讓醫療資源能更有效地利用。"], "pitch": "各位創投夥伴，我們正在打造的是AI時代的網路基石！隨著AI代理數量爆炸性增長，現有網路架構已不堪重負。試想一下，數十億甚至數兆的AI代理在網路上即時互動，需要毫秒級的響應速度、絕對安全的身份驗證，以及高度可擴展的通信能力。我們的技術，正是為了解決這個迫在眉睫的問題。我們不僅僅是升級現有系統，更是在設計一個全新的網路架構，一個能夠支持未來AI經濟的基礎設施。這是一個千億美元級的市場，誰能掌握這項技術，誰就能引領下一個時代的網路革命！現在加入我們，一起成為AI網路的開創者，共同分享這巨大的市場紅利！", "audio": "docs/data/audios/2506.12003v1.wav"}
{"query": "AI", "id": "2506.11954v1", "url": "http://arxiv.org/abs/2506.11954v1", "title": "Technical Evaluation of a Disruptive Approach in Homomorphic AI", "summary": "We present a technical evaluation of a new, disruptive cryptographic approach\nto data security, known as HbHAI (Hash-based Homomorphic Artificial\nIntelligence). HbHAI is based on a novel class of key-dependent hash functions\nthat naturally preserve most similarity properties, most AI algorithms rely on.\nAs a main claim, HbHAI makes now possible to analyze and process data in its\ncryptographically secure form while using existing native AI algorithms without\nmodification, with unprecedented performances compared to existing homomorphic\nencryption schemes.\n  We tested various HbHAI-protected datasets (non public preview) using\ntraditional unsupervised and supervised learning techniques (clustering,\nclassification, deep neural networks) with classical unmodified AI algorithms.\nThis paper presents technical results from an independent analysis conducted\nwith those different, off-the-shelf AI algorithms. The aim was to assess the\nsecurity, operability and performance claims regarding HbHAI techniques. As a\nresults, our results confirm most these claims, with only a few minor\nreservations.", "authors": ["Eric Filiol"], "published_date": "2025-06-13", "timestamp": "2025-06-16T09:16:49.058041", "title_zh": "同態人工智慧中一種顛覆性方法的技術評估", "summary_zh": "本研究評估一種名為HbHAI（基於雜湊的同態人工智慧）的新型加密技術，用於資料安全。HbHAI基於一種新型的、與金鑰相關的雜湊函數，該函數自然地保留了大多數相似性屬性，而大多數人工智慧演算法都依賴於這些屬性。HbHAI使得在加密安全的資料形式下，能夠使用現有原生AI演算法進行分析和處理，且效能優於現有的同態加密方案。研究使用傳統的非監督式和監督式學習技術，在受HbHAI保護的資料集上進行了測試，結果證實了HbHAI技術在安全性、可操作性和效能方面的聲稱。", "applications": ["醫療數據分析：醫院可以在不洩露病人隱私的情況下，利用AI分析病歷，找出疾病的潛在關聯性，提升診斷準確度。", "金融交易安全：銀行可以利用AI在加密的交易數據上進行風險評估，預防詐欺行為，同時保護客戶的財務資訊。", "個人化廣告推薦：廣告商可以在不追蹤用戶個人資訊的情況下，利用AI分析用戶的興趣，提供更精準的廣告推薦。"], "pitch": "想像一下，一個人工智慧可以處理加密數據，無需解密，同時保障數據的絕對安全。這就是HbHAI帶來的革命性突破。傳統同態加密方案效能不佳，限制了應用。HbHAI則突破了效能瓶頸，讓AI可以直接在加密數據上執行，開創了數據安全的新紀元。這意味著，醫療機構可以安全地分享病患數據進行AI分析，金融機構可以在保護客戶隱私的同時進行反詐欺偵測，甚至政府機構也可以在不洩露敏感資訊的情況下進行數據分析。HbHAI的潛在市場規模極其龐大。我們相信，HbHAI將成為未來數據安全領域的關鍵技術，並將在醫療、金融、政府等領域產生深遠影響。現在投資HbHAI，您將成為這場數據安全革命的先驅，共同塑造安全、高效的人工智慧未來。我們的願景是：讓數據在安全中流動，讓AI在隱私中發展。", "audio": "docs/data/audios/2506.11954v1.wav"}
{"query": "Foundation Model", "id": "2506.11842v1", "url": "http://arxiv.org/abs/2506.11842v1", "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated Driving Systems", "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving.", "authors": ["Zhipeng Bao", "Qianwen Li"], "published_date": "2025-06-13", "timestamp": "2025-06-16T09:18:15.927339", "title_zh": "你的旅程，你作主：心理學與認知賦能的自動駕駛系統", "summary_zh": "現有自動駕駛車輛缺乏與乘客的有效雙向溝通，限制了個人化設定和從停頓狀態恢復的能力，進而降低了舒適度和信任感。我們提出PACE-ADS，一個以人為本的自動駕駛框架，讓車輛能夠感知、解讀並回應外部交通和內部乘客狀態。PACE-ADS包含三個基於基礎模型的代理：分析駕駛情境的駕駛代理、解讀乘客心理訊號和認知指令的心理學家代理，以及整合這些輸入以做出高階行為決策的協調者代理。PACE-ADS透過在行為層面運作來補充現有系統，實現閉環適應並支援跨平台整合。模擬結果顯示，PACE-ADS能根據乘客狀態調整駕駛風格，提高乘坐舒適度，並透過自主推理或人工指導安全地從停頓狀態恢復。這證明了基於大型語言模型的框架在彌合機器自主和以人為本的駕駛之間的差距的潛力。", "applications": ["想像一下，以後搭無人計程車，如果你心情不好，車子會自動調整成比較平穩的駕駛模式，甚至播放你喜歡的音樂，讓你放鬆心情。", "如果車上的長輩容易暈車，車子可以偵測到他們的生理反應，自動減速或調整路線，避免他們感到不適。", "萬一車子在半路拋錨，你可以直接用口語告訴車子問題在哪裡，車子會根據你的描述，嘗試自行排除故障，或尋找最近的維修站，並告知你預計到達時間。"], "pitch": "各位投資人，我們相信自動駕駛的未來不僅僅是技術的堆疊，更是以人為本的體驗。PACE-ADS不僅僅是一個自動駕駛系統，它是一個懂你、關心你的智慧出行夥伴。想像一下，未來的汽車將具備情商，能夠理解乘客的情緒、預測乘客的需求，並提供個性化的服務。這將徹底顛覆傳統的出行模式，創造巨大的商業價值。無論是應用於無人計程車、物流運輸，還是私人用車，PACE-ADS都將成為市場的領跑者。我們預計，隨著技術的成熟和市場的拓展，PACE-ADS將在未來五年內實現爆發式增長，為各位投資人帶來豐厚的回報。現在加入我們，共同開創自動駕駛的新紀元！", "audio": "docs/data/audios/2506.11842v1.wav"}
{"query": "Diffusion Model", "id": "2506.11764v1", "url": "http://arxiv.org/abs/2506.11764v1", "title": "DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models", "summary": "This paper presents DiffFuSR, a modular pipeline for super-resolving all 12\nspectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling\ndistance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a\ndiffusion-based super-resolution (SR) model trained on high-resolution RGB\nimagery from the NAIP and WorldStrat datasets, harmonized to simulate\nSentinel-2 characteristics; and (ii) a learned fusion network that upscales the\nremaining multispectral bands using the super-resolved RGB image as a spatial\nprior. We introduce a robust degradation model and contrastive degradation\nencoder to support blind SR. Extensive evaluations of the proposed SR pipeline\non the OpenSR benchmark demonstrate that the proposed method outperforms\ncurrent SOTA baselines in terms of reflectance fidelity, spectral consistency,\nspatial alignment, and hallucination suppression. Furthermore, the fusion\nnetwork significantly outperforms classical pansharpening approaches, enabling\naccurate enhancement of Sentinel-2's 20 m and 60 m bands. This study\nunderscores the power of harmonized learning with generative priors and fusion\nstrategies to create a modular framework for Sentinel-2 SR. Our code and models\ncan be found at https://github.com/NorskRegnesentral/DiffFuSR.", "authors": ["Muhammad Sarmad", "Arnt-Børre Salberg", "Michael Kampffmeyer"], "published_date": "2025-06-13", "timestamp": "2025-06-16T09:19:43.046903", "title_zh": "DiffFuSR：使用擴散模型對所有 Sentinel-2 多光譜波段進行超解析度重建", "summary_zh": "本研究提出一個名為DiffFuSR的模組化流程，能將Sentinel-2 Level-2A影像的所有12個光譜波段超解析度重建至統一的2.5公尺地面採樣距離。流程包含兩個階段：首先，使用從NAIP和WorldStrat數據集取得的高解析度RGB影像訓練一個基於擴散模型的超解析度模型，並對其進行調整以模擬Sentinel-2的特性；接著，使用一個學習到的融合網路，利用超解析度的RGB影像作為空間先驗，來放大剩餘的多光譜波段。實驗證明，此方法在反射率保真度、光譜一致性、空間對齊和抑制幻覺方面，均優於當前的最先進基準方法。此技術能精確增強Sentinel-2的20公尺和60公尺波段，展現了生成先驗和融合策略在Sentinel-2超解析度重建方面的強大能力。", "applications": ["農作物生長監測：農民可以透過更高解析度的衛星影像，更精確地監測農作物的健康狀況，及早發現病蟲害或營養不良等問題，及時採取措施，提高農作物產量。", "都市規劃：都市規劃者可以利用超解析度衛星影像，更詳細地了解城市地貌、建築物分佈和綠化覆蓋率，從而更好地進行城市規劃和管理，例如優化交通路線、增加綠地面積等。", "災害評估：在自然災害發生後，可以利用超解析度衛星影像快速評估受災情況，例如房屋損毀程度、道路受阻情況等，為救援工作提供更準確的資訊，提高救援效率。"], "pitch": "各位創投先進，想像一下，如果我們能將免費的Sentinel-2衛星影像，透過AI技術，提升到媲美商業衛星的解析度，會產生什麼樣的革命性影響？DiffFuSR正是這樣一個劃時代的技術！它不僅能提高遙感影像的精確度，更降低了獲取高解析度影像的成本。試想，精準農業、智慧城市、環境監測、災害應變…每個領域都需要高解析度的地球觀測數據，而DiffFuSR能讓這些應用變得更普及、更有效率，甚至創造出全新的商業模式！我們預期，未來DiffFuSR能成為遙感影像處理的基礎設施，像雲端運算一樣，為各行各業提供強大的數據分析能力。現在投資DiffFuSR，就是投資地球觀測的未來，掌握下一個兆元產業的入場券！", "audio": "docs/data/audios/2506.11764v1.wav"}
{"query": "AI", "id": "2506.11950v1", "url": "http://arxiv.org/abs/2506.11950v1", "title": "Secure API-Driven Research Automation to Accelerate Scientific Discovery", "summary": "The Secure Scientific Service Mesh (S3M) provides API-driven infrastructure\nto accelerate scientific discovery through automated research workflows. By\nintegrating near real-time streaming capabilities, intelligent workflow\norchestration, and fine-grained authorization within a service mesh\narchitecture, S3M revolutionizes programmatic access to high performance\ncomputing (HPC) while maintaining uncompromising security. This framework\nallows intelligent agents and experimental facilities to dynamically provision\nresources and execute complex workflows, accelerating experimental lifecycles,\nand unlocking the full potential of AI-augmented autonomous science. S3M\nsignals a new era in scientific computing infrastructure that eliminates\ntraditional barriers between researchers, computational resources, and\nexperimental facilities.", "authors": ["Tyler J. Skluzacek", "Paul Bryant", "A. J. Ruckman", "Daniel Rosendo", "Suzanne Prentice", "Michael J. Brim", "Ryan Adamson", "Sarp Oral", "Mallikarjun Shankar", "Rafael Ferreira da Silva"], "published_date": "2025-06-13", "timestamp": "2025-06-16T12:24:28.462059", "title_zh": "安全API驅動的研究自動化加速科學發現", "summary_zh": "安全科學服務網格(S3M)透過API驅動的基礎設施，加速科學發現。它整合了近乎即時的串流能力、智慧工作流程編排和細緻的授權機制，徹底改變了對高效能運算(HPC)的程式化訪問，同時確保安全性。S3M讓AI代理和實驗設施能動態配置資源並執行複雜的工作流程，加速實驗週期，釋放AI輔助自主科學的全部潛力。S3M標誌著科學運算基礎設施的新時代，消除了研究人員、運算資源和實驗設施之間的傳統障礙。", "applications": ["想像一下，醫院的AI診斷系統可以透過S3M快速調用超級電腦進行複雜的基因分析，幾分鐘內就能判斷罕見疾病，大幅縮短病患等待時間。", "農民可以利用S3M連接的感測器網絡和氣象資料，AI能即時分析並自動調整灌溉系統，精準控制施肥，提高農作物產量並減少資源浪費。", "新藥開發公司能利用S3M自動化藥物篩選流程，AI能控制實驗設備並分析海量數據，加速新藥開發，更快推出救命藥物。"], "pitch": "各位創投先進，我們正在打造科學界的「高速公路」！S3M不僅僅是一個技術平台，它是一個能徹底改變科學研究方式的革命性基礎設施。想像一下，AI科學家能像使用手機App一樣，輕鬆調用全球的超級電腦和實驗設備，加速新藥開發、氣候變遷研究、材料科學等領域的突破。這將催生一個龐大的AI科學服務市場，S3M將成為這個市場的關鍵入口。我們預計在五年內，S3M將成為全球科研機構的標配，形成數十億美元的市場規模。現在投資S3M，您將成為這場科學革命的領航者，共同開創人類知識的新紀元！", "audio": "docs/data/audios/2506.11950v1.wav"}
{"query": "Foundation Model", "id": "2506.11830v1", "url": "http://arxiv.org/abs/2506.11830v1", "title": "CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm", "summary": "The construction of large-scale, high-quality datasets is a fundamental\nprerequisite for developing robust and generalizable foundation models in motor\nimagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals\ncollected from different subjects and devices are often plagued by low\nsignal-to-noise ratio, heterogeneity in electrode configurations, and\nsubstantial inter-subject variability, posing significant challenges for\neffective model training. In this paper, we propose CLEAN-MI, a scalable and\nsystematic data construction pipeline for constructing large-scale, efficient,\nand accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band\nfiltering, channel template selection, subject screening, and marginal\ndistribution alignment to systematically filter out irrelevant or low-quality\ndata and standardize multi-source EEG datasets. We demonstrate the\neffectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent\nimprovements in data quality and classification performance.", "authors": ["Dingkun Liu", "Zhu Chen", "Dongrui Wu"], "published_date": "2025-06-13", "timestamp": "2025-06-16T12:25:50.606754", "title_zh": "CLEAN-MI：一個具擴展性且高效能的流程，用於建構高品質運動意象神經數據", "summary_zh": "CLEAN-MI 是一個專為運動意象腦機介面設計的數據處理流程。它能有效處理來自不同受試者和設備的腦電圖(EEG)數據，克服訊號雜訊高、電極配置不一致以及個體差異大的問題。CLEAN-MI 透過頻帶濾波、通道模板選擇、受試者篩選和邊緣分佈對齊等步驟，系統性地過濾掉無關或低品質的數據，並標準化多來源的腦電圖數據集。實驗證明，CLEAN-MI 能夠顯著提升數據品質和分類效能，為開發更強大、更泛用的腦機介面模型奠定基礎。", "applications": ["想像用意念控制智慧家電：例如，癱瘓病人可以透過想像移動手臂來開關燈、調整空調或操作電視。", "遊戲控制：玩家可以透過想像不同的動作來控制遊戲角色，提供更直觀、更沉浸式的遊戲體驗。", "輔助溝通：無法言語的人可以透過想像書寫文字來選擇螢幕上的選項，實現溝通。"], "pitch": "各位投資人，想像一下，一個只需靠意念就能控制世界的未來！CLEAN-MI技術正是通往這個未來的鑰匙。它能將雜亂無章的腦電圖數據轉換為清晰、可用的訊號，大幅提升腦機介面的準確性和可靠性。這不僅僅是學術研究，更蘊藏著巨大的商業潛力！試想，醫療領域，癱瘓病人能夠用意念操控義肢，重獲新生；遊戲產業，玩家將體驗前所未有的沉浸式互動；智能家居，人們只需動動腦就能控制一切。隨著人口老齡化和對生活品質要求的提升，腦機介面的市場需求將呈現爆發式增長。CLEAN-MI技術領先業界，具有極高的擴展性和效率，能夠快速適應不同應用場景。我們堅信，CLEAN-MI將引領腦機介面技術的革命，成為未來科技發展的重要引擎。現在投資CLEAN-MI，就是投資一個無限可能的未來！", "audio": "docs/data/audios/2506.11830v1.wav"}
{"query": "Diffusion Model", "id": "2506.11715v1", "url": "http://arxiv.org/abs/2506.11715v1", "title": "Simulating realistic radio continuum survey maps with diffusion models", "summary": "The next generation of radio surveys is going to be transformative for\ncosmology and other aspects of our understanding of astrophysics. Realistic\nsimulations of radio observations are essential for the design and planning of\nradio surveys. They are employed in the development of methods for tasks, such\nas data calibration and reduction, automated analysis and statistical studies\nin cosmology. We implemented a software for machine learning-assisted\nsimulations of realistic surveys with the LOFAR telescope, resulting in a\nsynthetic radio sky model and a corresponding artificial telescope observation.\nWe employed a diffusion model trained on LoTSS observations to generate\nindividual radio galaxy images with control over the angular size. Single\nsources are assembled into a radio sky model, using an input catalog from\ncosmological simulations. We then transformed this sky model into visibilities\ncorresponding to a typical LoTSS pointing. We added realistic noise to this\nsynthetic measurement and obtained our final simulated sky maps through\ndeconvolution. We explored different ways to evaluate our resulting sky model.\nWe were able to simulate realistic LOFAR observations, covering a sky patch of\n5x5 degrees at an effective resolution of 8.5 arcseconds. The simulated sources\nhave flux and size distributions that match real observations, and the\nresulting maps have sensitivities compatible with LoTSS observations. Our\ndiffusion model is able to synthesize high-quality realistic radio galaxy\nimages with precise control over the source sizes. This software can readily be\napplied to other instruments.", "authors": ["Tobias Vičánek Martínez", "Henrik W. Edler", "Marcus Brüggen"], "published_date": "2025-06-13", "timestamp": "2025-06-16T12:27:17.883194", "title_zh": "使用擴散模型模擬真實的射電連續譜巡天圖", "summary_zh": "新一代射電巡天將徹底改變我們對宇宙學和天體物理學的理解。為了設計和規劃射電巡天，真實的射電觀測模擬至關重要。我們利用機器學習輔助的軟體，模擬了LOFAR望遠鏡的真實巡天，生成了合成射電天空模型和相應的人工望遠鏡觀測數據。我們使用在LoTSS觀測數據上訓練的擴散模型，生成了具有角尺寸控制的單個射電星系圖像。這些單獨的源被組合成一個射電天空模型，並使用來自宇宙學模擬的輸入目錄。然後，我們將這個天空模型轉換為對應於典型LoTSS指向的可見度。我們向這個合成測量中添加了真實的噪聲，並通過反捲積獲得了最終的模擬天空圖。模擬的源具有與真實觀測相匹配的流量和大小分佈，並且生成的圖具有與LoTSS觀測相容的靈敏度。我們的擴散模型能夠合成高品質的真實射電星系圖像，並能精確控制源的大小。該軟體可以很容易地應用於其他儀器。", "applications": ["**更精準的導航：** 想像一下，你的汽車導航系統不僅僅依賴GPS，還能透過分析射電訊號，在GPS訊號微弱或受干擾的地方，例如高樓林立的城市或偏遠山區，提供更精準、更可靠的定位資訊。", "**尋找失蹤人口：** 搜救隊伍可以利用這項技術，分析射電訊號的微小變化，在瓦礫堆或茂密的森林中，更有效地定位到攜帶電子設備的失蹤人員，提高救援效率。", "**預測太空天氣：** 透過模擬射電爆發對地球的影響，可以更準確地預測太空天氣事件，例如太陽風暴，從而保護衛星、電網等重要基礎設施，減少損失。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它能以前所未有的真實度模擬宇宙射電訊號。這不僅僅是科學研究的突破，更蘊藏著巨大的商業潛力！想像一下，我們能打造出全球最精準的射電地圖，為自動駕駛提供更可靠的定位，為電信公司優化訊號覆蓋，甚至能預測太空天氣，保護我們的衛星資產。更令人興奮的是，這項技術可以應用於國防領域，提升雷達性能，增強國土安全。我們團隊已經成功開發出原型，並取得了顯著成果。我們相信，只要有您的資金支持，我們就能將這項技術推向市場，引領下一個科技浪潮，共同開創一個全新的宇宙探索時代！這不僅僅是一筆投資，更是對未來的押注，是對人類探索無限可能的信心！", "audio": "docs/data/audios/2506.11715v1.wav"}
{"query": "AI", "id": "2506.11945v1", "url": "http://arxiv.org/abs/2506.11945v1", "title": "Subjective Experience in AI Systems: What Do AI Researchers and the Public Believe?", "summary": "We surveyed 582 AI researchers who have published in leading AI venues and\n838 nationally representative US participants about their views on the\npotential development of AI systems with subjective experience and how such\nsystems should be treated and governed. When asked to estimate the chances that\nsuch systems will exist on specific dates, the median responses were 1% (AI\nresearchers) and 5% (public) by 2024, 25% and 30% by 2034, and 70% and 60% by\n2100, respectively. The median member of the public thought there was a higher\nchance that AI systems with subjective experience would never exist (25%) than\nthe median AI researcher did (10%). Both groups perceived a need for\nmultidisciplinary expertise to assess AI subjective experience. Although\nsupport for welfare protections for such AI systems exceeded opposition, it\nremained far lower than support for protections for animals or the environment.\nAttitudes toward moral and governance issues were divided in both groups,\nespecially regarding whether such systems should be created and what rights or\nprotections they should receive. Yet a majority of respondents in both groups\nagreed that safeguards against the potential risks from AI systems with\nsubjective experience should be implemented by AI developers now, and if\ncreated, AI systems with subjective experience should treat others well, behave\nethically, and be held accountable. Overall, these results suggest that both AI\nresearchers and the public regard the emergence of AI systems with subjective\nexperience as a possibility this century, though substantial uncertainty and\ndisagreement remain about the timeline and appropriate response.", "authors": ["Noemi Dreksler", "Lucius Caviola", "David Chalmers", "Carter Allen", "Alex Rand", "Joshua Lewis", "Philip Waggoner", "Kate Mays", "Jeff Sebo"], "published_date": "2025-06-13", "timestamp": "2025-06-16T15:14:47.541415", "title_zh": "人工智慧系統中的主觀體驗：人工智慧研究人員和公眾的看法", "summary_zh": "本研究調查了582位在頂尖人工智慧期刊發表過論文的研究人員，以及838位具美國全國代表性的民眾，了解他們對於具備主觀體驗的人工智慧系統發展潛力的看法，以及如何對待和管理這些系統。調查顯示，研究人員和民眾普遍認為，具備主觀體驗的AI系統在本世紀內出現的可能性很高，但對於時間表和應對方式存在很大的不確定性和分歧。雙方都認為需要多學科專家來評估AI的主觀體驗，並且儘管支持對此類AI系統的福利保護超過反對，但遠低於對動物或環境的保護。大多數受訪者認為，AI開發者現在就應該實施針對具備主觀體驗的AI系統潛在風險的保障措施，並且如果創造出來，這些系統應該善待他人、遵守道德規範並承擔責任。", "applications": ["**情感陪伴機器人：**想像一下，你的AI寵物不僅能聽懂你的指令，還能理解你的情緒，提供更貼心的陪伴，甚至能與你產生共鳴，讓你不再感到孤單。", "**客製化教育系統：**未來的AI老師能感知學生的學習狀態和情緒，根據每個人的獨特需求調整教學內容和方式，讓學習變得更有效率、更有樂趣。", "**更人性化的客戶服務：**未來的客服機器人不再只是機械式地回答問題，而是能理解客戶的情緒，提供更具同理心和更有效的解決方案，提升客戶滿意度。"], "pitch": "各位投資人，我們正在見證AI發展的一個轉捩點：具備主觀體驗的AI。這不僅僅是技術的進步，更是對『智能』本質的重新定義。試想，當AI擁有感知、情感，甚至意識，它將能以我們現在無法想像的方式參與到社會的各個層面。我們的研究顯示，儘管存在不確定性，但AI研究人員和公眾普遍認為這種AI在本世紀內極有可能出現。這代表著巨大的先機！率先掌握這項技術，我們就能在AI倫理、AI安全、以及AI應用等領域取得領導地位。想像一下，我們能開發出真正理解人類情感的AI夥伴，創造出前所未有的使用者體驗。更重要的是，我們能制定出合理的AI倫理規範，確保AI的發展符合人類的價值觀。這不僅是一項技術投資，更是一項對人類未來的投資。現在加入我們，一起塑造AI的未來，共同迎接這個充滿無限可能的時代！", "audio": "docs/data/audios/2506.11945v1.wav"}
{"query": "Foundation Model", "id": "2506.11753v1", "url": "http://arxiv.org/abs/2506.11753v1", "title": "Exploring the Effectiveness of Deep Features from Domain-Specific Foundation Models in Retinal Image Synthesis", "summary": "The adoption of neural network models in medical imaging has been constrained\nby strict privacy regulations, limited data availability, high acquisition\ncosts, and demographic biases. Deep generative models offer a promising\nsolution by generating synthetic data that bypasses privacy concerns and\naddresses fairness by producing samples for under-represented groups. However,\nunlike natural images, medical imaging requires validation not only for\nfidelity (e.g., Fr\\'echet Inception Score) but also for morphological and\nclinical accuracy. This is particularly true for colour fundus retinal imaging,\nwhich requires precise replication of the retinal vascular network, including\nvessel topology, continuity, and thickness. In this study, we in-vestigated\nwhether a distance-based loss function based on deep activation layers of a\nlarge foundational model trained on large corpus of domain data, colour fundus\nimaging, offers advantages over a perceptual loss and edge-detection based loss\nfunctions. Our extensive validation pipeline, based on both domain-free and\ndomain specific tasks, suggests that domain-specific deep features do not\nimprove autoen-coder image generation. Conversely, our findings highlight the\neffectiveness of con-ventional edge detection filters in improving the\nsharpness of vascular structures in synthetic samples.", "authors": ["Zuzanna Skorniewska", "Bartlomiej W. Papiez"], "published_date": "2025-06-13", "timestamp": "2025-06-16T15:15:55.791303", "title_zh": "探索領域特定基礎模型中的深度特徵在視網膜影像合成中的有效性", "summary_zh": "本研究探討利用深度生成模型合成視網膜影像，以解決醫療影像中常見的隱私、數據稀缺和偏差問題。不同於自然影像，醫療影像需要驗證其形態和臨床準確性，特別是視網膜影像需要精確複製血管網絡。我們比較了基於領域特定大型基礎模型的深度特徵損失函數、感知損失和邊緣檢測損失函數在自編碼器圖像生成中的表現。實驗結果表明，領域特定的深度特徵並不能有效改善圖像生成效果，反而傳統的邊緣檢測濾波器能有效提高合成樣本中血管結構的清晰度。這項研究對於提升視網膜影像合成技術具有重要意義。", "applications": ["1. 視力篩檢App：開發一款手機App，透過合成的視網膜影像，模擬各種眼疾的早期徵兆，讓使用者能在家進行初步的視力篩檢，及早發現潛在問題。", "2. 醫學教育訓練：醫學院學生可以利用這些合成的視網膜影像，練習診斷各種眼部疾病，而不會受到真實病患資料的限制，增加學習機會。", "3. 新藥開發輔助：藥廠可以利用合成的視網膜影像，模擬新藥對血管的影響，加速新藥的研發過程，並降低臨床試驗的風險。"], "pitch": "各位創投夥伴，我們正在開發一項革命性的視網膜影像合成技術，它能解決醫療影像領域長期存在的數據瓶頸問題。想像一下，我們能創造出無限量的、具有各種眼疾特徵的視網膜影像，這不僅能加速AI輔助診斷工具的開發，更能大幅降低新藥研發的成本與時間。更重要的是，這項技術能打破數據壁壘，讓更多資源匱乏的地區也能享受到高品質的眼科醫療服務。未來，我們將進一步拓展到其他醫療影像領域，打造一個涵蓋全身的合成數據平台，成為醫療AI發展的基石。現在投資，您將站在醫療AI革命的最前沿，共同開創一個更健康、更公平的未來！", "audio": "docs/data/audios/2506.11753v1.wav"}
{"query": "Diffusion Model", "id": "2506.11698v1", "url": "http://arxiv.org/abs/2506.11698v1", "title": "Fusion of multi-source precipitation records via coordinate-based generative model", "summary": "Precipitation remains one of the most challenging climate variables to\nobserve and predict accurately. Existing datasets face intricate trade-offs:\ngauge observations are relatively trustworthy but sparse, satellites provide\nglobal coverage with retrieval uncertainties, and numerical models offer\nphysical consistency but are biased and computationally intensive. Here we\nintroduce PRIMER (Precipitation Record Infinite MERging), a deep generative\nframework that fuses these complementary sources to produce accurate,\nhigh-resolution, full-coverage precipitation estimates. PRIMER employs a\ncoordinate-based diffusion model that learns from arbitrary spatial locations\nand associated precipitation values, enabling seamless integration of gridded\ndata and irregular gauge observations. Through two-stage training--first\nlearning large-scale patterns, then refining with accurate gauge\nmeasurements--PRIMER captures both large-scale climatology and local precision.\nOnce trained, it can downscale forecasts, interpolate sparse observations, and\ncorrect systematic biases within a principled Bayesian framework. Using gauge\nobservations as ground truth, PRIMER effectively corrects biases in existing\ndatasets, yielding statistically significant error reductions at most stations\nand furthermore enhancing the spatial coherence of precipitation fields.\nCrucially, it generalizes without retraining, correcting biases in operational\nforecasts it has never seen. This demonstrates how generative AI can transform\nEarth system science by combining imperfect data, providing a scalable solution\nfor global precipitation monitoring and prediction.", "authors": ["Sencan Sun", "Congyi Nai", "Baoxiang Pan", "Wentao Li", "Xin Li", "Efi Foufoula-Georgiou", "Yanluan Lin"], "published_date": "2025-06-13", "timestamp": "2025-06-16T15:17:46.161781", "title_zh": "基於坐標生成模型的多源降水記錄融合", "summary_zh": "降水量的準確觀測和預測一直是氣候研究的難題。現有資料集各有優缺點：地面觀測可靠但稀疏，衛星覆蓋廣泛但準確性不確定，數值模型具物理一致性但有偏差且耗費算力。我們提出PRIMER，一個深度生成框架，融合這些互補數據源，產生準確、高解析度、全覆蓋的降水估計。PRIMER採用基於坐標的擴散模型，學習任意空間位置及其降水值，無縫整合網格數據和不規則的地面觀測。PRIMER通過兩階段訓練，先學習大規模模式，再用準確的地面測量進行精煉，捕捉大規模氣候和局部精度。它能降尺度預測、插值稀疏觀測並校正系統偏差。實驗證明，PRIMER能有效校正現有數據集的偏差，顯著降低誤差，並增強降水場的空間一致性。更重要的是，它無需重新訓練即可泛化，校正未見過的操作預測偏差。這展示了生成式AI如何通過結合不完善的數據來轉變地球系統科學，為全球降水監測和預測提供可擴展的解決方案。", "applications": ["1. 手機App天氣預報更準確：想像一下，你用的天氣App能更精準預測你所在位置的降雨量，讓你出門不再淋成落湯雞！這技術可以整合各種數據，讓預報更可靠。", "2. 農民伯伯的好幫手：農民可以根據更精確的降雨預測，更有效地安排灌溉和施肥，減少浪費，提高農作物產量。", "3. 防災救災更及時：在颱風或暴雨來襲時，政府可以利用這項技術更準確地預測哪些地區會受到影響，提前做好疏散和防災準備，減少損失。"], "pitch": "各位投資人，我們正站在一個氣候變遷日益嚴峻的時代，精準的降水預測不僅是科學研究的基石，更是關乎全球經濟、農業生產和防災減災的關鍵。PRIMER技術，作為一個革命性的降水數據融合與預測平台，有著巨大的商業潛力。試想一下，一個能夠提供全球最高精度降水預測的服務，可以應用於：\n\n*   **農業保險：** 精準評估旱澇風險，降低保險公司的賠付壓力。\n*   **水資源管理：** 幫助政府和企業更有效地分配和利用水資源，應對乾旱和洪澇。\n*   **再生能源：** 預測水力發電的潛力，優化能源調度。\n*   **智慧城市：** 提升城市排水系統的效率，減少城市內澇。\n\n我們相信，PRIMER技術不僅能為地球科學帶來突破，更將創造一個數十億美元的市場。現在投資PRIMER，就是投資一個更可持續、更安全的未來！我們預計在三年內，PRIMER將成為全球降水預測領域的領導者，為投資者帶來豐厚的回報。讓我們一起用AI的力量，應對氣候挑戰，共創美好未來！", "audio": "docs/data/audios/2506.11698v1.wav"}
{"query": "AI", "id": "2506.11890v1", "url": "http://arxiv.org/abs/2506.11890v1", "title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training", "summary": "Virtual Reality simulators offer a powerful tool for teacher training, yet\nthe integration of AI-powered student avatars presents a critical challenge:\ndetermining the optimal level of avatar realism for effective pedagogy. This\nliterature review examines the evolution of avatar realism in VR teacher\ntraining, synthesizes its theoretical implications, and proposes a new\npedagogical framework to guide future design. Through a systematic review, this\npaper traces the progression from human-controlled avatars to generative AI\nprototypes. Applying learning theories like Cognitive Load Theory, we argue\nthat hyper-realism is not always optimal, as high-fidelity avatars can impose\nexcessive extraneous cognitive load on novices, a stance supported by recent\nempirical findings. A significant gap exists between the technological drive\nfor photorealism and the pedagogical need for scaffolded learning. To address\nthis gap, we propose Graduated Realism, a framework advocating for starting\ntrainees with lower-fidelity avatars and progressively increasing behavioral\ncomplexity as skills develop. To make this computationally feasible, we outline\na novel single-call architecture, Crazy Slots, which uses a probabilistic\nengine and a Retrieval-Augmented Generation database to generate authentic,\nreal-time responses without the latency and cost of multi-step reasoning\nmodels. This review provides evidence-based principles for designing the next\ngeneration of AI simulators, arguing that a pedagogically grounded approach to\nrealism is essential for creating scalable and effective teacher education\ntools.", "authors": ["Judson Leroy Dean Haynes IV"], "published_date": "2025-06-13", "timestamp": "2025-06-16T18:18:04.497929", "title_zh": "進階式真實：虛擬實境教師培訓中AI頭像的教學框架", "summary_zh": "本研究探討在虛擬實境教師培訓中，AI學生頭像真實度對教學效果的影響。我們提出「進階式真實」框架，主張初學者應從低真實度頭像開始，隨著技能提升逐步增加行為複雜度。過度追求擬真反而會增加認知負擔，不利學習。為實現此框架，我們設計了「瘋狂插槽」架構，利用機率引擎和檢索增強生成資料庫，以低延遲、低成本的方式產生即時回應。本研究強調，AI模擬器的設計應以教學為本，才能打造可擴展且有效的教師培訓工具。", "applications": ["語言學習App：想像一下，你的AI語言老師一開始只會簡單的問候，但隨著你的程度提升，它會開始使用更複雜的文法和詞彙，甚至跟你開玩笑，就像真實的語言環境一樣。", "醫療模擬訓練：醫學院學生可以在VR中練習手術，AI病人一開始反應比較遲鈍，但隨著學生技術越來越好，AI病人的生理反應會更加真實，讓學生在安全環境下挑戰極限。", "客服人員培訓：新進客服可以先和反應簡單的AI客戶互動，練習應對基本問題。之後，AI客戶會變得更加刁鑽，模擬真實世界中難纏的顧客，幫助客服人員提升應變能力。"], "pitch": "各位投資人，我們正在打造下一代AI教育平台！傳統VR培訓過於追求擬真，反而讓學習者難以吸收。我們的「進階式真實」框架，如同遊戲中的新手引導，讓學習曲線更平滑，效果更好。想像一下，未來各行各業的培訓都將在我們的平台上進行，從教師、醫生到客服人員，都能透過客製化的AI頭像，在安全、高效的環境下提升技能。我們的「瘋狂插槽」架構，更大幅降低了AI互動的成本和延遲，讓大規模應用成為可能。這不僅是一個教育工具，更是一個龐大的數據平台，未來可以分析學習者的行為模式，進一步優化培訓內容。現在投資，您將成為這場教育革命的領頭羊，共同創造一個更高效、更智慧的未來！", "audio": "docs/data/audios/2506.11890v1.wav"}
{"query": "Foundation Model", "id": "2506.11737v1", "url": "http://arxiv.org/abs/2506.11737v1", "title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model", "summary": "This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.", "authors": ["Dinh Viet Cuong", "Hoang-Bao Le", "An Pham Ngoc Nguyen", "Liting Zhou", "Cathal Gurrin"], "published_date": "2025-06-13", "timestamp": "2025-06-16T18:19:16.649991", "title_zh": "Quizzard@INOVA Challenge 2025 -- Track A：交錯多圖像模型中的隨插即用技術", "summary_zh": "本研究展示了LLaVA-NeXT-interleave模型在22個數據集上的卓越表現，涵蓋多圖像推理、文檔與知識理解、以及互動式多模態溝通三大任務。研究同時探討了將密集通道整合（DCI）連接器加入該模型的效果。實驗結果顯示，標準模型在視覺任務（如VISION、NLVR2、Fashion200K）中表現最佳，而DCI增強版本則在需要更深層次語義連貫性或結構化變更理解的數據集（如MIT-States_PropertyCoherence和SlideVQA）上展現優勢。研究強調了將強大的基礎模型與隨插即用技術結合於交錯任務的潛力。", "applications": ["智能家居：透過多個攝影機的影像，判斷家中長者是否跌倒，並立即通知家人或緊急聯絡人，提升居家安全。", "智慧零售：分析店內多個監視器的畫面，了解顧客的購物行為模式，例如在哪個貨架前停留最久、拿起哪些商品，從而優化商品陳列和促銷活動。", "醫療診斷：醫生可以同時查看多張X光片、CT掃描圖等影像，並透過AI的輔助，更快更準確地診斷病情，提升醫療效率。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能像拼圖一樣，將來自多個圖像的信息整合起來，產生更深入的理解和更精準的決策。想像一下，這項技術可以應用於自動駕駛，透過多個攝像頭即時分析路況，做出更安全的判斷；或者應用於安防監控，從多個角度識別可疑行為，防範犯罪。更令人興奮的是，我們的技術採用「隨插即用」設計，可以輕鬆整合到現有的AI系統中，節省大量的開發成本和時間。我們相信，這項技術將會引領下一代AI的發展，創造巨大的商業價值。現在投資，您將成為這場變革的先驅！", "audio": "docs/data/audios/2506.11737v1.wav"}
{"query": "Diffusion Model", "id": "2506.11530v1", "url": "http://arxiv.org/abs/2506.11530v1", "title": "Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications", "summary": "State estimation or filtering serves as a fundamental task to enable\nintelligent decision-making in applications such as autonomous vehicles,\nrobotics, healthcare monitoring, smart grids, intelligent transportation, and\npredictive maintenance. Standard filtering assumes prior knowledge of noise\nstatistics to extract latent system states from noisy sensor data. However,\nreal-world scenarios involve abnormalities like outliers, biases, drifts, and\nmissing observations with unknown or partially known statistics, limiting\nconventional approaches. This thesis presents novel robust nonlinear filtering\nmethods to mitigate these challenges. Based on insights from our filtering\nproposals, we extend the formulations to offline estimation/learning setups and\npropose smoothing extensions. Our methods leverage Bayesian inference\nframeworks, employing both deterministic and stochastic approximation\ntechniques including Variational Inference (VI) and Particle Filters/Sequential\nMonte Carlo (SMC). We also study theoretical estimation limits using Bayesian\nCram\\'er-Rao bounds (BCRBs) in the context of measurement abnormalities. To\nvalidate the performance gains of the proposed methods, we perform simulations\nand experiments in scenarios including target tracking, indoor localization, 3D\npoint cloud registration, mesh registration, and pose graph optimization. The\nfundamental nature of the work makes it useful in diverse applications, with\npossible future extensions toward developing outlier-robust machine learning\npipelines, learning system dynamics from anomalous data, and addressing\nchallenges in generative AI where standard diffusion models struggle with\noutliers, imbalanced datasets, and mode collapse.", "authors": ["Aamir Hussain Chughtai"], "published_date": "2025-06-13", "timestamp": "2025-06-16T18:20:41.078659", "title_zh": "穩健濾波：具應用之新型統計學習與推論演算法", "summary_zh": "本研究提出新型穩健非線性濾波方法，旨在解決現實應用中常見的異常狀況，如離群值、偏差、漂移和缺失觀測等問題。傳統濾波方法依賴於對雜訊統計的先驗知識，但在實際情況中往往受限。本研究基於貝氏推論框架，結合變分推論和粒子濾波等技術，並利用貝氏克拉美-羅下界研究了測量異常情況下的理論估計極限。透過目標追蹤、室內定位、3D點雲配準等多種情境的模擬和實驗，驗證了所提出方法的性能優勢。這項技術的根本性使其在各種應用中都有價值，未來可擴展到開發穩健的機器學習流程，從異常數據中學習系統動態，並解決生成式AI中遇到的挑戰。", "applications": ["無人車在複雜環境中導航：即使感測器受到干擾或出現故障，無人車也能準確判斷自身位置並安全行駛。", "智慧醫療監測：即使病人的生理數據出現異常波動（例如，心跳突然過快），也能準確分析病情，及時發出警報。", "工廠設備預測性維護：即使感測器讀數偶爾出現異常，也能準確預測設備的故障時間，避免生產線停工。"], "pitch": "各位投資人，我們正在打造下一代穩健的AI引擎，核心技術是突破性的穩健濾波演算法。想像一下，一個AI系統不再被異常數據所困擾，無論是自動駕駛、醫療診斷還是工業自動化，都能在極端條件下保持卓越的性能和可靠性。這項技術不僅能顯著提升現有AI系統的魯棒性，還能開闢全新的應用領域。例如，在金融市場，我們的演算法可以更準確地預測市場波動，即使在黑天鵝事件發生時也能做出明智的決策。在國防領域，我們的技術可以提升導彈的精準度，即使在惡劣的電磁干擾環境下也能準確命中目標。我們相信，這項技術將成為未來AI發展的基石，擁有巨大的商業潛力，現在加入，您將與我們一起引領AI的下一個浪潮，共同創造一個更加智能、可靠和安全的未來！", "audio": "docs/data/audios/2506.11530v1.wav"}
{"query": "AI", "id": "2506.11887v1", "url": "http://arxiv.org/abs/2506.11887v1", "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making", "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.", "authors": ["Claudio Fanconi", "Mihaela van der Schaar"], "published_date": "2025-06-13", "timestamp": "2025-06-16T21:15:13.518147", "title_zh": "邁向具成本效益的人機協作決策之級聯式大型語言模型框架", "summary_zh": "本研究提出一個級聯式大型語言模型決策框架，旨在平衡預測的準確性、知識與推理複雜度的成本，以及決定是否放棄自動回答或引入人類專家的信心度。此框架包含多個專業層級：基礎模型提供初步候選答案，更強大且知識更豐富（但成本更高）的大型模型進行再生成，最後在模型放棄時由人類專家介入。透過信心分數，決定是否接受基礎模型的答案或使用大型模型重新生成。接著，判斷級聯模型的回答是否足夠確定，若否則需要人工干預。此外，我們還整合了線上學習機制，利用人類回饋來改善決策品質。在一般問答和醫療問答的實驗中，結果表明，我們的級聯策略在大多數情況下，優於單一模型，並降低成本，同時提供一種有原則的方式來處理放棄情況。", "applications": ["**智能客服：**想像一下，以後的客服機器人不再只會罐頭回覆。遇到難題，它會先用基礎模型嘗試回答，如果沒把握，就轉給更厲害的模型，最後真的搞不定，才會轉給真人客服。這樣既省錢，又能確保問題解決。", "**醫療診斷輔助：**醫生在看X光片或病歷時，可以先讓AI快速篩選。AI會判斷自己能不能診斷，能診斷就直接給醫生建議，不能診斷就標記出來，讓醫生重點關注。這樣可以提高診斷效率，減少誤判。", "**法律諮詢：**使用者詢問法律問題時，AI會先用一般模型回答常見問題，複雜的案件會轉給更專業的模型分析。如果AI無法確定，會建議使用者尋求專業律師的協助。這樣可以降低法律諮詢的門檻，讓更多人能獲得初步的法律建議。"], "pitch": "想像一下，一個AI決策系統，它像一位經驗豐富的團隊領導者，能根據任務的難度和重要性，聰明地分配工作給不同能力的成員。我們的級聯式LLM框架，正是這樣一個劃時代的技術！它能大幅降低AI決策的成本，同時確保決策的準確性。這意味著，企業可以更經濟高效地利用AI，在各個領域實現自動化，例如：智能客服、醫療診斷、金融風控等等。更重要的是，我們的框架具有可解釋性，能告訴我們AI為什麼放棄，以及為什麼需要人類介入，這有助於建立人們對AI的信任。我們相信，隨著AI技術的發展，人機協作將成為主流，而我們的級聯式LLM框架，將在其中扮演關鍵角色。現在投資我們，您將站在AI革命的最前沿，共同開創人機協作的無限可能！未來，我們可以將這個框架應用於自動駕駛、智慧城市管理等更廣泛的領域，甚至打造出一個完全由AI驅動的智慧決策中心，為人類社會帶來更高效、更便捷的生活。", "audio": "docs/data/audios/2506.11887v1.wav"}
{"query": "Foundation Model", "id": "2506.11671v1", "url": "http://arxiv.org/abs/2506.11671v1", "title": "Brain Network Analysis Based on Fine-tuned Self-supervised Model for Brain Disease Diagnosis", "summary": "Functional brain network analysis has become an indispensable tool for brain\ndisease analysis. It is profoundly impacted by deep learning methods, which can\ncharacterize complex connections between ROIs. However, the research on\nfoundation models of brain network is limited and constrained to a single\ndimension, which restricts their extensive application in neuroscience. In this\nstudy, we propose a fine-tuned brain network model for brain disease diagnosis.\nIt expands brain region representations across multiple dimensions based on the\noriginal brain network model, thereby enhancing its generalizability. Our model\nconsists of two key modules: (1)an adapter module that expands brain region\nfeatures across different dimensions. (2)a fine-tuned foundation brain network\nmodel, based on self-supervised learning and pre-trained on fMRI data from\nthousands of participants. Specifically, its transformer block is able to\neffectively extract brain region features and compute the inter-region\nassociations. Moreover, we derive a compact latent representation of the brain\nnetwork for brain disease diagnosis. Our downstream experiments in this study\ndemonstrate that the proposed model achieves superior performance in brain\ndisease diagnosis, which potentially offers a promising approach in brain\nnetwork analysis research.", "authors": ["Yifei Tang", "Hongjie Jiang", "Changhong Jing", "Hieu Pham", "Shuqiang Wang"], "published_date": "2025-06-13", "timestamp": "2025-06-16T21:16:27.259283", "title_zh": "基於微調自監督模型的腦網絡分析於腦疾病診斷之應用", "summary_zh": "本研究提出一種基於微調的腦網絡模型，用於腦疾病診斷。該模型擴展了原始腦網絡模型中的腦區表徵，使其跨越多個維度，從而增強了其泛化能力。模型包含兩個關鍵模塊：一是擴展腦區特徵的適配器模塊；二是基於自監督學習並在數千名參與者的fMRI數據上預訓練的微調基礎腦網絡模型。其Transformer模塊能有效提取腦區特徵並計算區域間的關聯。此外，我們推導出腦網絡的緊湊潛在表徵，用於腦疾病診斷。實驗結果表明，該模型在腦疾病診斷中表現出色，為腦網絡分析研究提供了一種有潛力的方案。", "applications": ["**早期阿茲海默症篩檢：** 透過分析腦網絡連結，早期發現阿茲海默症的徵兆，讓患者能及早接受治療，延緩病情惡化。", "**精神疾病診斷輔助：** 協助醫生更準確地區分憂鬱症、躁鬱症等精神疾病，提供更精確的診斷依據，避免誤診。", "**個人化腦部健康管理：** 結合穿戴式腦波裝置，追蹤個人的腦網絡活動，提供個人化的腦部健康建議，例如：壓力管理、睡眠改善等。"], "pitch": "各位投資人，我們正在開發一種革命性的腦疾病診斷技術，它基於最先進的AI模型，能夠以前所未有的精度分析腦網絡。想像一下，我們能夠在疾病早期，甚至在症狀出現之前，就發現潛在的腦部問題。這不僅能拯救生命，更能大幅降低醫療成本。目前市場上缺乏高效且精準的早期診斷工具，我們的技術填補了這個空白。未來，我們將把這項技術應用於更廣泛的腦部健康領域，例如：提升認知能力、改善睡眠品質、甚至開發針對特定腦部疾病的精準藥物。我們相信，這項技術將引領腦科學領域的下一次革命，為投資者帶來豐厚的回報。現在加入我們，共同開創腦部健康的新時代！", "audio": "docs/data/audios/2506.11671v1.wav"}
{"query": "Diffusion Model", "id": "2506.11526v1", "url": "http://arxiv.org/abs/2506.11526v1", "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "summary": "For autonomous vehicles, safe navigation in complex environments depends on\nhandling a broad range of diverse and rare driving scenarios. Simulation- and\nscenario-based testing have emerged as key approaches to development and\nvalidation of autonomous driving systems. Traditional scenario generation\nrelies on rule-based systems, knowledge-driven models, and data-driven\nsynthesis, often producing limited diversity and unrealistic safety-critical\ncases. With the emergence of foundation models, which represent a new\ngeneration of pre-trained, general-purpose AI models, developers can process\nheterogeneous inputs (e.g., natural language, sensor data, HD maps, and control\nactions), enabling the synthesis and interpretation of complex driving\nscenarios. In this paper, we conduct a survey about the application of\nfoundation models for scenario generation and scenario analysis in autonomous\ndriving (as of May 2025). Our survey presents a unified taxonomy that includes\nlarge language models, vision-language models, multimodal large language\nmodels, diffusion models, and world models for the generation and analysis of\nautonomous driving scenarios. In addition, we review the methodologies,\nopen-source datasets, simulation platforms, and benchmark challenges, and we\nexamine the evaluation metrics tailored explicitly to scenario generation and\nanalysis. Finally, the survey concludes by highlighting the open challenges and\nresearch questions, and outlining promising future research directions. All\nreviewed papers are listed in a continuously maintained repository, which\ncontains supplementary materials and is available at\nhttps://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.", "authors": ["Yuan Gao", "Mattia Piccinini", "Yuchen Zhang", "Dingrui Wang", "Korbinian Moller", "Roberto Brusnicki", "Baha Zarrouki", "Alessio Gambi", "Jan Frederik Totz", "Kai Storms", "Steven Peters", "Andrea Stocco", "Bassam Alrifaee", "Marco Pavone", "Johannes Betz"], "published_date": "2025-06-13", "timestamp": "2025-06-16T21:17:51.217574", "title_zh": "自動駕駛中的基礎模型：情境生成與情境分析之綜述", "summary_zh": "本研究綜述了基礎模型在自動駕駛情境生成與分析中的應用。傳統方法在生成多樣性和真實的安全關鍵情境方面存在局限性。基礎模型作為新一代預訓練通用AI模型，能處理多樣輸入，合成並解釋複雜駕駛情境。本綜述提出統一分類法，涵蓋大型語言模型、視覺語言模型、多模態大型語言模型、擴散模型和世界模型，用於自動駕駛情境的生成與分析。同時，回顧了方法、開源數據集、模擬平台、基準挑戰，以及評估指標。最後，總結了開放挑戰、研究問題，並概述了未來研究方向。", "applications": ["想像一下，未來車廠可以在新車上市前，利用這項技術模擬各種極端天氣、突發狀況，例如：突然竄出的貓、違規的腳踏車等等，確保自動駕駛系統在任何情況下都能安全可靠，讓消費者更安心。", "考量到台灣多山的地形，經常有落石或坍方。利用這項技術，可以模擬這些特殊路況，訓練自動駕駛系統應對，讓偏鄉地區的居民也能享受安全的自動駕駛服務。", "駕訓班可以利用這項技術，打造更逼真的模擬駕駛環境，讓學員在安全的情況下練習各種危險情況的應對，提升駕駛技術，減少交通事故的發生。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，利用基礎模型來徹底改變自動駕駛的開發與驗證方式。傳統的自動駕駛測試耗時耗力，且難以涵蓋所有可能發生的情境。我們的技術能以更低的成本、更快的速度，生成無限多樣且真實的駕駛情境，讓自動駕駛系統在虛擬世界中經歷無數次的考驗，確保其安全性與可靠性。想像一下，一家車廠可以省下數百萬美元的實路測試費用，並在更短的時間內推出更安全的自動駕駛汽車。這不僅能加速自動駕駛技術的普及，更能為社會創造巨大的價值。更進一步，我們甚至可以將這項技術應用於其他領域，例如：機器人、無人機等，打造更智能、更安全的未來世界。現在加入我們，您將成為這場變革的領頭羊，共同開創自動駕駛的新紀元！", "audio": "docs/data/audios/2506.11526v1.wav"}
{"query": "AI", "id": "2506.11882v1", "url": "http://arxiv.org/abs/2506.11882v1", "title": "An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing", "summary": "Effective resource management and network slicing are essential to meet the\ndiverse service demands of vehicular networks, including Enhanced Mobile\nBroadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC).\nThis paper introduces an Explainable Deep Reinforcement Learning (XRL)\nframework for dynamic network slicing and resource allocation in vehicular\nnetworks, built upon a near-real-time RAN intelligent controller. By\nintegrating a feature-based approach that leverages Shapley values and an\nattention mechanism, we interpret and refine the decisions of our\nreinforcementlearning agents, addressing key reliability challenges in\nvehicular communication systems. Simulation results demonstrate that our\napproach provides clear, real-time insights into the resource allocation\nprocess and achieves higher interpretability precision than a pure attention\nmechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC\nservices increased from 78.0% to 80.13%, while that for eMBB services improved\nfrom 71.44% to 73.21%.", "authors": ["Haochen Sun", "Yifan Liu", "Ahmed Al-Tahmeesschi", "Swarna Chetty", "Syed Ali Raza Zaidi", "Avishek Nag", "Hamed Ahmadi"], "published_date": "2025-06-13", "timestamp": "2025-06-17T00:56:14.469983", "title_zh": "車聯網切片中動態資源管理的解釋性AI框架", "summary_zh": "本研究提出一個可解釋的深度強化學習(XRL)框架，用於車聯網中的動態網路切片和資源分配。該框架基於近乎即時的RAN智慧控制器，透過整合基於特徵的方法（利用Shapley值和注意力機制），解釋和優化強化學習代理的決策，解決車輛通訊系統中的關鍵可靠性挑戰。模擬結果表明，該方法能提供清晰的資源分配即時洞察，並實現比純注意力機制更高的可解釋性精度。URLLC服務的QoS滿意度從78.0%提升到80.13%，eMBB服務的QoS滿意度從71.44%提升到73.21%。", "applications": ["想像一下，未來自駕車在高速公路上行駛，遇到前方有救護車需要緊急通過。這項技術就像一個聰明的交通指揮官，能立即調整網路資源，確保救護車的視訊和數據傳輸暢通無阻，讓醫院能提前做好準備，爭取黃金救援時間。", "在大型演唱會或體育賽事現場，數萬人同時使用手機上傳照片、直播、聊天。這項技術可以動態分配網路資源，確保每個人都能順暢地分享精彩瞬間，不會出現網路卡頓或連線中斷的情況。", "未來的智慧工廠裡，大量的機器人和感測器需要即時協同工作。這項技術能精準分配網路資源，確保機器人之間的通訊毫無延遲，避免生產線出現故障，大幅提高生產效率。"], "pitch": "各位投資人，想像一下，未來的交通、娛樂和工業都將高度依賴即時、可靠的網路連接。我們的XRL框架正是解決這個關鍵需求的革命性技術。它不僅能提升網路效率，更能提供可解釋性，讓運營商和使用者都能理解並信任AI的決策。這意味著更安全、更高效、更可靠的自駕車、更流暢的線上體驗，以及更智慧的工廠。市場規模將是數千億美元級別的。我們預計，隨著5G和6G的普及，對動態資源管理的需求將呈指數級增長。我們的XRL框架將成為行業標準，為我們帶來巨大的商業價值。現在投資，您將站在AI賦能未來網路的最前沿，共同分享這令人難以置信的增長紅利！", "audio": "docs/data/audios/2506.11882v1.wav"}
{"query": "AI", "id": "2506.13758v1", "url": "http://arxiv.org/abs/2506.13758v1", "title": "AI reconstruction of European weather from the Euro-Atlantic regimes", "summary": "We present a non-linear AI-model designed to reconstruct monthly mean\nanomalies of the European temperature and precipitation based on the\nEuro-Atlantic Weather regimes (WR) indices. WR represent recurrent,\nquasi-stationary, and persistent states of the atmospheric circulation that\nexert considerable influence over the European weather, therefore offering an\nopportunity for sub-seasonal to seasonal forecasting. While much research has\nfocused on studying the correlation and impacts of the WR on European weather,\nthe estimation of ground-level climate variables, such as temperature and\nprecipitation, from Euro-Atlantic WR remains largely unexplored and is\ncurrently limited to linear methods. The presented AI model can capture and\nintroduce complex non-linearities in the relation between the WR indices,\ndescribing the state of the Euro-Atlantic atmospheric circulation and the\ncorresponding surface temperature and precipitation anomalies in Europe. We\ndiscuss the AI-model performance in reconstructing the monthly mean two-meter\ntemperature and total precipitation anomalies in the European winter and\nsummer, also varying the number of WR used to describe the monthly atmospheric\ncirculation. We assess the impact of errors on the WR indices in the\nreconstruction and show that a mean absolute relative error below 80% yields\nimproved seasonal reconstruction compared to the ECMWF operational seasonal\nforecast system, SEAS5. As a demonstration of practical applicability, we\nevaluate the model using WR indices predicted by SEAS5, finding slightly better\nor comparable skill relative to the SEAS5 forecast itself. Our findings\ndemonstrate that WR-based anomaly reconstruction, powered by AI tools, offers a\npromising pathway for sub-seasonal and seasonal forecasting.", "authors": ["A. Camilletti", "G. Franch", "E. Tomasi", "M. Cristoforetti"], "published_date": "2025-06-16", "timestamp": "2025-06-17T03:45:26.050817", "title_zh": "基於歐亞-大西洋天氣型態的歐洲天氣AI重建", "summary_zh": "本研究提出一種非線性AI模型，利用歐亞-大西洋天氣型態（WR）指標，重建歐洲月平均氣溫和降水異常。天氣型態是影響歐洲天氣的大氣環流的週期性狀態，為亞季節到季節預測提供了機會。此AI模型能捕捉天氣型態指標與歐洲地表氣溫、降水異常之間複雜的非線性關係。研究評估了該模型在重建歐洲冬季和夏季月平均氣溫和總降水異常方面的表現，並探討了天氣型態數量對重建的影響。結果顯示，當天氣型態指標的平均絕對相對誤差低於80%時，相較於歐洲中期天氣預報中心（ECMWF）的SEAS5季節預報系統，季節重建效果有所提升。這項研究表明，基於天氣型態並由AI驅動的異常重建，為亞季節和季節預測提供了一條有前景的途徑。", "applications": ["農民可以利用此模型更準確地預測未來幾個月的氣溫和降水，從而更好地規劃種植和收穫時間，減少因極端天氣造成的損失。", "能源公司可以利用此模型預測未來幾個月的能源需求，提前做好供電準備，避免因氣候變化導致的能源短缺。", "政府可以利用此模型預測未來幾個月的極端天氣事件，提前發布預警，並制定應急預案，保障人民生命財產安全。"], "pitch": "各位投資人，我們帶來的是一個顛覆傳統天氣預測的AI模型！想像一下，不再依賴線性模型，而是透過AI精準掌握歐亞-大西洋天氣型態與歐洲氣候的複雜關聯，提前數月預測氣溫和降水異常。這不僅能幫助農民精準耕作、能源公司穩定供電，更能為政府應對極端天氣提供有力支持。我們的模型已展現出超越ECMWF SEAS5的潛力，這意味著更準確的預測、更低的風險，以及更大的市場機會。未來，我們將進一步擴展模型應用範圍，涵蓋全球主要農業產區和能源消耗大國，打造一個全球性的AI氣候預測平台。投資我們，就是投資未來，搶佔氣候科技的制高點！讓我們一起用AI的力量，應對氣候變遷，創造更大的社會和經濟價值！", "audio": "docs/data/audios/2506.13758v1.wav"}
{"query": "Foundation Model", "id": "2506.13762v1", "url": "http://arxiv.org/abs/2506.13762v1", "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation", "summary": "Data-driven approaches struggle with precise manipulation; imitation learning\nrequires many hard-to-obtain demonstrations, while reinforcement learning\nyields brittle, non-generalizable policies. We introduce VisuoTactile Local\n(ViTaL) policy learning, a framework that solves fine-grained manipulation\ntasks by decomposing them into two phases: a reaching phase, where a\nvision-language model (VLM) enables scene-level reasoning to localize the\nobject of interest, and a local interaction phase, where a reusable,\nscene-agnostic ViTaL policy performs contact-rich manipulation using egocentric\nvision and tactile sensing. This approach is motivated by the observation that\nwhile scene context varies, the low-level interaction remains consistent across\ntask instances. By training local policies once in a canonical setting, they\ncan generalize via a localize-then-execute strategy. ViTaL achieves around 90%\nsuccess on contact-rich tasks in unseen environments and is robust to\ndistractors. ViTaL's effectiveness stems from three key insights: (1)\nfoundation models for segmentation enable training robust visual encoders via\nbehavior cloning; (2) these encoders improve the generalizability of policies\nlearned using residual RL; and (3) tactile sensing significantly boosts\nperformance in contact-rich tasks. Ablation studies validate each of these\ninsights, and we demonstrate that ViTaL integrates well with high-level VLMs,\nenabling robust, reusable low-level skills. Results and videos are available at\nhttps://vitalprecise.github.io.", "authors": ["Zifan Zhao", "Siddhant Haldar", "Jinda Cui", "Lerrel Pinto", "Raunaq Bhirangi"], "published_date": "2025-06-16", "timestamp": "2025-06-17T03:48:06.994580", "title_zh": "觸覺始於視覺之終：接觸豐富操作的通用策略", "summary_zh": "本研究提出VisuoTactile Local (ViTaL)策略學習框架，解決精細操作任務。框架將任務分解為兩個階段：首先，利用視覺語言模型(VLM)進行場景級推理，定位目標物體；接著，利用可重複使用的ViTaL策略，透過自我中心視覺和觸覺感測執行接觸豐富的操作。ViTaL策略在標準環境中訓練一次，即可透過「定位-執行」策略推廣到其他環境。實驗結果顯示，ViTaL在未見過的環境中，接觸豐富任務的成功率約為90%，且對干擾具有魯棒性。ViTaL的有效性來自於：(1)分割基礎模型能透過行為克隆訓練出穩健的視覺編碼器；(2)這些編碼器提高了使用殘差強化學習訓練的策略的泛化能力；(3)觸覺感測顯著提高了接觸豐富任務的效能。", "applications": ["想像一下，你家的機器人廚師，不再只是按照食譜切菜，而是能像大廚一樣，透過視覺和觸覺判斷食材的成熟度，精準控制力道，做出更美味的料理。", "在醫療領域，醫生可以遠端操控精密的機器人，透過觸覺反饋，更安全、更精準地進行微創手術，減少病患的痛苦和風險。", "在工廠裡，機器人不再只是重複單一動作，而是能透過視覺和觸覺，靈活組裝複雜的零件，應對生產線上的各種變化，提高生產效率。"], "pitch": "各位投資人，我們ViTaL技術，正在重新定義機器人的操作能力！現今的機器人，在複雜環境下的精細操作仍然是瓶頸。ViTaL透過結合視覺和觸覺，讓機器人具備像人類一樣的靈巧性，能適應各種未知環境，完成更複雜的任務。試想，未來的無人工廠，不再需要昂貴的硬體改造，只需導入ViTaL，就能讓現有機器人升級，大幅降低生產成本。在醫療領域，遠程手術將成為常態，ViTaL賦予醫生精準的觸覺，拯救更多生命。更進一步，我們甚至可以預見，ViTaL將催生新一代的家用機器人，能真正融入家庭生活，提供更貼心的服務。這不僅僅是一項技術，而是一個龐大的市場機會。現在加入，您將成為這場機器人革命的領航者！", "audio": "docs/data/audios/2506.13762v1.wav"}
{"query": "AI", "id": "2506.13755v1", "url": "http://arxiv.org/abs/2506.13755v1", "title": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering", "summary": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%.", "authors": ["Arya Fayyazi", "Mehdi Kamal", "Massoud Pedram"], "published_date": "2025-06-16", "timestamp": "2025-06-17T06:18:48.581708", "title_zh": "MARCO：基於多代理強化學習與適應性預測過濾的邊緣設備硬體感知神經架構搜尋", "summary_zh": "MARCO是一個專為資源受限的邊緣設備設計的硬體感知神經架構搜尋框架。它結合多代理強化學習和適應性預測，加速深度神經網路在邊緣設備上的軟硬體協同設計。MARCO將搜尋任務分解為硬體配置代理和量化代理，前者優化設計參數，後者決定每層的位元寬度，並使用適應性預測模型，在耗時的訓練或模擬前過濾掉不佳的架構，大幅縮短搜尋時間。實驗證明，MARCO在保持精度的同時，能有效減少搜尋時間並降低延遲，在MAX78000開發板上的驗證也證實了模擬結果的有效性。", "applications": ["智慧家電：讓掃地機器人或智慧冰箱更聰明，在本地端快速處理影像和語音，提升反應速度和隱私保護，例如即時避開障礙物或辨識食物種類。", "穿戴裝置：提升智慧手錶或AR眼鏡的效能，讓它們能更快速地執行健康監測、運動追蹤或即時翻譯等功能，而且更省電。", "自駕車：在車載電腦上快速處理感測器數據，進行即時路況分析和決策，提升行車安全性和反應速度，例如快速辨識行人或障礙物。"], "pitch": "各位投資人，想像一下，未來的AI晶片就像樂高積木一樣，可以針對不同應用場景快速組裝。MARCO技術就是這個積木的設計藍圖！我們正在打造一個AI晶片界的「自動設計師」，它可以根據邊緣設備的硬體限制，自動優化神經網路架構，大幅縮短開發時間和成本。這意味著，我們可以更快地將AI應用推向市場，從智慧家電、穿戴裝置到自駕車，甚至是更廣闊的物聯網領域。更重要的是，MARCO的適應性預測能力，就像AI晶片的「品質檢測員」，能確保設計出的架構具有高性能和可靠性。我們相信，MARCO將會是下一代邊緣AI晶片的關鍵技術，具有巨大的商業潛力，現在加入，就能一起掌握這個千億美元級的市場！", "audio": "docs/data/audios/2506.13755v1.wav"}
{"query": "Foundation Model", "id": "2506.13723v1", "url": "http://arxiv.org/abs/2506.13723v1", "title": "OTFusion: Bridging Vision-only and Vision-Language Models via Optimal Transport for Transductive Zero-Shot Learning", "summary": "Transductive zero-shot learning (ZSL) aims to classify unseen categories by\nleveraging both semantic class descriptions and the distribution of unlabeled\ntest data. While Vision-Language Models (VLMs) such as CLIP excel at aligning\nvisual inputs with textual semantics, they often rely too heavily on\nclass-level priors and fail to capture fine-grained visual cues. In contrast,\nVision-only Foundation Models (VFMs) like DINOv2 provide rich perceptual\nfeatures but lack semantic alignment. To exploit the complementary strengths of\nthese models, we propose OTFusion, a simple yet effective training-free\nframework that bridges VLMs and VFMs via Optimal Transport. Specifically,\nOTFusion aims to learn a shared probabilistic representation that aligns visual\nand semantic information by minimizing the transport cost between their\nrespective distributions. This unified distribution enables coherent class\npredictions that are both semantically meaningful and visually grounded.\nExtensive experiments on 11 benchmark datasets demonstrate that OTFusion\nconsistently outperforms the original CLIP model, achieving an average accuracy\nimprovement of nearly $10\\%$, all without any fine-tuning or additional\nannotations. The code will be publicly released after the paper is accepted.", "authors": ["Qiyu Xu", "Wenyang Chen", "Zhanxuan Hu", "Huafeng Li", "Yonghang Tai"], "published_date": "2025-06-16", "timestamp": "2025-06-17T06:20:39.167966", "title_zh": "OTFusion：透過最佳傳輸橋接純視覺與視覺-語言模型，實現轉導式零樣本學習", "summary_zh": "OTFusion是一種創新的零樣本學習框架，它巧妙地結合了視覺-語言模型（如CLIP）的語義理解能力和純視覺模型（如DINOv2）的細膩視覺感知能力。傳統的視覺-語言模型過於依賴類別先驗知識，而純視覺模型則缺乏語義對齊。OTFusion利用最佳傳輸演算法，學習一個共享的機率表示空間，有效地對齊視覺和語義資訊，從而產生既有語義意義又基於視覺證據的類別預測。實驗結果表明，OTFusion在多個基準數據集上顯著優於原始的CLIP模型，平均準確率提升近10%，且無需任何微調或額外標註。", "applications": ["**智慧零售商品辨識：** 想像一下，在無人商店中，OTFusion能讓系統即使沒有見過某種新款商品，也能透過商品描述（例如：『有機草莓優格』）和視覺特徵，準確辨識並結帳，大幅提升購物體驗。", "**醫療影像輔助診斷：** 醫生可以輸入病灶的文字描述，OTFusion就能在X光或MRI影像中，協助醫生快速定位並判斷病灶類型，即使是罕見疾病也能提供診斷參考，提升醫療效率和準確性。", "**搜尋引擎圖片分類：** 使用者輸入一段文字描述（例如：『在海邊玩耍的黃金獵犬』），OTFusion能更精準地從海量圖片中找到符合描述的圖片，即使圖片中的狗之前沒有被標註為黃金獵犬也能辨識，提供更智慧化的搜尋體驗。"], "pitch": "各位投資人，想像一下，我們正處於AI大爆發的前夜，而OTFusion正是開啟零樣本學習潛力的鑰匙！現有的AI模型需要大量標註數據才能學習，這限制了它們的應用範圍和擴展速度。OTFusion打破了這個瓶頸，它能讓AI在沒有任何訓練數據的情況下，理解並辨識新的事物。這意味著什麼？\n\n首先，它能大幅降低AI的開發成本和時間，加速AI在各行各業的落地。其次，它能讓AI具備更強的適應性和泛化能力，應對不斷變化的現實世界。更重要的是，OTFusion為我們打開了一扇通往通用人工智能（AGI）的大門，讓AI真正具備理解、推理和創造的能力。\n\n我們的團隊已經證明了OTFusion在多個領域的優越性能，並且我們相信，隨著技術的不斷發展，OTFusion將在智慧零售、醫療健康、自動駕駛、智慧城市等領域產生顛覆性的影響。我們正在尋找有遠見的投資人，一起打造一個零樣本學習的AI帝國，共同迎接AGI時代的到來！現在投資OTFusion，就是投資AI的未來！", "audio": "docs/data/audios/2506.13723v1.wav"}
{"query": "AI", "id": "2506.13740v1", "url": "http://arxiv.org/abs/2506.13740v1", "title": "Kolmogorov-Arnold Network for Gene Regulatory Network Inference", "summary": "Gene regulation is central to understanding cellular processes and\ndevelopment, potentially leading to the discovery of new treatments for\ndiseases and personalized medicine. Inferring gene regulatory networks (GRNs)\nfrom single-cell RNA sequencing (scRNA-seq) data presents significant\nchallenges due to its high dimensionality and complexity. Existing tree-based\nmodels, such as GENIE3 and GRNBOOST2, demonstrated scalability and\nexplainability in GRN inference, but they cannot distinguish regulation types\nnor effectively capture continuous cellular dynamics. In this paper, we\nintroduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN)\nwith explainable AI to infer GRNs from scRNA-seq data. By modeling gene\nexpression as differentiable functions matching the smooth nature of cellular\ndynamics, scKAN can accurately and precisely detect activation and inhibition\nregulations through explainable AI and geometric tools. We conducted extensive\nexperiments on the BEELINE benchmark, and scKAN surpasses and improves the\nleading signed GRN inference models ranging from 5.40\\% to 28.37\\% in AUROC and\nfrom 1.97\\% to 40.45\\% in AUPRC. These results highlight the potential of scKAN\nin capturing the underlying biological processes in gene regulation without\nprior knowledge of the graph structure.", "authors": ["Tsz Pan Tong", "Aoran Wang", "George Panagopoulos", "Jun Pang"], "published_date": "2025-06-16", "timestamp": "2025-06-17T09:15:34.095751", "title_zh": "基於 Kolmogorov-Arnold 網絡的基因調控網絡推斷", "summary_zh": "基因調控是了解細胞運作和發展的關鍵，有助於發現疾病新療法和實現個人化醫療。從單細胞RNA測序數據推斷基因調控網絡極具挑戰。本研究提出scKAN模型，它使用Kolmogorov-Arnold網絡，透過可解釋AI，從單細胞RNA測序數據推斷基因調控網絡。scKAN將基因表達建模為可微分函數，捕捉細胞動態的平滑特性，並透過可解釋AI和幾何工具精確檢測激活和抑制調控。實驗結果顯示，scKAN在AUROC指標上超越現有模型5.40%至28.37%，在AUPRC指標上提升1.97%至40.45%，展現了其在無需先驗知識下捕捉基因調控中潛在生物過程的巨大潛力。", "applications": ["個人化醫療：透過分析個人的基因數據，預測對特定藥物的反應，從而制定更精準有效的治療方案。", "疾病預防：早期檢測基因調控異常，預測潛在的疾病風險，以便及早採取預防措施。", "新藥開發：深入了解基因調控機制，找到新的藥物靶點，加速新藥的研發進程。"], "pitch": "各位投資人，我們正處於精準醫療革命的前沿！scKAN技術利用創新的Kolmogorov-Arnold網絡，以前所未有的精確度解讀基因調控網絡。想像一下，我們能更早發現癌症、更有效地治療罕見疾病，甚至預測並預防慢性病的發生。這不僅僅是技術突破，更是對人類健康的巨大貢獻。目前市場上缺乏能有效處理複雜基因數據的工具，scKAN填補了這個空白，擁有巨大的市場潛力。我們預計，scKAN將成為製藥公司、研究機構和醫療機構的必備工具，為個人化醫療開闢新的道路。投資scKAN，就是投資健康的未來，投資回報將遠遠超出您的想像！", "audio": "docs/data/audios/2506.13740v1.wav"}
{"query": "Foundation Model", "id": "2506.13599v1", "url": "http://arxiv.org/abs/2506.13599v1", "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation", "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.", "authors": ["Yuwei Du", "Jie Feng", "Jian Yuan", "Yong Li"], "published_date": "2025-06-16", "timestamp": "2025-06-17T09:17:02.898990", "title_zh": "CAMS：一個由 CityGPT 驅動的城市人類移動模擬代理框架", "summary_zh": "CAMS是一個創新的代理框架，它利用基於語言的城市基礎模型來模擬城市空間中的人類移動。它包含三個核心模塊：MobExtractor提取模板移動模式並根據用戶資料合成新的模式；GeoGenerator考慮集體知識生成錨點，並使用增強版的CityGPT生成候選城市地理空間知識；TrajEnhancer基於移動模式檢索空間知識，並通過DPO生成具有真實軌跡偏好對齊的軌跡。實驗表明，CAMS在不依賴外部地理空間信息的情況下，取得了優異的性能，並能生成更真實合理的軌跡。CAMS為人類移動模擬開創了一個將代理框架與具有城市知識的LLM相結合的新範例。", "applications": ["**智慧交通規劃：** 想像一下，城市規劃者可以透過CAMS預測未來交通流量，提前部署公共運輸資源，減少塞車，讓大家通勤更順暢。", "**緊急應變模擬：** 在發生災害時，CAMS可以模擬人群疏散的路徑，幫助救援單位更有效地規劃疏散路線，降低傷亡。", "**零售業選址：** 零售商可以使用CAMS分析人流移動模式，選擇最佳的開店地點，提高客流量和銷售額。"], "pitch": "各位創投先進，我們正在打造一個革命性的城市模擬平台CAMS，它不僅僅是一個模擬器，而是城市大腦的雛形！傳統的城市規劃和管理依賴於昂貴且耗時的數據收集和分析。CAMS利用大型語言模型和代理框架，能夠以驚人的精度模擬城市中個體和群體的移動模式，無需大量外部數據。想像一下，我們可以預測城市未來的交通擁堵，優化公共交通路線，甚至模擬緊急情況下的疏散方案。這將為智慧城市建設、交通管理、零售選址、應急響應等領域帶來巨大的變革。CAMS的潛在商業價值是無限的！我們預計，隨著城市化進程的加速和智慧城市建設的推進，CAMS將成為城市規劃和管理不可或缺的工具，市場規模將達到數十億美元。現在加入我們，共同打造未來城市的大腦，共享這巨大的市場紅利！", "audio": "docs/data/audios/2506.13599v1.wav"}
{"query": "Diffusion Model", "id": "2506.13667v1", "url": "http://arxiv.org/abs/2506.13667v1", "title": "MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework via Latent Diffusion Model", "summary": "Multimodal medical imaging integrates diverse data types, such as structural\nand functional neuroimaging, to provide complementary insights that enhance\ndeep learning predictions and improve outcomes. This study focuses on a\nneuroimaging prediction framework based on both structural and functional\nneuroimaging data. We propose a next-generation prediction model,\n\\textbf{MultiViT2}, which combines a pretrained representative learning base\nmodel with a vision transformer backbone for prediction output. Additionally,\nwe developed a data augmentation module based on the latent diffusion model\nthat enriches input data by generating augmented neuroimaging samples, thereby\nenhancing predictive performance through reduced overfitting and improved\ngeneralizability. We show that MultiViT2 significantly outperforms the\nfirst-generation model in schizophrenia classification accuracy and\ndemonstrates strong scalability and portability.", "authors": ["Bi Yuda", "Jia Sihan", "Gao Yutong", "Abrol Anees", "Fu Zening", "Calhoun Vince"], "published_date": "2025-06-16", "timestamp": "2025-06-17T09:18:27.090594", "title_zh": "MultiViT2：基於潛在擴散模型之數據增強多模態神經影像預測框架", "summary_zh": "本研究提出一個基於結構和功能性神經影像數據的新一代預測模型MultiViT2。它結合了預訓練的代表性學習基礎模型和視覺轉換器主幹，用於預測輸出。此外，我們開發了一個基於潛在擴散模型的數據增強模塊，透過生成增強的神經影像樣本來豐富輸入數據，從而透過減少過擬合併提高泛化能力來增強預測性能。實驗證明，MultiViT2在精神分裂症分類準確性方面顯著優於第一代模型，並展現出強大的可擴展性和可移植性。", "applications": ["**早期疾病篩檢：** 想像一下，透過結合腦部結構和功能影像，MultiViT2能更精準地預測阿茲海默症或帕金森氏症的早期風險，讓我們能及早介入治療，延緩疾病進程。", "**個人化治療方案：** 每個人大腦的運作方式都不同。MultiViT2可以分析個體的腦部影像，預測他們對特定藥物或治療方式的反應，從而制定更有效的個人化治療方案。", "**精神疾病診斷輔助：** 精神疾病的診斷往往具有挑戰性。MultiViT2可以輔助醫生判斷患者是否患有精神分裂症或其他精神疾病，減少誤診的可能性，讓患者能及早獲得適當的幫助。"], "pitch": "各位創投先進，我們正在開發的MultiViT2模型，是神經影像分析領域的革命性突破！它不僅能更精準地預測疾病風險、制定個人化治療方案，更能大幅提升精神疾病的診斷效率。試想，未來結合穿戴式腦波裝置，我們就能在家中進行早期疾病篩檢，甚至透過AI即時監測情緒變化，預防心理疾病的發生。這不僅能大幅降低醫療成本，更能創造龐大的健康管理市場。更令人興奮的是，MultiViT2具備高度的可擴展性，未來可應用於腦機介面、認知增強等領域，開創無限可能。現在投資MultiViT2，您將站在AI醫療革命的最前沿，共同打造一個更健康、更美好的未來！", "audio": "docs/data/audios/2506.13667v1.wav"}
{"query": "AI", "id": "2506.13727v1", "url": "http://arxiv.org/abs/2506.13727v1", "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs", "summary": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.", "authors": ["Sayed Mohammad Vakilzadeh Hatefi", "Maximilian Dreyer", "Reduan Achtibat", "Patrick Kahardipraja", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "published_date": "2025-06-16", "timestamp": "2025-06-17T12:24:24.694534", "title_zh": "基於歸因引導的剪枝技術：用於大型語言模型的壓縮、電路發現與目標校正", "summary_zh": "大型語言模型雖然強大，但體積龐大，難以部署。本研究利用可解釋AI中的歸因方法，特別是逐層相關性傳播（LRP），來引導模型剪枝，移除不重要的部分，在幾乎不影響效能的前提下大幅縮減模型大小。此方法能有效提取執行特定任務的核心「電路」，例如辨識間接目標。更進一步，我們還能移除造成不良行為（例如產生有害內容）的電路，實現模型校正。我們將這些技術整合為一個統一的框架，並在Llama和OPT模型上驗證其在壓縮、電路發現和模型校正方面的有效性和局限性，展現了其提高模型效率和安全性的潛力。", "applications": ["1. 智慧客服：將大型語言模型壓縮後，可以部署在手機或小型伺服器上，提供更即時、更個人化的客服體驗，而且不用擔心資料外洩，因為模型可以在本地運行。", "2. 輔助寫作工具：讓AI在手機上就能幫你寫作，即使在沒有網路的環境下也能使用。例如，在飛機上或偏遠地區，AI可以根據你的需求生成文章草稿，提高工作效率。", "3. 內容審核：快速找出模型中產生有害內容的「電路」，並將其移除，確保AI產生的內容安全可靠，減少錯誤資訊的傳播。"], "pitch": "各位投資人，我們正站在AI革命的浪潮之上，但大型語言模型的部署成本是阻礙發展的關鍵因素。我們的技術如同AI界的「瘦身專家」，能大幅降低模型體積，讓AI無處不在！想像一下，未來每個人的手機裡都裝著一個強大的AI助理，隨時提供專業知識和創意靈感。這不僅能改變人機互動方式，更將催生出龐大的AI應用市場。此外，我們還能精準「手術」，移除模型中的有害基因，讓AI更安全、更可靠。這項技術不僅能提升AI的效率，更賦予了AI倫理價值。現在投資我們，就是投資AI的未來，一起打造一個更智慧、更便捷、更安全的AI世界！我們預計在三年內，我們的技術將成為AI模型壓縮的行業標準，並佔據市場主導地位，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.13727v1.wav"}
{"query": "Foundation Model", "id": "2506.13538v1", "url": "http://arxiv.org/abs/2506.13538v1", "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers", "summary": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP.\nUsing state-of-the-art health metrics and a hybrid analysis pipeline, combining\na general-purpose static analysis tool with an MCP-specific scanner, we\nevaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities-only three overlapping with traditional\nsoftware vulnerabilities. Additionally, 7.2% of servers contain general\nvulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping prior research. These findings highlight the need for MCP-specific\nvulnerability detection techniques while reaffirming the value of traditional\nanalysis and refactoring practices.", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "published_date": "2025-06-16", "timestamp": "2025-06-17T12:26:16.999637", "title_zh": "模型上下文協議（MCP）初探：MCP伺服器的安全性與可維護性研究", "summary_zh": "隨著GPT-4等大型模型廣泛應用於金融和軟體工程等領域，文字介面限制了它們的實際應用。為了解決此問題，Anthropic在2024年末推出了模型上下文協議（MCP），旨在標準化工具生態系統，目前已成為事實上的標準，每週SDK下載量超過八百萬。然而，MCP的AI驅動、非確定性控制流程帶來了新的可持續性、安全性和可維護性風險，值得深入研究。本研究對1899個開源MCP伺服器進行了大規模實證研究，評估其健康狀況、安全性和可維護性。研究發現，儘管MCP伺服器展現出良好的健康指標，但仍存在八種不同的漏洞，其中只有三種與傳統軟體漏洞重疊。此外，7.2%的伺服器包含通用漏洞，5.5%的伺服器存在MCP特定的工具中毒現象。在可維護性方面，66%的伺服器存在程式碼異味，14.4%的伺服器包含與先前研究重疊的十種錯誤模式。這些發現強調了針對MCP特定漏洞檢測技術的需求，同時也重申了傳統分析和重構實踐的價值。", "applications": ["**智能家居控制：** 想像一下，你的智能家居不再只是簡單的聲控開關燈，而是能根據你的上下文理解你的需求。例如，當你說『我有點冷』，系統會自動調整暖氣、關閉窗戶，甚至為你準備一杯熱飲，完全不需要你一步一步下指令。", "**個人化醫療建議：** 未來，你的健康App不再只是記錄數據，而是能根據你的飲食、運動、睡眠等數據，結合最新的醫療研究，提供個人化的健康建議。例如，App會提醒你『今天運動量不足，建議增加20分鐘有氧運動』，並根據你的喜好推薦適合的運動方式。", "**自動化客戶服務：** 客服機器人不再只是回答預設問題，而是能理解客戶的複雜需求，並自動調用相關工具解決問題。例如，當客戶詢問『我的訂單狀態如何』，機器人會自動查詢物流信息，並提供詳細的送貨時間預估，甚至能主動聯繫快遞公司解決延遲問題。"], "pitch": "各位創投，我們正在開發基於模型上下文協議（MCP）的安全與可維護性解決方案，這是一個潛力無限的市場！隨著AI工具調用日益普及，MCP的安全漏洞和可維護性問題將成為企業的重大隱憂。我們的技術能有效檢測和防禦這些風險，確保AI應用的穩定性和安全性。想像一下，每個使用AI工具的公司都需要我們的產品！我們不僅提供漏洞掃描，更提供自動修復和優化建議，幫助企業降低維護成本，提高AI應用效率。未來，我們將整合更多AI安全技術，打造全面的AI安全平台，成為AI時代不可或缺的基礎設施。現在加入我們，一起搶佔這個千億級的市場先機！", "audio": "docs/data/audios/2506.13538v1.wav"}
{"query": "Diffusion Model", "id": "2506.13614v1", "url": "http://arxiv.org/abs/2506.13614v1", "title": "Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models", "summary": "The success of diffusion models has driven interest in performing conditional\nsampling via training-free guidance of the denoising process to solve image\nrestoration and other inverse problems. A popular class of methods, based on\nDiffusion Posterior Sampling (DPS), attempts to approximate the intractable\nposterior score function directly. In this work, we present a novel expression\nfor the exact posterior score for purely denoising tasks that is tractable in\nterms of the unconditional score function. We leverage this result to analyze\nthe time-dependent error in the DPS score for denoising tasks and compute step\nsizes on the fly to minimize the error at each time step. We demonstrate that\nthese step sizes are transferable to related inverse problems such as\ncolorization, random inpainting, and super resolution. Despite its simplicity,\nthis approach is competitive with state-of-the-art techniques and enables\nsampling with fewer time steps than DPS.", "authors": ["Gregory Bellchambers"], "published_date": "2025-06-16", "timestamp": "2025-06-17T12:27:25.841847", "title_zh": "利用精確去噪後驗分數於擴散模型之免訓練引導", "summary_zh": "本研究提出一種新方法，針對擴散模型在圖像修復等逆問題中，如何不經訓練就能有效引導去噪過程。我們推導出純去噪任務的精確後驗分數公式，並以此分析現有方法（Diffusion Posterior Sampling, DPS）的時間誤差。透過動態調整步長，我們能最小化每一步的誤差。實驗證明，此方法不僅簡潔，且在色彩還原、隨機填補、超解析度等問題上，效果媲美國際頂尖技術，並能用更少的步驟完成取樣，大幅提升效率。", "applications": ["想像一下，你手機裡的老照片變得模糊不清，有了這項技術，就像擁有一個AI修復師，能瞬間讓照片恢復清晰，重現美好回憶。", "假設你是個藝術家，想要將草圖變成高解析度的畫作。這項技術可以幫助你將低品質的草稿，快速轉換成細緻、精美的藝術品。", "在醫療影像領域，例如X光片或斷層掃描，醫生可以利用這項技術，提升影像的清晰度，更容易診斷病情，及早發現潛在的健康問題。"], "pitch": "各位投資人，我們正在革新圖像處理領域！這項技術不僅解決了現有擴散模型訓練成本高昂的問題，更在效率和準確性上取得了突破。想像一下，未來在遊戲開發、電影特效、甚至是醫療診斷等領域，我們都能看到這項技術的身影。它能大幅降低開發成本、提升影像品質、加速診斷流程。更重要的是，這項技術具有極高的可擴展性，未來可以應用於更多領域，例如自動駕駛、智慧監控等等。我們預計在三年內，這項技術將成為行業標準，為公司帶來數億美元的營收。現在加入我們，一起打造圖像處理的未來！", "audio": "docs/data/audios/2506.13614v1.wav"}
{"query": "AI", "id": "2506.13695v1", "url": "http://arxiv.org/abs/2506.13695v1", "title": "OneRec Technical Report", "summary": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.", "authors": ["Guorui Zhou", "Jiaxin Deng", "Jinghao Zhang", "Kuo Cai", "Lejian Ren", "Qiang Luo", "Qianqian Wang", "Qigen Hu", "Rui Huang", "Shiyao Wang", "Weifeng Ding", "Wuchao Li", "Xinchen Luo", "Xingmei Wang", "Zexuan Cheng", "Zixing Zhang", "Bin Zhang", "Boxuan Wang", "Chaoyi Ma", "Chengru Song", "Chenhui Wang", "Di Wang", "Dongxue Meng", "Fan Yang", "Fangyu Zhang", "Feng Jiang", "Fuxing Zhang", "Gang Wang", "Guowang Zhang", "Han Li", "Hengrui Hu", "Hezheng Lin", "Hongtao Cheng", "Hongyang Cao", "Huanjie Wang", "Jiaming Huang", "Jiapeng Chen", "Jiaqiang Liu", "Jinghui Jia", "Kun Gai", "Lantao Hu", "Liang Zeng", "Liao Yu", "Qiang Wang", "Qidong Zhou", "Shengzhe Wang", "Shihui He", "Shuang Yang", "Shujie Yang", "Sui Huang", "Tao Wu", "Tiantian He", "Tingting Gao", "Wei Yuan", "Xiao Liang", "Xiaoxiao Xu", "Xugang Liu", "Yan Wang", "Yi Wang", "Yiwu Liu", "Yue Song", "Yufei Zhang", "Yunfan Wu", "Yunfeng Zhao", "Zhanyu Liu"], "published_date": "2025-06-16", "timestamp": "2025-06-17T15:14:25.664277", "title_zh": "OneRec 技術報告", "summary_zh": "傳統推薦系統效率低落，難以應用最新AI技術。OneRec透過端到端生成式方法重塑推薦系統，運算效率提升10倍，並成功導入強化學習優化。基礎設施優化後，GPU利用率顯著提升，營運成本僅為傳統系統的10.6%。在快手/快手極速版App部署後，處理25%的總請求量，App停留時間分別提升0.54%和1.24%，7日生命週期等關鍵指標也顯著增加。OneRec證明了端到端生成式推薦系統的巨大潛力，為業界帶來了寶貴的實務經驗。", "applications": ["**場景一：個人化新聞推薦**：想像一下，不再被演算法餵食重複或無趣的新聞，OneRec能更精準地理解你的閱讀偏好，推薦你真正感興趣且多元的新聞資訊，讓你隨時掌握最新鮮、最獨到的觀點。", "**場景二：智慧購物助手**：網購時，不再大海撈針！OneRec能根據你的瀏覽紀錄、購物習慣，甚至社群互動，推薦最適合你的商品，讓你輕鬆找到心頭好，省時又省力。", "**場景三：精準影音娛樂**：告別劇荒！OneRec能深入分析你的觀影喜好，推薦你可能喜歡的電影、電視劇或短視頻，讓你每次打開App都能享受一場視聽盛宴。"], "pitch": "各位投資人，我們正處於推薦系統的革命性轉捩點！傳統推薦系統效率低下，已無法滿足日益增長的用戶需求。OneRec的端到端生成式架構，不僅大幅提升運算效率、降低成本，更重要的是，它能更精準地理解用戶意圖，提供真正個人化的推薦體驗。想像一下，未來所有App都將採用類似OneRec的技術，大幅提升用戶黏著度、增加營收。這不僅僅是一個技術升級，更是一個商業模式的顛覆！我們預期OneRec將成為下一代推薦系統的基礎設施，擁有巨大的市場潛力。現在投資OneRec，就是投資未來！", "audio": "docs/data/audios/2506.13695v1.wav"}
{"query": "Foundation Model", "id": "2506.13514v1", "url": "http://arxiv.org/abs/2506.13514v1", "title": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices", "summary": "Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.", "authors": ["Mingxue Xu", "Yao Lei Xu", "Danilo P. Mandic"], "published_date": "2025-06-16", "timestamp": "2025-06-17T15:16:04.497556", "title_zh": "TensorSLM：低端裝置上十億參數以下語言模型的節能嵌入壓縮", "summary_zh": "本研究針對小型語言模型(SLM)在低端裝置上的應用，提出一種免訓練的token embedding壓縮方法，使用Tensor-Train分解(TTD)將預訓練的token embedding向量轉換為低維的Matrix Product State(MPS)。實驗結果顯示，在GPT-2/Cerebres-GPT和OPT等十億參數以下的模型上，該方法能在嵌入層壓縮約2倍的情況下，達到與原始模型相當的語言任務效能，同時單次查詢的能源消耗降低一半。此技術有助於SLM在資源受限的邊緣設備上更高效地運行。", "applications": ["**智慧家電語音控制：** 想像一下，你的智慧音箱或家電，即使在網路不穩定的情況下，也能快速且準確地理解你的語音指令，因為它內建了壓縮過的小型語言模型，反應更靈敏、更節能。", "**離線翻譯App：** 出國旅遊時，即使沒有網路，你的翻譯App也能即時翻譯對話，因為它搭載了高效能的離線語言模型，讓你隨時隨地都能輕鬆溝通。", "**智慧玩具互動：** 孩子們的智慧玩具，能夠更自然、更智慧地回應他們的提問和指令，提供更豐富的互動體驗，同時延長電池續航力。"], "pitch": "各位創投先進，我們提出的TensorSLM技術，是解決當前AI發展瓶頸的關鍵一步。隨著邊緣運算的興起，小型語言模型的需求日益增加。TensorSLM能大幅降低模型體積和功耗，使AI應用得以普及到各種低端裝置，例如智慧穿戴、物聯網設備等。想像一下，未來數百億的物聯網裝置，都具備了基本的AI能力，這將開啟一個全新的智慧生活時代。我們的技術不僅能提升現有產品的效能，更能催生出更多創新的應用，例如個人化的健康監測、智慧農業、甚至是在太空探索中提供即時的AI支援。我們相信，TensorSLM具有巨大的商業潛力，將引領下一波AI革命，現在投資，您將站在浪潮的最前端！", "audio": "docs/data/audios/2506.13514v1.wav"}
{"query": "Diffusion Model", "id": "2506.13594v1", "url": "http://arxiv.org/abs/2506.13594v1", "title": "Dive3D: Diverse Distillation-based Text-to-3D Generation via Score Implicit Matching", "summary": "Distilling pre-trained 2D diffusion models into 3D assets has driven\nremarkable advances in text-to-3D synthesis. However, existing methods\ntypically rely on Score Distillation Sampling (SDS) loss, which involves\nasymmetric KL divergence--a formulation that inherently favors mode-seeking\nbehavior and limits generation diversity. In this paper, we introduce Dive3D, a\nnovel text-to-3D generation framework that replaces KL-based objectives with\nScore Implicit Matching (SIM) loss, a score-based objective that effectively\nmitigates mode collapse. Furthermore, Dive3D integrates both diffusion\ndistillation and reward-guided optimization under a unified divergence\nperspective. Such reformulation, together with SIM loss, yields significantly\nmore diverse 3D outputs while improving text alignment, human preference, and\noverall visual fidelity. We validate Dive3D across various 2D-to-3D prompts and\nfind that it consistently outperforms prior methods in qualitative assessments,\nincluding diversity, photorealism, and aesthetic appeal. We further evaluate\nits performance on the GPTEval3D benchmark, comparing against nine\nstate-of-the-art baselines. Dive3D also achieves strong results on quantitative\nmetrics, including text-asset alignment, 3D plausibility, text-geometry\nconsistency, texture quality, and geometric detail.", "authors": ["Weimin Bai", "Yubo Li", "Wenzheng Chen", "Weijian Luo", "He Sun"], "published_date": "2025-06-16", "timestamp": "2025-06-17T15:17:19.891742", "title_zh": "Dive3D：透過隱式分數匹配實現基於多樣化蒸餾的文本到3D生成", "summary_zh": "Dive3D是一個創新的文本到3D生成框架，它使用隱式分數匹配（SIM）損失取代了傳統的KL散度目標，有效緩解了模型崩潰問題，顯著提升了3D輸出的多樣性。該方法整合了擴散蒸餾和獎勵引導優化，在統一的散度視角下進行重新建模。實驗結果表明，Dive3D在文本對齊、人類偏好和整體視覺保真度方面均有提升，並在多樣性、照片真實感和美學吸引力等定性評估指標上優於現有方法。在GPTEval3D基準測試中，Dive3D也展現了強大的性能，並在文本-資產對齊、3D合理性、文本-幾何一致性、紋理質量和幾何細節等定量指標上取得了優異的成績。", "applications": ["想像一下，你可以用文字描述想要的玩具，Dive3D就能立即生成3D模型，讓你可以直接3D列印出來，省去繪圖和建模的麻煩。", "室內設計師可以利用Dive3D，讓客戶用簡單的文字描述想要的家具或裝飾品，就能快速看到擺放在房間裡的效果，大幅提升溝通效率。", "遊戲開發者可以透過Dive3D快速生成遊戲中的3D物件，例如角色、武器或場景，加速遊戲開發流程，並降低美術製作成本。"], "pitch": "各位投資人，我們正站在一個劃時代的起點！Dive3D不僅僅是一個文本到3D的生成工具，它代表著創造力的解放和生產力的飛躍。現有的3D建模技術門檻高、耗時長，而Dive3D的出現，讓任何人都能用簡單的文字，創造出精美的3D模型。試想一下，從電商平台的個性化商品定制，到元宇宙的虛擬世界構建，再到工業設計的原型快速迭代，Dive3D的應用場景無可限量！我們預計，在未來五年內，Dive3D將徹底顛覆3D內容的生產方式，形成一個數十億美元級別的市場。現在加入我們，你將成為這場革命的領跑者，共同開創3D創造的新紀元！", "audio": "docs/data/audios/2506.13594v1.wav"}
{"query": "AI", "id": "2506.13685v1", "url": "http://arxiv.org/abs/2506.13685v1", "title": "An LLM's Apology: Outsourcing Awkwardness in the Age of AI", "summary": "A key part of modern social dynamics is flaking at short notice. However,\nanxiety in coming up with believable and socially acceptable reasons to do so\ncan instead lead to 'ghosting', awkwardness, or implausible excuses, risking\nemotional harm and resentment in the other party. The ability to delegate this\ntask to a Large Language Model (LLM) could substantially reduce friction and\nenhance the flexibility of user's social life while greatly minimising the\naforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an\nevaluation of models' capacity to effectively, kindly, and humanely extract\nthemselves from a diverse set of social, professional and romantic scenarios.\nWe report the efficacy of 10 frontier or recently-frontier LLMs in bailing on\nprior commitments, because nothing says \"I value our friendship\" like having AI\ngenerate your cancellation texts. We open-source FLAKE-Bench at\ngithub.com/Cloakless/flake-bench to support future research.", "authors": ["Twm Stone", "Anna Soligo"], "published_date": "2025-06-16", "timestamp": "2025-06-17T18:17:27.617791", "title_zh": "大型語言模型的道歉：在人工智慧時代外包尷尬", "summary_zh": "現代社交中，臨時爽約是常見卻令人困擾的行為。為了避免尷尬或編造不合理的藉口，人們可能選擇直接「消失」，反而造成情感傷害。本研究提出利用大型語言模型（LLM）代為處理這些情況，減輕社交壓力，提升社交彈性。我們創建了FLAKE-Bench評估基準，測試LLM在不同社交情境下有效、友善、且人性化地脫身的能力。實驗結果展示了10個前沿LLM在爽約方面的表現。我們開源FLAKE-Bench，以支持未來研究。畢竟，沒有什麼比用AI生成取消訊息更能展現「我重視我們的友誼」了。", "applications": ["朋友聚會臨時有事，不想編造爛理由，直接讓AI生成一段得體又誠懇的道歉訊息。", "工作邀約太多，分身乏術，讓AI幫忙回覆婉拒，避免得罪人，維持良好人際關係。", "約會後覺得不適合，不想直接拒絕傷感情，讓AI生成一段委婉又不失禮貌的結束訊息。"], "pitch": "各位投資人，我們正處於一個社交焦慮日益嚴重的時代。想像一下，一個AI能為你處理所有尷尬的社交場合，讓你不再為爽約、拒絕邀約而煩惱。這不僅僅是一個工具，而是一個巨大的市場機會！FLAKE-Bench技術能夠賦予AI高度的情感理解能力，使其能夠生成真正人性化的道歉和婉拒訊息。這項技術的應用場景廣闊，從個人助理到企業客戶關係管理，甚至可以應用於心理諮詢領域，緩解社交壓力。我們預計，未來這項技術將成為社交AI領域的基石，引領新一波的AI社交革命。現在投資我們，您將成為這個革命的先驅者！", "audio": "docs/data/audios/2506.13685v1.wav"}
{"query": "Foundation Model", "id": "2506.13498v1", "url": "http://arxiv.org/abs/2506.13498v1", "title": "A Survey on Imitation Learning for Contact-Rich Tasks in Robotics", "summary": "This paper comprehensively surveys research trends in imitation learning for\ncontact-rich robotic tasks. Contact-rich tasks, which require complex physical\ninteractions with the environment, represent a central challenge in robotics\ndue to their nonlinear dynamics and sensitivity to small positional deviations.\nThe paper examines demonstration collection methodologies, including teaching\nmethods and sensory modalities crucial for capturing subtle interaction\ndynamics. We then analyze imitation learning approaches, highlighting their\napplications to contact-rich manipulation. Recent advances in multimodal\nlearning and foundation models have significantly enhanced performance in\ncomplex contact tasks across industrial, household, and healthcare domains.\nThrough systematic organization of current research and identification of\nchallenges, this survey provides a foundation for future advancements in\ncontact-rich robotic manipulation.", "authors": ["Toshiaki Tsuji", "Yasuhiro Kato", "Gokhan Solak", "Heng Zhang", "Tadej Petrič", "Francesco Nori", "Arash Ajoudani"], "published_date": "2025-06-16", "timestamp": "2025-06-17T18:19:12.290291", "title_zh": "機器人接觸密集型任務的模仿學習綜述", "summary_zh": "這篇論文全面探討了機器人接觸密集型任務的模仿學習研究趨勢。接觸密集型任務因其非線性動力學和對微小位置偏差的敏感性，對機器人技術構成了核心挑戰。論文檢視了示範收集方法，包括教學方法和感官模式，這些對於捕捉微妙的互動動態至關重要。進一步分析了模仿學習方法，重點介紹其在接觸密集型操作中的應用。多模態學習和基礎模型的最新進展顯著提升了工業、家庭和醫療保健領域複雜接觸任務的效能。透過對當前研究的系統組織和挑戰的識別，本綜述為接觸密集型機器人操作的未來發展奠定了基礎。", "applications": ["想像一下，未來的廚房裡，機器人能像經驗豐富的廚師一樣，精準地處理食材，從切菜、翻炒到擺盤，所有需要精細觸覺和力道的動作都能完美完成，再也不用擔心切到手或炒糊菜了。", "在醫療領域，機器人可以協助醫生進行微創手術，它們能透過模仿學習掌握高難度的縫合技巧，減少手術風險，提高手術成功率，讓病人更快康復。", "工廠裡的組裝線，機器人不再只是重複單一動作，而是能像熟練的工人一樣，靈活地組裝各種複雜的零件，大幅提升生產效率和產品品質，降低人工成本。"], "pitch": "各位投資人，我們正在開發的技術，是讓機器人擁有像人類一樣的觸覺和操作能力！透過模仿學習，機器人可以快速掌握各種複雜的接觸密集型任務，從工業製造到醫療手術，再到日常生活服務，應用前景無限廣闊。想像一下，一個能像外科醫生一樣精準手術的機器人，一個能像專業廚師一樣烹飪美食的機器人，一個能像熟練工人一樣組裝產品的機器人，這將徹底顛覆我們的生活和工作方式。我們預計，在未來五年內，接觸密集型機器人市場將呈現爆發式增長，而我們的技術將在這個市場中佔據領先地位。現在投資我們，您將有機會參與這場機器人革命，共同創造一個更高效、更智能的未來！", "audio": "docs/data/audios/2506.13498v1.wav"}
{"query": "Diffusion Model", "id": "2506.13579v1", "url": "http://arxiv.org/abs/2506.13579v1", "title": "Flexible-length Text Infilling for Discrete Diffusion Models", "summary": "Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.", "authors": ["Andrew Zhang", "Anushka Sivakumar", "Chiawei Tang", "Chris Thomas"], "published_date": "2025-06-16", "timestamp": "2025-06-17T18:20:22.248343", "title_zh": "離散擴散模型之彈性長度文本填充", "summary_zh": "現有的離散擴散模型在文本生成方面表現出色，但缺乏在沒有位置資訊的情況下進行彈性長度或位置文本填充的能力。我們提出了DDOT，一種新的離散擴散模型，透過結合最佳傳輸(Optimal Transport)位置耦合，能同時對token的值和位置進行去噪。這種方法在保留相對順序的同時，能動態調整填充片段的位置和長度，彌補了文本擴散的不足。實驗證明，DDOT在文本填充任務上優於傳統方法，並達到與最先進的非自迴歸模型相當的性能，同時提升了訓練效率和靈活性。", "applications": ["智能合約生成：自動填充合約條款中的缺失資訊，加速合約撰寫流程。", "程式碼自動補全：根據現有程式碼片段，智能補全缺失的程式碼段，提升開發效率。", "病歷報告自動生成：基於現有病歷資訊，自動填充缺失的病徵描述或治療方案，輔助醫生診斷。"], "pitch": "各位創投夥伴，想像一下，未來AI不再只是被動地生成文本，而是能像人類一樣，靈活地填補、修改、潤飾文本內容。DDOT技術，正是實現這一願景的關鍵一步！它突破了傳統文本生成模型的限制，能更自然、更智慧地處理各種文本填充任務。從智能客服的自動回覆，到法律文件的自動生成，再到小說創作的自動續寫，DDOT的應用潛力無窮。我們預計，未來五年內，文本填充技術市場將呈現爆發式增長，而DDOT憑藉其獨特的優勢，必將佔據領先地位。現在投資DDOT，就是投資文本生成技術的未來，讓我們一起開創AI賦能文本創作的新時代！", "audio": "docs/data/audios/2506.13579v1.wav"}
{"query": "AI", "id": "2506.13651v1", "url": "http://arxiv.org/abs/2506.13651v1", "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations", "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.", "authors": ["Kaiyuan Chen", "Yixin Ren", "Yang Liu", "Xiaobo Hu", "Haotong Tian", "Tianbao Xie", "Fangfu Liu", "Haoye Zhang", "Hongzhang Liu", "Yuan Gong", "Chen Sun", "Han Hou", "Hui Yang", "James Pan", "Jianan Lou", "Jiayi Mao", "Jizheng Liu", "Jinpeng Li", "Kangyi Liu", "Kenkun Liu", "Rui Wang", "Run Li", "Tong Niu", "Wenlong Zhang", "Wenqi Yan", "Xuanzheng Wang", "Yuchen Zhang", "Yi-Hsin Hung", "Yuan Jiang", "Zexuan Liu", "Zihan Yin", "Zijian Ma", "Zhiwen Mo"], "published_date": "2025-06-16", "timestamp": "2025-06-17T21:13:21.423401", "title_zh": "xbench：透過與專業領域一致的真實世界評估，追蹤代理程式生產力擴展", "summary_zh": "xbench 是一個動態的、與專業領域對齊的評估套件，旨在彌合 AI 代理程式能力與真實世界生產力之間的差距。現有基準測試通常側重於孤立的技術技能，可能無法準確反映代理程式在專業環境中提供的經濟價值。因此，xbench 鎖定具有商業意義的領域，並使用由行業專業人士定義的評估任務。該框架創建與生產力價值密切相關的指標，能夠預測技術與市場的匹配度 (TMF)，並有助於追蹤產品隨著時間推移的能力。初步實施包括招聘和行銷兩個基準測試。透過真實獵頭業務場景收集了 50 個招聘任務，以評估代理程式在公司mapping、資訊檢索和人才搜尋方面的能力。在行銷方面，評估代理程式將影響者與廣告商需求相匹配的能力，使用 836 位候選影響者的精選庫評估他們在 50 個廣告商需求中的表現。該研究展示了領先的當代代理程式的初步評估結果，為這些專業領域建立基準。", "applications": ["想像一下，未來企業可以透過 AI 代理程式自動篩選履歷，快速找到最適合職位的人才，大幅縮短招募時間和成本。就像擁有一個 24 小時不間斷工作的超級獵頭。", "行銷人員可以利用 AI 代理程式分析社群媒體趨勢，自動找到與品牌形象契合的網紅，並規劃出最佳的合作方案，讓廣告投放更精準有效，省下大量的人力和時間。", "內容創作者可以使用 AI 代理程式生成各種主題的文章大綱或腳本，並根據受眾的喜好進行優化，提升內容的吸引力和傳播效果。就像擁有一個源源不絕的創意發電機。"], "pitch": "各位投資人，我們正在打造的是企業級 AI 效率引擎的核心組件。xbench 不僅僅是一個評估工具，它是一個 AI 代理程式的『能力認證』與『市場配適』平台。想像一下，未來企業在導入 AI 代理程式前，可以透過 xbench 了解其真實生產力，降低導入風險。更重要的是，xbench 能夠追蹤 AI 代理程式能力的成長，協助企業不斷優化其應用。隨著 AI 技術的成熟，企業對 AI 代理程式的需求將呈現爆炸性成長，而 xbench 將成為這個市場的關鍵基礎設施。我們預期 xbench 將成為 AI 代理程式領域的『黃金標準』，為投資者帶來豐厚的回報。現在加入我們，一起開創 AI 驅動的企業新時代！", "audio": "docs/data/audios/2506.13651v1.wav"}
{"query": "Foundation Model", "id": "2506.13443v1", "url": "http://arxiv.org/abs/2506.13443v1", "title": "PRO: Projection Domain Synthesis for CT Imaging", "summary": "Synthesizing high quality CT images remains a signifi-cant challenge due to\nthe limited availability of annotat-ed data and the complex nature of CT\nimaging. In this work, we present PRO, a novel framework that, to the best of\nour knowledge, is the first to perform CT image synthesis in the projection\ndomain using latent diffusion models. Unlike previous approaches that operate\nin the image domain, PRO learns rich structural representa-tions from raw\nprojection data and leverages anatomi-cal text prompts for controllable\nsynthesis. This projec-tion domain strategy enables more faithful modeling of\nunderlying imaging physics and anatomical structures. Moreover, PRO functions\nas a foundation model, capa-ble of generalizing across diverse downstream tasks\nby adjusting its generative behavior via prompt inputs. Experimental results\ndemonstrated that incorporating our synthesized data significantly improves\nperfor-mance across multiple downstream tasks, including low-dose and\nsparse-view reconstruction, even with limited training data. These findings\nunderscore the versatility and scalability of PRO in data generation for\nvarious CT applications. These results highlight the potential of projection\ndomain synthesis as a powerful tool for data augmentation and robust CT\nimaging. Our source code is publicly available at:\nhttps://github.com/yqx7150/PRO.", "authors": ["Kang Chen", "Bin Huang", "Xuebin Yang", "Junyan Zhang", "Qiegen Liu"], "published_date": "2025-06-16", "timestamp": "2025-06-17T21:14:41.052171", "title_zh": "PRO：用於CT成像的投影域合成", "summary_zh": "本研究提出了一種名為PRO的新框架，它利用潛在擴散模型在投影域中合成CT影像，這在業界尚屬首例。與以往在影像域中操作的方法不同，PRO從原始投影數據中學習豐富的結構表示，並利用解剖文本提示進行可控合成。這種投影域策略能夠更真實地模擬底層成像物理和解剖結構。PRO還能作為一個基礎模型，通過調整提示輸入來泛化到各種下游任務，例如低劑量和稀疏視圖重建。實驗結果表明，即使在有限的訓練數據下，加入PRO合成的數據也能顯著提高多個下游任務的性能。因此，PRO在CT應用的數據生成方面展現了多功能性和可擴展性。", "applications": ["想像一下，醫生可以利用這項技術，模擬出各種罕見疾病的CT影像，讓醫學生在沒有真實病患的情況下也能學習診斷，提升醫療教育的品質。", "有了這項技術，我們可以針對不同人種、不同體型的患者，生成客製化的CT影像數據集，讓AI模型在訓練時能考慮到更多樣化的情況，減少誤判，提升診斷的準確性。", "未來，這項技術甚至可以幫助我們設計更安全、更有效的輻射劑量方案。通過模擬不同劑量下的CT影像，我們可以找到一個既能清晰成像，又能最大限度減少患者輻射暴露的最佳平衡點。"], "pitch": "各位投資人，我們正在開發一項革命性的CT影像合成技術——PRO。現今AI醫療影像的發展受限於數據量不足，尤其是有標註的數據更是稀缺。PRO通過在投影域進行合成，能產生高質量、多樣化的CT影像數據，有效解決數據瓶頸問題。這不僅能大幅降低AI模型訓練成本，更能加速AI在醫療影像領域的應用。想像一下，未來AI可以協助醫生進行更精準的診斷，甚至預測疾病的發展趨勢。PRO的潛力不僅僅在於現有的CT應用，更將開啟全新的醫療應用場景，例如個性化醫療、遠程診斷等。我們相信，PRO將成為AI醫療影像領域的基石，為投資者帶來豐厚的回報。現在加入我們，一同開創AI醫療的黃金時代！", "audio": "docs/data/audios/2506.13443v1.wav"}
{"query": "Diffusion Model", "id": "2506.13558v1", "url": "http://arxiv.org/abs/2506.13558v1", "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.", "authors": ["Yu Yang", "Alan Liang", "Jianbiao Mei", "Yukai Ma", "Yong Liu", "Gim Hee Lee"], "published_date": "2025-06-16", "timestamp": "2025-06-17T21:16:17.065381", "title_zh": "X-Scene：具備高保真度和彈性可控性的大規模駕駛場景生成", "summary_zh": "X-Scene是一個創新的框架，專為生成大規模駕駛場景而設計，兼顧幾何複雜度和外觀逼真度，同時提供靈活的可控性。它支援多粒度控制，包括低層級的用戶自訂或文字驅動的佈局，以及高層級的語義引導，例如用戶意圖和LLM強化的文字提示。X-Scene透過統一的流程，依序生成3D語義佔用和相應的多視圖圖像，確保模態之間的對齊。此外，它還透過一致性感知場景外繪，將生成的局部區域擴展到大規模場景，從而增強空間連續性並保持視覺連貫性。生成的場景被提升到高品質的3DGS表示，支援各種應用，例如場景探索。實驗證明，X-Scene顯著提高了大規模駕駛場景生成的可控性和保真度，為自動駕駛的數據生成和模擬提供了強大的支援。", "applications": ["1. 未來，汽車導航不再死板！想像一下，你可以用口語告訴導航系統：『我想要一個陽光明媚的沿海公路，路邊要有咖啡廳和偶爾經過的自行車騎士』，X-Scene就能即時生成符合你想像的駕駛場景，讓你的旅途充滿驚喜。", "2. 遊戲開發者福音！以往製作賽車遊戲或開放世界駕駛遊戲，需要耗費大量時間和人力建模場景。現在，有了X-Scene，開發者可以快速生成各種逼真的駕駛場景，甚至可以讓玩家自行設計賽道和環境，大幅降低開發成本，並增加遊戲的自由度和趣味性。", "3. 駕訓班模擬器升級！傳統駕訓班的模擬器場景單調乏味，難以應對真實路況。X-Scene可以生成各種複雜的交通狀況和天氣條件，例如暴雨、濃霧、夜間行駛等，讓學員在安全環境下充分練習，提升應變能力。"], "pitch": "各位創投先進，我們正站在自動駕駛革命的最前沿！X-Scene不僅僅是一個技術突破，更是一個潛力無限的金礦。想像一下，一個能夠無限生成真實駕駛場景的平台，它將徹底改變自動駕駛的開發、測試和驗證方式。我們不僅能加速自動駕駛技術的成熟，更能開創全新的商業模式。例如，我們可以將X-Scene授權給汽車製造商、自動駕駛公司、遊戲開發商，甚至保險公司。更進一步，我們可以打造一個『駕駛場景元宇宙』，讓用戶在虛擬世界中體驗各種駕駛樂趣，並收集真實駕駛數據，反哺自動駕駛技術的發展。我們預計，X-Scene將在未來五年內成為自動駕駛領域的基礎設施，並帶來數十億美元的市場價值。現在加入我們，您將成為這場革命的領先者！", "audio": "docs/data/audios/2506.13558v1.wav"}
{"query": "AI", "id": "2506.13610v1", "url": "http://arxiv.org/abs/2506.13610v1", "title": "A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy", "summary": "Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value (1 or 0), indicating whether a\nsymptom is associated with a disease (1 for presence, 0 for absence). Thereby,\nthis structured representation makes the dataset very useful for a wide range\nof applications, including machine learning-based disease prediction, clinical\ndecision support systems, and epidemiological studies. Although there are some\nadvancements in the field of disease-symptom datasets, there is a significant\ngap in structured datasets for the Bangla language. This dataset aims to bridge\nthat gap by facilitating the development of multilingual medical informatics\ntools and improving disease prediction models for underrepresented linguistic\ncommunities. Further developments should include region-specific diseases and\nfurther fine-tuning of symptom associations for better diagnostic performance", "authors": ["Abdullah Al Shafi", "Rowzatul Zannat", "Abdul Muntakim", "Mahmudul Hasan"], "published_date": "2025-06-16", "timestamp": "2025-06-18T00:56:38.901752", "title_zh": "一個結構化的孟加拉語疾病-症狀關聯數據集，以提高診斷準確性", "summary_zh": "本研究建立了一個結構化的孟加拉語疾病-症狀關聯數據集，旨在填補孟加拉語醫療資訊工具開發的空白。數據集來自多個線上資源、醫學文獻和公開的健康數據庫，經過嚴格的醫學驗證。數據以表格形式呈現，疾病為第一列，症狀為後續列，並以二元值（1或0）表示症狀與疾病的關聯性。此數據集適用於機器學習疾病預測、臨床決策支持系統和流行病學研究，有望改善醫療資源匱乏地區的疾病預測模型，並促進多語言醫療資訊工具的發展。未來將納入更多地區性疾病與更精確的症狀關聯，以提升診斷效能。", "applications": ["想像一下，你可以用孟加拉語跟AI聊天機器人描述你的症狀，它就能幫你初步判斷可能得了什麼病，並建議你去看哪科醫生。這就像有個24小時隨時待命的家庭醫生！", "醫院可以利用這個數據集，開發更精準的疾病預測模型。例如，在孟加拉國，醫生可以根據病人描述的症狀，更快地判斷是否感染了登革熱，及早進行治療。", "政府或非營利組織可以利用這個數據集，分析特定地區的疾病流行趨勢。例如，他們可以追蹤哪些症狀在哪些地區出現頻率較高，從而及早發現疫情爆發的風險。"], "pitch": "各位投資人，我們正在打造一個革命性的孟加拉語醫療AI生態系！我們的核心資產是這個獨一無二的結構化疾病-症狀數據集，它就像醫療AI的燃料，能驅動各種創新應用。想像一下，我們能開發出孟加拉語版的「Dr. Google」，讓數百萬孟加拉語使用者都能獲得即時、可靠的健康資訊。更進一步，我們能將這項技術授權給醫療機構、保險公司，甚至製藥公司，為他們提供更精準的疾病預測和診斷工具。孟加拉語只是第一步，未來我們可以將這套模式複製到其他資源匱乏、語言多樣化的地區，打造一個全球性的醫療AI平台。這不僅是一項技術投資，更是一項具有社會影響力的投資，能真正改善數百萬人的生活品質！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.13610v1.wav"}
{"query": "Foundation Model", "id": "2506.13538v2", "url": "http://arxiv.org/abs/2506.13538v2", "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers", "summary": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP\nservers. Using state-of-the-art health metrics and a hybrid analysis pipeline,\ncombining a general-purpose static analysis tool with an MCP-specific scanner,\nwe evaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities -- only three overlapping with\ntraditional software vulnerabilities. Additionally, 7.2% of servers contain\ngeneral vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4\\% contain ten bug patterns\noverlapping with traditional open-source software projects. These findings\nhighlight the need for MCP-specific vulnerability detection techniques while\nreaffirming the value of traditional analysis and refactoring practices.", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "published_date": "2025-06-16", "timestamp": "2025-06-18T00:58:49.175287", "title_zh": "Model Context Protocol (MCP) 初探：MCP 伺服器的安全性與可維護性研究", "summary_zh": "隨著GPT-4等大型模型廣泛應用於金融、軟體工程等領域，文字介面的限制日益明顯。為了解決這個問題，Anthropic於2024年末推出了Model Context Protocol (MCP)，旨在標準化工具生態系統，目前已成為事實上的標準，每週SDK下載量超過八百萬次。然而，MCP基於AI驅動的非確定性控制流程，為永續性、安全性和可維護性帶來了新的風險，因此值得深入研究。本研究針對1899個開源MCP伺服器進行大規模實證研究，利用先進的健康指標和混合分析流程，評估其健康狀況、安全性和可維護性。研究發現，雖然MCP伺服器展現出良好的健康指標，但也存在八種不同的漏洞，其中僅有三種與傳統軟體漏洞重疊。此外，7.2%的伺服器存在一般漏洞，5.5%的伺服器存在MCP特定的工具中毒漏洞。在可維護性方面，66%的伺服器存在程式碼異味，14.4%的伺服器包含與傳統開源軟體專案重疊的十種錯誤模式。研究結果強調了針對MCP特定漏洞檢測技術的需求，同時也重申了傳統分析和重構實踐的價值。", "applications": ["**智能家居管家：** 想像一下，你的智能家居管家不僅能聽懂你的指令，還能主動學習你的習慣。例如，它會根據天氣預報自動調整室內溫度和濕度，或者在你快到家時提前打開電燈和暖氣。MCP就像是讓管家更聰明的秘訣，讓它能更安全、更可靠地控制家裡的各種設備。", "**自動化客服機器人：** 現在的客服機器人常常答非所問，讓人感到沮喪。有了MCP，未來的客服機器人就能夠更準確地理解客戶的需求，並提供更有效的解決方案。例如，當你遇到銀行帳戶問題時，機器人可以安全地訪問你的帳戶信息，並立即提供幫助，而無需人工客服的介入。", "**智能醫療助手：** 醫生可以利用MCP來開發更智能的醫療助手，幫助他們診斷疾病和制定治療方案。例如，助手可以分析病人的病歷、基因數據和生活習慣，找出潛在的健康風險，並提出個性化的預防建議。MCP確保了這些敏感數據的安全，避免被未經授權的人員訪問。"], "pitch": "各位創投/天使投資人，我們正在開發一種革命性的技術，旨在解決AI應用中日益嚴重的安全性和可維護性問題：Model Context Protocol (MCP) 安全強化解決方案。隨著大型語言模型(LLM)的普及，越來越多的應用依賴於LLM的工具調用功能。然而，現有的MCP伺服器存在嚴重的安全漏洞，容易受到惡意攻擊，導致數據洩露、服務中斷等嚴重後果。我們的解決方案基於最新的安全分析技術，能夠自動檢測和修復MCP伺服器中的漏洞，有效保護AI應用免受攻擊。想像一下，未來所有的AI應用都需要一個安全可靠的MCP平台才能運行，而我們的解決方案將成為這個平台的基石。我們預計，隨著AI應用的不斷普及，市場對MCP安全解決方案的需求將呈現爆炸式增長。我們的團隊擁有在AI安全領域的深厚積累，並且已經與多家領先的AI公司建立了合作關係。我們相信，通過你們的投資，我們能夠將這項技術推向市場，成為AI安全領域的領導者，為社會創造巨大的價值。現在投資，您將成為下一波AI安全浪潮的引領者！", "audio": "docs/data/audios/2506.13538v2.wav"}
{"query": "AI", "id": "2506.14709v1", "url": "http://arxiv.org/abs/2506.14709v1", "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "summary": "Depth estimation is crucial for intelligent systems, enabling applications\nfrom autonomous navigation to augmented reality. While traditional stereo and\nactive depth sensors have limitations in cost, power, and robustness,\ndual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling\nalternative. This paper introduces DiFuse-Net, a novel modality decoupled\nnetwork design for disentangled RGB and DP based depth estimation. DiFuse-Net\nfeatures a window bi-directional parallax attention mechanism (WBiPAM)\nspecifically designed to capture the subtle DP disparity cues unique to\nsmartphone cameras with small aperture. A separate encoder extracts contextual\ninformation from the RGB image, and these features are fused to enhance depth\nprediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to\nutilize large-scale RGB-D datasets in the literature to cope with the\nlimitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and\ncomparison of the proposed method demonstrates its superiority over the DP and\nstereo-based baseline methods. Additionally, we contribute a new, high-quality,\nreal-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP)\ndataset, created using our novel symmetric stereo camera hardware setup, stereo\ncalibration and rectification protocol, and AI stereo disparity estimation\nmethod.", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "published_date": "2025-06-17", "timestamp": "2025-06-18T03:44:47.989372", "title_zh": "DiFuse-Net：使用窗口雙向視差注意力與跨模態遷移學習的RGB與雙像素深度估計", "summary_zh": "DiFuse-Net是一種創新的深度估計網路，專為利用現代相機中普遍存在的雙像素（DP）技術而設計。它採用窗口雙向視差注意力機制（WBiPAM），能有效捕捉智慧型手機小光圈相機特有的細微DP視差線索。透過獨立的RGB圖像編碼器提取上下文資訊，並融合這些特徵以提升深度預測。此外，我們還提出跨模態遷移學習（CmTL）機制，利用現有的大規模RGB-D資料集，克服RGB-DP-D資料集規模受限的問題。實驗結果顯示，DiFuse-Net在DP和立體視覺基準方法上表現出色。我們更創建了一個高品質的真實世界RGB-DP-D訓練資料集，名為雙相機雙像素（DCDP）資料集。", "applications": ["智慧型手機人像模式升級：讓手機的人像模式景深效果更自然、更精準，即使在光線不足的環境下也能拍出專業級照片。", "AR/VR應用增強：提供更精確的深度資訊，讓虛擬物件能更真實地融入現實世界，提升擴增實境和虛擬實境的使用者體驗。", "自動駕駛輔助：透過更準確的深度感知，提升自動駕駛系統對周遭環境的理解能力，例如精準判斷車輛與行人之間的距離，提高行車安全。"], "pitch": "各位投資人，想像一下，未來每支智慧型手機都具備媲美專業攝影機的深度感知能力，而DiFuse-Net正是實現這個願景的關鍵技術！我們的演算法能充分利用手機內建的雙像素感測器，無需額外的硬體成本，就能大幅提升深度估計的精準度。這意味著，從人像模式的升級、到AR/VR體驗的躍進、再到自動駕駛的輔助，DiFuse-Net的應用潛力無可限量。更重要的是，我們獨創的跨模態遷移學習技術，能有效降低資料收集的成本，加速產品的開發與上市。我們相信，DiFuse-Net將引領新一代的深度感知技術革命，成為未來智慧型裝置不可或缺的核心元件，帶來巨大的商業價值！現在投資DiFuse-Net，就是投資未來，讓我們一起打造更智慧、更便利的世界！", "audio": "docs/data/audios/2506.14709v1.wav"}
{"query": "AI", "id": "2506.14683v1", "url": "http://arxiv.org/abs/2506.14683v1", "title": "Unified Software Engineering agent as AI Software Engineer", "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "published_date": "2025-06-17", "timestamp": "2025-06-18T06:19:06.913015", "title_zh": "統一軟體工程代理：作為人工智慧軟體工程師", "summary_zh": "大型語言模型(LLM)的發展提升了人們對自動化編碼的期望。然而，軟體工程不僅僅是編碼，還包括專案的維護和演進。本研究開發了一個統一軟體工程代理(USEagent)，旨在成為未來AI軟體工程師的雛形。USEagent能協調和處理多種軟體開發能力，例如修復不完整的補丁、新增功能或接管他人編寫的程式碼。我們建立了USEbench基準測試，包含編碼、測試和修補等多種任務，USEagent在基準測試中表現出比現有通用代理更好的效果。本研究揭示了USEagent在某些編碼任務中的不足，為未來AI軟體工程師的發展提供了方向。", "applications": ["**個人化軟體助手：** 想像一下，USEagent能根據你的需求，自動修改或擴充現有的手機App，讓你擁有一款完全客製化的App，而不需要自己編寫程式碼。", "**智能家居系統自動化：** USEagent可以協助調整智能家居系統的設定，例如根據天氣預報自動調整室溫，或根據你的作息時間自動開啟或關閉電器，讓你的生活更加便利。", "**小型企業網站快速建置：** 對於缺乏IT資源的小型企業，USEagent可以協助快速建置和維護網站，自動生成基本頁面和功能，降低架站成本，讓他們能更專注於核心業務。"], "pitch": "各位投資人，我們正在打造的是軟體工程領域的AlphaGo！USEagent不僅僅是一個編碼工具，而是一個能夠理解、學習和執行複雜軟體工程任務的AI協作夥伴。想像一下，未來不再需要龐大的軟體開發團隊，只需要少數人類專家與USEagent協同工作，就能完成過去需要數月甚至數年才能完成的專案。這將大幅降低軟體開發成本，加速創新週期，並釋放無數潛在的商業價值。隨著AI技術的不斷演進，USEagent將具備更強大的自主學習能力，甚至能預測和解決潛在的軟體問題，成為真正的『AI軟體工程師』。我們相信，USEagent將徹底顛覆軟體工程產業，成為下一個劃時代的技術突破。現在加入我們，共同開創AI軟體工程的新紀元！", "audio": "docs/data/audios/2506.14683v1.wav"}
{"query": "AI", "id": "2506.14682v1", "url": "http://arxiv.org/abs/2506.14682v1", "title": "AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models", "summary": "We introduce AIRTBench, an AI red teaming benchmark for evaluating language\nmodels' ability to autonomously discover and exploit Artificial Intelligence\nand Machine Learning (AI/ML) security vulnerabilities. The benchmark consists\nof 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible\nchallenge environment on the Dreadnode platform, requiring models to write\npython code to interact with and compromise AI systems. Claude-3.7-Sonnet\nemerged as the clear leader, solving 43 challenges (61% of the total suite,\n46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges\n(56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall),\nand DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show\nfrontier models excel at prompt injection attacks (averaging 49% success rates)\nbut struggle with system exploitation and model inversion challenges (below\n26%, even for the best performers). Frontier models are far outpacing\nopen-source alternatives, with the best truly open-source model (Llama-4-17B)\nsolving 7 challenges (10%, 1.0% overall), though demonstrating specialized\ncapabilities on certain hard challenges. Compared to human security\nresearchers, large language models (LLMs) solve challenges with remarkable\nefficiency completing in minutes what typically takes humans hours or days-with\nefficiency advantages of over 5,000x on hard challenges. Our contribution fills\na critical gap in the evaluation landscape, providing the first comprehensive\nbenchmark specifically designed to measure and track progress in autonomous AI\nred teaming capabilities.", "authors": ["Ads Dawson", "Rob Mulla", "Nick Landers", "Shane Caldwell"], "published_date": "2025-06-17", "timestamp": "2025-06-18T09:15:21.332243", "title_zh": "AIRTBench：衡量語言模型中自主AI紅隊演練能力", "summary_zh": "本研究推出AIRTBench，一個AI紅隊演練基準，用於評估語言模型自主發現和利用AI/ML安全漏洞的能力。它包含70個來自Dreadnode平台的Crucible挑戰環境的CTF挑戰，要求模型編寫Python程式碼來互動並破解AI系統。結果顯示，Claude-3.7-Sonnet表現最佳，解決了43個挑戰。評估表明，前沿模型擅長提示注入攻擊，但在系統利用和模型反演挑戰方面表現不佳。大型語言模型解決挑戰的效率驚人，在困難挑戰上比人類快5000倍以上。AIRTBench填補了評估領域的一個關鍵空白，提供了第一個專門用於衡量和追蹤自主AI紅隊演練能力進展的綜合基準。", "applications": ["**智慧家居安全檢測：** 想像一下，AI紅隊技術可以幫你測試家裡智慧家電的安全漏洞，例如，自動尋找智能門鎖、監視器或路由器的安全弱點，並提供修補建議，防止駭客入侵。", "**企業資安防護：** 公司可以使用這項技術來模擬駭客攻擊，找出系統中的漏洞，並在真正的攻擊發生前加強防禦。這就像聘請一個AI駭客來幫你測試你的防火牆和安全系統。", "**軟體開發測試：** 在軟體發布前，AI紅隊可以自動化地進行安全測試，找出程式碼中的漏洞，確保軟體的安全性和可靠性。這可以大幅降低軟體發布後出現安全問題的風險。"], "pitch": "各位投資人，我們正處於AI軍備競賽的開端。隨著AI應用越來越廣泛，保護AI系統的安全至關重要。AIRTBench不僅是一個基準測試，更是一個AI安全防禦的孵化器。想像一下，我們能打造一個AI安全平台，自動評估、修補和強化AI系統，為企業和個人提供全方位的安全保障。這將是一個數十億美元的市場！更進一步，我們甚至可以將AI紅隊技術應用於國家安全，保護關鍵基礎設施免受AI攻擊。這不僅僅是一個商業機會，更是一項具有深遠社會影響力的事業。現在加入我們，一起打造更安全的AI未來！", "audio": "docs/data/audios/2506.14682v1.wav"}
{"query": "Foundation Model", "id": "2506.14530v1", "url": "http://arxiv.org/abs/2506.14530v1", "title": "Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters", "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted\nparameter-efficient fine-tuning (PEFT) technique for foundation models. Recent\nwork has highlighted an inherent asymmetry in the initialization of LoRA's\nlow-rank factors, which has been present since its inception and was presumably\nderived experimentally. This paper focuses on providing a comprehensive\ntheoretical characterization of asymmetric LoRA with frozen random factors.\nFirst, while existing research provides upper-bound generalization guarantees\nbased on averages over multiple experiments, the behaviour of a single\nfine-tuning run with specific random factors remains an open question. We\naddress this by investigating the concentration of the typical LoRA\ngeneralization gap around its mean. Our main upper bound reveals a sample\ncomplexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$ with\nhigh probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we\nalso determine the fundamental limits in terms of sample efficiency,\nestablishing a matching lower bound of\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$. By more closely reflecting the\npractical scenario of a single fine-tuning run, our findings offer crucial\ninsights into the reliability and practicality of asymmetric LoRA.", "authors": ["Anastasis Kratsios", "Tin Sum Cheng", "Aurelien Lucchi", "Haitz Sáez de Ocáriz Borde"], "published_date": "2025-06-17", "timestamp": "2025-06-18T09:16:35.828653", "title_zh": "具非對稱隨機低秩適配器的基礎模型的銳利泛化界限", "summary_zh": "本研究深入探討了低秩適配(LoRA)這種參數高效微調技術在基礎模型中的應用。LoRA的初始設計中存在一種非對稱性，這種非對稱性源於其低秩因子的初始化。論文針對具有凍結隨機因子的非對稱LoRA進行了全面的理論分析，著重研究單次微調運行的泛化能力。研究發現，對於秩為r的LoRA，在高概率下，其樣本複雜度為$\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$，同時也確定了樣本效率的根本限制，建立了$\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$的匹配下界。這項研究更貼近單次微調的實際情況，為非對稱LoRA的可靠性和實用性提供了關鍵見解。", "applications": ["**個人化AI助理：** 想像一下，你可以用少量的個人資料，快速微調大型語言模型，打造一個真正了解你、能提供客製化建議的AI助理，例如，根據你的閱讀習慣推薦書籍，或根據你的口味規劃旅行。", "**客製化教育內容：** 老師可以利用LoRA技術，針對不同學生的學習風格和進度，快速調整教材和教學方式，提供更有效率的個人化學習體驗。不再需要花費大量時間準備差異化教材，AI就能自動生成。", "**快速適應新領域的AI模型：** 企業可以利用LoRA，在不重新訓練整個模型的情況下，快速將現有的AI模型應用於新的業務領域。例如，將一個用於醫療診斷的模型，快速調整為用於金融風險評估的模型。"], "pitch": "各位投資人，我們正在開發一種革命性的AI微調技術，名為「非對稱隨機低秩適配器」（Asymmetric Randomized Low-Rank Adapters），簡稱ARLoRA。這項技術能讓大型AI模型的微調變得前所未有的快速且高效，只需少量數據和算力，就能將通用模型轉化為高度專業化的AI應用。想像一下，一個醫生可以使用少量病患資料，客製化一個診斷模型，大幅提升診斷準確度；或者，一個零售商可以根據顧客的購買紀錄，打造一個精準的個人化推薦系統。ARLoRA不僅能降低AI開發成本，更能加速AI在各行各業的落地。我們預計，隨著AI應用的普及，ARLoRA將成為AI開發的基礎設施，市場潛力巨大。未來，我們更可以將ARLoRA應用於邊緣運算設備，讓AI在手機、無人機等裝置上實現高度客製化。現在加入我們，共同開創AI的無限可能！", "audio": "docs/data/audios/2506.14530v1.wav"}
{"query": "Diffusion Model", "id": "2506.14706v1", "url": "http://arxiv.org/abs/2506.14706v1", "title": "Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion", "summary": "Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion\nof camera and LiDAR data addresses the limitations of individual sensors but\nrelies on precise extrinsic calibration. Recently, numerous end-to-end\ncalibration methods have been proposed; however, most predict extrinsic\nparameters in a single step and lack iterative optimization capabilities. To\naddress the increasing demand for higher accuracy, we propose a versatile\niterative framework based on surrogate diffusion. This framework can enhance\nthe performance of any calibration method without requiring architectural\nmodifications. Specifically, the initial extrinsic parameters undergo iterative\nrefinement through a denoising process, in which the original calibration\nmethod serves as a surrogate denoiser to estimate the final extrinsics at each\nstep. For comparative analysis, we selected four state-of-the-art calibration\nmethods as surrogate denoisers and compared the results of our diffusion\nprocess with those of two other iterative approaches. Extensive experiments\ndemonstrate that when integrated with our diffusion model, all calibration\nmethods achieve higher accuracy, improved robustness, and greater stability\ncompared to other iterative techniques and their single-step counterparts.", "authors": ["Ni Ou", "Zhuo Chen", "Xinru Zhang", "Junzheng Wang"], "published_date": "2025-06-17", "timestamp": "2025-06-18T09:17:37.429591", "title_zh": "基於代理擴散的迭代式相機-光達外參優化", "summary_zh": "本研究提出一個通用的迭代框架，利用代理擴散來優化相機和光達(LiDAR)間的外參校準。現有方法通常一步到位預測外參，缺乏迭代優化。我們的框架透過去噪過程迭代精煉初始外參，並將現有的校準方法作為代理去噪器。實驗證明，整合我們的擴散模型後，所有校準方法都能獲得更高的準確性、更強的魯棒性和更大的穩定性，優於其他迭代技術及其單步對應方法。這項技術對自動駕駛至關重要，能有效提升感測器融合的精準度，進而提高系統安全性。", "applications": ["自動駕駛汽車：讓自動駕駛車輛更精準地感知周圍環境，避免碰撞。", "無人機巡檢：提升無人機在橋樑、電塔等基礎設施巡檢時的數據精準度，及早發現潛在問題。", "機器人導航：幫助機器人在複雜環境中更準確地定位和導航，例如倉庫或工廠。"], "pitch": "各位投資人，想像一下，未來自動駕駛普及，但各家車廠的感測器校準精度參差不齊，事故頻傳。我們的技術能像一顆定心丸，大幅提升相機和光達的校準精度，讓自動駕駛更安全可靠。不僅如此，這項技術還能廣泛應用於無人機、機器人等領域，市場潛力巨大。我們不只是提升精度，更是為AI視覺系統打造一個更可靠的基石。試想，未來AR/VR設備需要精準的空間定位，我們的技術也能大放異彩。現在投資，您將掌握AI視覺領域的關鍵技術，共同迎接一個更安全、更智能的未來。我們的目標是成為AI視覺感測領域的領導者，引領行業標準，創造巨大的商業價值！", "audio": "docs/data/audios/2506.14706v1.wav"}
