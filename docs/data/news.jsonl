{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:19:31.539877", "title_zh": "視覺圖形競技場：評估視覺和多模態大型語言模型的視覺概念化能力", "summary_zh": "近年來，多模態大型語言模型在視覺問答方面取得了顯著進展。然而，AI在「概念化」能力上仍存在差距，也就是辨識和推理相同概念，不受視覺形式變化的影響。為了解決這個問題，我們推出了視覺圖形競技場（VGA），它是一個包含六個基於圖形的任務的數據集，旨在評估和提升AI系統的視覺抽象能力。VGA使用不同的圖形佈局來測試獨立於視覺形式的推理。實驗結果顯示，人類在各項任務中幾乎達到完美準確度，而模型在同構檢測方面完全失敗，在路徑/循環任務方面表現有限，這突顯了當前AI模型在視覺理解方面的根本局限性。VGA提供了一個框架，旨在推動AI視覺模型在概念化方面取得類似人類的進展。", "applications": ["**自動駕駛：** 讓汽車能辨識不同角度或光線下的交通標誌，確保行車安全。例如，即使交通標誌被樹葉遮蔽一部分，或因光線反射而變形，汽車也能正確判斷其意義。", "**醫療影像分析：** 協助醫生辨識X光片或斷層掃描中不同形態的腫瘤，提高診斷準確性。例如，即使腫瘤形狀不規則或與周圍組織融合，AI也能準確辨識並標記。", "**智慧零售：** 讓機器人能辨識貨架上不同包裝或擺放方式的商品，提升倉儲和物流效率。例如，即使商品條碼被遮蓋，或商品被隨意堆放，機器人也能準確辨識商品種類和數量。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！視覺圖形競技場（VGA）不僅僅是一個數據集，它是解鎖AI真正視覺理解能力的鑰匙。試想一下，一個能像人類一樣理解世界，不受視覺表象干擾的AI，它將顛覆自動駕駛、醫療診斷、智慧製造等各個領域。目前AI在概念化方面的不足，正是我們VGA的機會。我們正在打造下一代AI視覺引擎，它將超越簡單的模式匹配，真正理解圖像背後的概念。這意味著更安全可靠的自動駕駛、更精準高效的醫療診斷、以及更智能化的生產流程。我們的團隊由頂尖的AI專家組成，我們有信心將VGA打造成AI視覺領域的黃金標準。現在投資VGA，您不僅僅是投資一個數據集，更是投資一個充滿無限可能的未來！讓我們一起引領這場視覺智能的革命，共同創造一個更智能、更美好的世界！", "audio": "docs/data/audios/2506.06242v1.wav"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:20:54.025845", "title_zh": "TerraFM：適用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "TerraFM是一個利用深度學習技術，結合Sentinel-1和Sentinel-2衛星影像的可擴展自監督學習模型。它透過獨特的空間瓦片和土地覆蓋感知採樣方法，豐富了空間和語義覆蓋範圍。TerraFM將雷達和光學輸入視為自然增強，透過模態特定的patch嵌入和自適應交叉注意力融合來統一處理。其訓練策略結合了局部-全局對比學習，並引入雙中心機制，以解決土地覆蓋中長尾分佈的問題。TerraFM在分類和分割任務上表現出色，優於先前的模型，為地球觀測領域帶來了更強大的通用性和準確性。", "applications": ["農作物監測：農民可以利用TerraFM分析衛星影像，了解農作物的生長狀況、預測產量，及早發現病蟲害，提高農業生產效率。", "災害評估：在地震、洪水等災害發生後，TerraFM可以快速分析災區的受損情況，協助救援人員制定更有效的救援計畫，並進行災後重建。", "環境保護：環保機構可以利用TerraFM監測森林砍伐、水污染等環境問題，及時採取措施保護地球資源。"], "pitch": "各位創投先進，想像一下，我們正站在一個前所未有的數據金礦之上：地球觀測數據！TerraFM，我們的殺手級應用，正是開啟這座寶藏的鑰匙。它不僅能整合不同衛星感測器的數據，更具備強大的泛化能力，能應用於農業、災害管理、環境監測等各個領域。這意味著什麼？更精準的作物預測，減少糧食浪費；更快速的災害評估，拯救更多生命；更有效的環境監測，守護我們的地球。但這還不是全部！TerraFM的自監督學習能力，使其能不斷從海量數據中自我提升，就像一個永動機，不斷產生價值。未來，我們甚至可以將TerraFM應用於城市規劃、基礎設施建設、甚至是國防安全等更廣闊的領域。現在投資TerraFM，就是投資地球的未來，回報將遠超您的想像！", "audio": "docs/data/audios/2506.06281v1.wav"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:22:30.194724", "title_zh": "STARFlow：擴展潛在歸一化流以實現高解析度圖像合成", "summary_zh": "STARFlow是一種基於歸一化流的可擴展生成模型，在高解析度圖像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了歸一化流的表達能力和自迴歸Transformer的結構化建模能力。STARFlow通過深度-淺層設計、在預訓練自編碼器的潛在空間中建模以及創新的引導算法，顯著提高了可擴展性。該模型保持端到端的歸一化流，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類條件和文本條件圖像生成任務中表現出色，其樣本品質接近最先進的擴散模型。這是首次成功展示歸一化流在此規模和解析度下有效運作。", "applications": ["想像一下，你想要一張獨一無二的寵物照片，但你沒有專業攝影師。STARFlow可以根據你的文字描述，例如「一隻戴著皇冠的可愛貓咪」，自動生成一張高解析度的照片。", "假設你是遊戲開發者，需要大量不同的遊戲角色和場景。STARFlow可以幫助你快速生成各種風格的遊戲素材，節省大量美術設計的時間和成本。", "如果你是室內設計師，想向客戶展示不同裝修風格的效果圖。STARFlow可以根據客戶的描述，快速生成逼真的室內設計圖，方便客戶選擇。"], "pitch": "各位投資人，我們帶來的是STARFlow，一項革命性的圖像生成技術，它將徹底改變圖像內容創作的遊戲規則！想像一下，一個可以根據簡單的文字描述，就能生成照片級別真實圖像的世界。STARFlow不僅僅是一個技術突破，它是一座金礦！在廣告行銷領域，它可以創造出高度個性化的廣告素材，大幅提升點擊率和轉換率。在娛樂產業，它可以賦予遊戲開發者和電影製作人前所未有的創作自由。在電商領域，它可以自動生成商品圖片，降低運營成本。更重要的是，隨著元宇宙的興起，對虛擬內容的需求將呈現爆炸式增長，而STARFlow正是滿足這一需求的完美解決方案。我們的團隊擁有世界一流的AI專家，我們已經成功驗證了STARFlow的技術可行性和商業潛力。現在，我們需要您的投資，共同將STARFlow推向市場，搶佔先機，打造一個全新的圖像內容生態系統。我們相信，STARFlow將成為下一代圖像生成技術的領導者，為您帶來豐厚的回報！", "audio": "docs/data/audios/2506.06276v1.wav"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:31:58.254264", "title_zh": "以外科手術數據挑戰視覺語言模型：一個新數據集與廣泛的基準測試研究", "summary_zh": "本研究首次大規模評估視覺語言模型（VLMs）在腹腔鏡手術等內視鏡任務中的能力。我們使用多種先進模型、手術數據集和人工標註，探討VLMs能否勝任手術圖像的基本感知任務和進階的內視鏡場景理解任務，以及專用醫療VLMs與通用模型的比較。結果顯示，VLMs在物體計數和定位等基本任務上表現出色，但處理需要醫學知識的任務時性能顯著下降。令人驚訝的是，專用醫療VLMs的表現不如通用模型，表明它們尚未針對手術環境的複雜性進行優化。這項研究突顯了未來開發內視鏡AI系統的需求，並為改進醫療視覺語言模型指明了方向。", "applications": ["想像一下，未來醫生在做腹腔鏡手術時，AI能即時辨識手術視野中的器官、血管，甚至提醒醫生注意潛在風險，就像有個經驗豐富的助手在旁邊一樣。", "以後醫學院學生可以利用這個AI系統來模擬手術，AI會根據學生的操作給予即時反饋，讓他們在真實手術前就能累積經驗。", "開發一套居家健康監測系統，透過內視鏡影像分析，早期發現腸胃道疾病，讓民眾在家就能進行初步的健康檢查。"], "pitch": "各位投資人，我們正在打造醫療AI的未來！這項技術不僅僅是個研究項目，它將徹底改變外科手術的面貌。想像一下，AI能輔助醫生進行更精準、更安全的手術，降低醫療事故的發生率，並大幅縮短手術時間。更重要的是，我們發現專用醫療模型的表現不如通用模型，這代表著巨大的市場機會！我們將開發針對手術環境優化的VLMs，解決現有模型的瓶頸，打造出真正能夠理解手術場景的AI。這不僅能應用於手術室，還能拓展到遠程醫療、醫學教育等領域，潛在市場規模數十億美元！現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.06232v1.wav"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:33:25.131072", "title_zh": "RecGPT：用於序列推薦的基礎模型", "summary_zh": "RecGPT 是一個突破性的推薦系統，它像大型語言模型一樣，具備跨領域的泛化能力，不需要針對新領域重新訓練。它捨棄了傳統基於ID的方法，改為完全從文字特徵提取商品資訊，讓新商品能立即加入推薦，無需重新訓練模型。RecGPT 使用統一的商品符號化方法，將各種文字描述轉換為標準化的離散符號，消除了領域之間的障礙。此外，它還採用混合雙向因果注意力機制，捕捉商品內部的關聯和商品之間的順序關係。這種方法在六個數據集和工業場景中都展現了優越的性能，為推薦系統帶來了革命性的改變。", "applications": ["**個人化新聞推薦：** 不再只推薦你看過的新聞，而是根據你讀過的文章內容，推薦其他領域但主題相關的新聞，擴展你的知識視野。", "**跨平台商品推薦：** 假設你在A電商平台買了咖啡豆，RecGPT可以立刻在B平台上推薦你適合的咖啡濾杯或磨豆機，即使你在B平台沒有任何購買紀錄。", "**冷啟動影視推薦：** 新上映的冷門獨立電影，即使觀看人數不多，RecGPT也能透過電影簡介的文字內容，推薦給可能感興趣的觀眾，讓小眾佳作也能被發掘。"], "pitch": "各位投資人，想像一下，一個能理解所有商品和使用者喜好的超級推薦引擎，它不需要大量數據訓練，就能精準推薦，這就是RecGPT的潛力！傳統推薦系統就像個別的孤島，RecGPT則是一座連接所有島嶼的橋樑。它不僅解決了冷啟動和跨領域推薦的難題，更開創了全新的商業模式。我們可以將RecGPT授權給各個電商平台、內容平台，甚至線下零售商，讓他們輕鬆實現個性化推薦，提升銷售額和使用者滿意度。更進一步，RecGPT可以應用於智慧城市、智慧醫療等領域，例如根據病患的病歷和生活習慣，推薦個性化的健康管理方案。未來，RecGPT將成為AI推薦領域的領導者，引領下一代推薦技術的發展。現在投資RecGPT，就是投資未來！", "audio": "docs/data/audios/2506.06270v1.wav"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T20:34:50.492212", "title_zh": "擴散模型中的反義噪音", "summary_zh": "本研究深入探討擴散模型中反義初始噪音的特性。我們發現，無論是無條件模型、文本條件潛在擴散模型還是擴散後驗採樣器，將每個初始噪音與其負值配對，都能產生強烈負相關的樣本。我們提出「對稱猜想」，認為模型學習到的分數函數近似為仿射反對稱（奇對稱加上常數偏移）。基於這種負相關性，我們實現了兩個應用：一是提高Stable Diffusion等模型的圖像多樣性，且不損失品質；二是銳化不確定性量化，例如縮小高達90%的置信區間。此外，我們將雙點配對擴展到隨機準蒙地卡羅估計器，進一步提高了估計準確性。此框架無需訓練、適用於各種模型，且不增加運行時開銷。", "applications": ["想像一下，你想要用AI繪圖產生風景照，但每次生成的結果都很類似。使用這項技術，你可以輕鬆產生更多樣化的風景，讓你的照片集更加豐富。", "醫生在分析X光片時，常常需要判斷是否有微小的病灶。這項技術可以幫助醫生更精準地評估診斷結果的不確定性，提供更可靠的醫療建議。", "科學家在模擬氣候變遷時，需要考慮各種不確定因素。這項技術可以幫助他們更準確地預測氣候變遷的影響，為政策制定提供更可靠的依據。"], "pitch": "各位創投，擴散模型是AI領域的明日之星，但其生成結果的多樣性和預測的準確性仍有提升空間。我們的「反義噪音」技術，無需額外訓練成本，就能顯著提高圖像生成的多樣性，並大幅提升不確定性量化的準確性。這意味著，我們可以打造更具創意、更可靠的AI應用。試想一下，將這項技術應用於自動駕駛，可以更精準地預測路況風險；應用於金融市場預測，可以更有效地管理投資組合風險；應用於新藥研發，可以更快速地篩選潛力藥物。這不僅是一項技術突破，更是一個巨大的商業機會。我們相信，透過您的投資，我們可以將這項技術推向更廣闊的應用領域，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.06185v1.wav"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:44:00.677565", "title_zh": "「我們需要善用生成式AI來強化知識傳播」：透過生成式AI素養賦能年長者", "summary_zh": "本研究探討如何提升年長者對生成式AI的素養，讓他們了解其益處與潛在風險。研究採用混合方法，透過名為Litti的聊天機器人，評估其對年長者AI素養（知識、安全和道德使用）的影響。定量數據顯示AI素養有改善趨勢，但未達統計顯著性。質性訪談則揭示年長者對生成式AI的熟悉程度各異，但都渴望學習更多。研究發現Litti提供了正面的學習體驗，但並未顯著提升參與者對生成式AI的信任感或安全感。本研究強調了為快速增長的年長者人口設計AI素養教育的挑戰與機會。", "applications": ["**長輩專屬的AI健康管家：** Litti可以變成一個24小時待命的健康顧問，提醒長輩服藥、提供飲食建議，甚至在緊急情況下聯絡家人或救護車。它能用長輩習慣的語言溝通，讓他們更安心。", "**AI陪伴聊天解悶神器：** 許多長輩獨居，Litti可以陪他們聊天、分享新聞、甚至一起玩簡單的遊戲。它能記住長輩的喜好，提供客製化的內容，減少孤獨感。", "**銀髮族數位學習好幫手：** Litti可以教長輩如何使用智慧型手機、平板電腦，讓他們輕鬆上手網路購物、視訊通話，甚至參與線上課程，享受數位生活的便利。"], "pitch": "各位投資人，高齡化社會是全球趨勢，而生成式AI是賦能銀髮族、提升他們生活品質的關鍵技術。想像一下，一個由AI驅動的銀髮族生態系，包含個人化的健康管理、社交互動、數位學習等服務，市場潛力無窮！我們的Litti聊天機器人正是這個生態系的起點。它不僅能提升長輩的AI素養，更能成為他們信任的數位夥伴。我們計劃將Litti整合到各種銀髮族產品和服務中，例如智慧居家設備、遠距醫療平台等，打造一個龐大的銀髮族AI市場。現在投資我們，您將搶佔先機，共同開創銀髮經濟的下一個藍海！未來，我們甚至可以將Litti發展成具有情感理解能力的AI，真正成為長輩們的心靈伴侶，這將是劃時代的創新！", "audio": "docs/data/audios/2506.06225v1.wav"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:45:24.929005", "title_zh": "謎題世界：謎題狩獵中多模態、開放式推理的基準測試", "summary_zh": "「謎題狩獵」是一種複雜、多步驟的謎題類型，缺乏明確的問題定義。PuzzleWorld是一個大型基準測試，包含667個謎題狩獵風格的問題，旨在評估逐步、開放式和創造性的多模態推理。現有模型在最終答案的準確率上僅達到1-2%，最佳模型也僅解決了14%的謎題。研究顯示，模型在推理過程中存在短視近利的問題，並受限於基於語言的推論能力，且缺乏視覺和空間推理所需的草圖能力。此基準測試將有助於開發更通用、開放式和創造性的推理系統，可用於科學發現、數據分析和調查性問題解決等領域。", "applications": ["設計逃脫遊戲：PuzzleWorld可以幫助遊戲設計師創建更具挑戰性、更有趣的逃脫遊戲，透過AI自動生成謎題和線索，讓玩家有更好的遊戲體驗。", "輔助兒童教育：將PuzzleWorld應用於兒童教育，可以開發出更具互動性的學習工具，培養孩子的邏輯思維、空間推理和創造力，讓學習過程更加生動有趣。", "提升企業問題解決能力：企業可以利用PuzzleWorld來訓練員工的解決問題能力，透過模擬真實世界的複雜情境，提升團隊合作和創新思維。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻，生成式AI正快速改變世界。然而，現有的AI模型在處理需要多模態推理、開放式問題解決的複雜任務時，能力仍遠遠不足。PuzzleWorld的出現，正是為了填補這一空白。它不僅是一個基準測試，更是一個孕育新一代AI的搖籃。想像一下，未來的AI不僅能理解語言，還能看懂圖像、理解空間關係，甚至能像人類一樣進行創造性思考。這種AI將在科學研究、金融分析、甚至藝術創作等領域產生顛覆性的影響。我們相信，透過PuzzleWorld的持續發展，我們能打造出真正具有通用智能的AI，開創一個充滿無限可能的未來。現在投資PuzzleWorld，就是投資AI的未來，您將成為這場技術革命的先驅！", "audio": "docs/data/audios/2506.06211v1.wav"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "timestamp": "2025-06-09T13:47:05.009526", "title_zh": "擴散模型的反饋引導", "summary_zh": "現行的無分類器引導(CFG)雖然能提升條件式擴散模型的生成品質，但恆定的引導可能損害多樣性並導致記憶化。我們提出反饋引導(FBG)，它使用一個狀態相關係數，根據需求自我調節引導量。FBG基於第一原理推導，假設學習到的條件分佈被無條件分佈線性破壞。FBG利用自身對條件訊號資訊量的預測反饋，在推論過程中動態調整引導，挑戰了引導作為固定超參數的觀點。在ImageNet512x512基準測試中，FBG顯著優於CFG，並與LIG競爭，同時受益於強大的數學框架。在文本到圖像生成中，FBG能針對複雜提示自動應用更高的引導尺度，且易於與現有引導方案(如CFG或LIG)結合。", "applications": ["想像一下，你想要AI幫你畫一張生日派對的邀請函，但你只給了很簡單的描述，像是「生日快樂」。傳統的AI可能會畫出很普通的派對畫面。但用了反饋引導，AI會自動判斷這個提示太簡單，需要加強引導，於是它會加入更多細節，像是氣球、蛋糕、禮物等等，讓邀請函更豐富。", "假設你是服裝設計師，想用AI生成一些新的設計稿。你輸入一個比較模糊的概念，像是「未來感外套」。用了反饋引導的AI，會根據這個概念的複雜度，自動調整生成過程，確保生成的外套設計既有未來感，又不會過於抽象或難以理解，讓設計師能更容易的激發靈感。", "如果你在玩AI繪圖，想要生成一張特定風格的圖片，例如「梵谷風格的貓」。如果提示不夠明確，AI可能會畫出很普通的貓。但有了反饋引導，AI會自動加強梵谷風格的元素，像是用色、筆觸等等，讓生成的貓咪圖片更具藝術感，更符合你的期望。"], "pitch": "各位投資人，我們帶來的是擴散模型領域的革命性技術——反饋引導(FBG)。現有的生成式AI，如DALL-E、Midjourney等，都依賴於人工設定的引導參數，這不僅耗時，也限制了AI的創造力。FBG技術顛覆了這一模式，它讓AI能夠像一位經驗豐富的藝術家一樣，根據創作內容的複雜程度，自動調整引導的力度，從而生成更高品質、更具創意、更符合使用者需求的圖像。想像一下，未來，設計師、藝術家、甚至是普通使用者，都能夠輕鬆地利用AI創造出獨一無二的作品，而無需具備專業的AI知識。這將開啟一個全新的創意經濟時代，市場規模將是數百億美元級別的。更重要的是，FBG技術不僅僅局限於圖像生成，它還可以應用於音訊、影片、甚至3D模型的生成，潛力無限。我們相信，FBG技術將成為下一代生成式AI的核心引擎，而我們團隊將引領這場技術革命。現在加入我們，共同打造AI驅動的未來，您將獲得豐厚的回報！", "audio": "docs/data/audios/2506.06085v1.wav"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:29:11.968645", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺回饋", "summary_zh": "現有的視覺語言模型在文字到圖像檢索方面表現出色，但實際應用仍存在挑戰。GenIR針對「心理圖像檢索」任務，讓使用者能透過多輪互動，逐步逼近腦海中的圖像。GenIR的核心是利用擴散模型生成圖像，將AI系統的理解視覺化呈現，提供清晰且可操作的回饋。使用者能根據這些視覺回饋，更直觀有效地調整檢索條件。我們還建立了全自動流程，生成高品質的多輪心理圖像檢索數據集。實驗結果顯示，GenIR顯著優於現有的互動式方法，為未來研究奠定了基礎。", "applications": ["想像一下，你忘記了小時候最喜歡的玩具長什麼樣子，但還記得一些模糊的特徵。透過GenIR，你可以描述這些特徵，系統會生成可能的圖像，讓你逐步縮小範圍，最終找到你心心念念的玩具。", "假設你想在家裡重新裝潢，但腦海中只有一些零碎的想法。你可以用GenIR描述你想要的風格、顏色和家具，系統會生成不同的房間設計，幫助你找到最喜歡的方案，省去尋找靈感的時間。", "如果你正在尋找失散多年的親人，但只有一些模糊的記憶，例如臉部特徵或衣著風格。GenIR可以根據你的描述生成可能的圖像，幫助你擴大搜索範圍，增加找到親人的機會。"], "pitch": "各位投資人，我們相信GenIR將徹底改變圖像檢索的未來！現今的圖像檢索技術往往無法滿足人們腦海中模糊的需求。GenIR透過生成式視覺回饋，讓人機互動更加直觀高效，解決了這個痛點。想像一下，未來的電商平台，使用者只需描述想要的商品，AI就能生成商品圖像，甚至可以根據使用者的喜好客製化設計。在醫療領域，醫生可以透過GenIR，根據患者的描述生成病灶圖像，輔助診斷。在安全領域，警方可以根據目擊者的描述，生成嫌疑犯的模擬圖像，提高破案率。GenIR的應用場景無限廣闊，市場潛力巨大。我們已經建立了一個高品質的數據集，並開發了領先的生成式檢索方法。我們正在尋找有遠見的投資人，一起將GenIR推向市場，引領下一代圖像檢索革命！", "audio": "docs/data/audios/2506.06220v1.wav"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:30:20.416876", "title_zh": "文字到LoRA：即時轉換器適應", "summary_zh": "本研究提出Text-to-LoRA (T2L)模型，能根據自然語言描述，即時調整大型語言模型以適應特定任務。T2L是一種超網路，只需一次前向傳遞就能建構LoRA。經過九個預訓練LoRA適配器訓練後，T2L重建的LoRA實例在對應測試集上表現與特定任務適配器相當。更重要的是，T2L能壓縮數百個LoRA實例，並零樣本泛化到全新任務。這項技術大幅降低了基礎模型專業化的門檻，並以極少的計算資源實現基於語言的適應，讓AI模型客製化變得更快速、更普及。", "applications": ["AI繪圖客製化：使用者只要用文字描述想要的圖片風格(例如：水墨畫、卡通風格)，AI就能快速調整模型，產生符合需求的圖片。", "個人化AI助理：針對不同使用者的需求，例如：商業寫作、程式碼除錯等，AI助理能根據文字指令即時調整模型，提供更精準的協助。", "遊戲AI角色客製化：遊戲開發者可以透過文字描述，快速調整AI角色的行為模式或對話風格，讓遊戲體驗更加豐富。"], "pitch": "各位投資人，想像一下，未來每個人都能輕鬆客製化AI模型，就像調整手機App一樣簡單！Text-to-LoRA技術，正是實現這個願景的關鍵。它能讓AI模型根據文字指令即時調整，無需耗時費力的重新訓練。這意味著，我們能以極低的成本，打造出無數個針對特定領域或個人需求的AI應用。從AI繪圖、個人化助理到遊戲AI，市場潛力無限。更重要的是，T2L技術還能壓縮模型，讓AI應用在各種裝置上都能流暢運行。我們相信，Text-to-LoRA將徹底顛覆AI產業，成為新一代AI應用的基礎設施。現在加入我們，一起打造AI客製化的未來！", "audio": "docs/data/audios/2506.06105v1.wav"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "timestamp": "2025-06-09T15:32:05.943203", "title_zh": "Restereo：擴散立體影片生成與修復", "summary_zh": "本研究提出一個新穎的立體影片生成流程，不僅能從單眼2D影片生成3D立體影片，還能同時增強左右視角的影片品質。此方法透過在降質數據上微調模型進行修復，並以扭曲遮罩為條件進行一致的立體生成。因此，即使在相對較小的合成立體影片數據集上進行微調，也能應用於低品質的真實世界影片，同時實現立體影片的生成和修復。實驗結果表明，本方法在低解析度輸入的立體影片生成方面，在品質和數量上均優於現有方法。", "applications": ["在家用VR觀影時，即使影片來源畫質不佳，也能透過此技術即時提升畫質並轉換為立體3D，享受更沉浸式的觀影體驗。", "老舊照片或影片的數位修復：將舊照片或影片轉換為立體影像，讓回憶更加生動，並修復畫質，讓珍貴的影像資料得以保存。", "線上遊戲體驗優化：即時將2D遊戲畫面轉換為3D立體畫面，提升遊戲沉浸感，並修復遊戲畫面中可能存在的模糊或失真問題。"], "pitch": "各位創投先進，我們帶來的是Restereo，一項劃時代的立體影片生成與修復技術。想像一下，現今VR/AR內容的最大瓶頸是什麼？是高品質3D內容的匱乏！Restereo能將任何2D影片，甚至是低畫質的老舊影片，即時轉換為令人驚豔的3D立體影像，並同步提升畫質。這代表什麼？龐大的內容創作潛力！從個人用戶到大型影視公司，都能輕易創造出引人入勝的VR/AR體驗。更重要的是，我們能賦予歷史影像新的生命力，將塵封的記憶以更真實、更立體的方式呈現。未來，Restereo將成為元宇宙內容生態的基石，我們不只是在修復影片，我們是在打造一個全新的視覺世界！現在投資Restereo，就是投資元宇宙的未來！", "audio": "docs/data/audios/2506.06023v1.wav"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:35:20.869608", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型(LLM)在各領域快速發展，但在理論物理研究中的應用尚不成熟。本文認為，若將LLM智能體與領域知識和工具箱適當結合，有潛力加速理論、計算和應用物理學的發展。我們分析了LLM目前在物理學方面的能力，包括數學推理和程式碼生成，並指出了在物理直覺、約束滿足和可靠推理方面的關鍵差距。我們設想未來專門用於物理學的LLM能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要應對根本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧社群共同努力，以幫助推進物理學的科學發現。", "applications": ["**智慧教材：** LLM能根據學生的學習進度和理解程度，客製化物理教材和練習題，就像一位24小時隨時待命的私人物理家教。", "**科學玩具：** LLM可以嵌入到玩具中，讓孩子在玩樂中學習物理知識，例如，一個能回答物理問題的積木或一個能模擬物理現象的遊戲。", "**故障排除：** LLM可以協助工程師快速診斷複雜系統的故障，例如，分析感測器數據，找出發電廠或飛機引擎的潛在問題。"], "pitch": "各位投資人，我們正處於AI與物理學交匯的革命性時刻！想像一下，一個能自主設計實驗、推導新理論的AI科學家，這不再是科幻小說。我們的團隊正在開發專為物理學打造的LLM智能體，它能處理複雜的物理數據，提出創新的解決方案，並加速科學發現的進程。這項技術的潛在商業價值難以估量，從新材料的發現到能源效率的突破，再到太空探索的加速，都將受益於此。我們預計，未來物理學LLM將成為科研機構、工程公司和政府部門不可或缺的工具。現在投資我們，您將站在這場科學革命的最前沿，共同塑造未來！", "audio": "docs/data/audios/2506.06214v1.wav"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:36:57.027212", "title_zh": "醫學視覺語言模型之完全適形調整", "summary_zh": "大型預訓練的視覺語言模型（VLMs）在醫學影像分析中展現了前所未有的遷移能力。然而，其可靠性卻被忽略。本研究探討了在split conformal prediction (SCP)框架下VLMs的行為，該框架藉由標記的校準集，在輸出集上保證給定的錯誤水平。為了解決VLMs的zero-shot性能限制以及few-shot遷移學習管道無法滿足SCP的嚴格可交換性假設的問題，我們提出了完全適形調整，這是一種新穎的設定，用於聯合調整和適形預訓練的基礎模型，並使用few-shot調整集對每個測試數據點進行轉導操作。此外，我們使用SS-Text來補充這個框架，這是一種用於VLMs的免訓練線性探測求解器，可減輕這種轉導方法的計算成本。實驗結果表明，我們的框架在保持相同覆蓋率保證的同時，在集合效率上提供了高達27%的相對改進。", "applications": ["**遠距醫療影像判讀：** 想像一下，偏鄉地區的醫生可以透過手機App，將X光片上傳，AI就能快速提供初步診斷結果，協助醫生做出更精確的判斷，提升醫療效率。", "**個人化健康管理：** 未來，我們可以將自己的醫療影像，例如心電圖、眼底照片等，上傳到一個安全平台，AI會分析這些數據，並提供個人化的健康建議，例如飲食調整、運動計畫等。", "**新藥開發加速：** 藥廠可以利用這項技術，快速分析大量的醫學影像資料，找出潛在的藥物靶點，加速新藥開發的進程，讓更多疾病得到及時治療。"], "pitch": "各位投資人，我們帶來的是醫學影像AI的革命性突破！傳統AI在醫學影像判讀上，準確度參差不齊，醫生往往不敢完全信任。我們的「完全適形調整」技術，能讓AI在判讀醫學影像時，不僅給出結果，還能提供信賴度評估，讓醫生更安心。想像一下，這項技術能大幅降低誤診率，提升醫療品質，減少醫療糾紛。更重要的是，它能解放醫生的時間，讓他們能更專注於病人護理。市場潛力巨大！從遠距醫療、個人化健康管理，到新藥開發，都有廣闊的應用前景。我們預期，在未來五年內，這項技術將成為醫學影像AI的產業標準，帶領我們在精準醫療時代搶佔先機。現在加入我們，您將成為這場醫療革命的領航者！", "audio": "docs/data/audios/2506.06076v1.wav"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "timestamp": "2025-06-09T18:38:37.267711", "title_zh": "基於再生擴散模型之免優化通用浮水印偽造", "summary_zh": "浮水印技術被廣泛應用於追蹤和驗證AI生成圖像的來源，但存在偽造風險。本研究揭示了一種更嚴重的免優化通用浮水印偽造方法，利用現有的再生擴散模型，名為PnP（Plug-and-Plant）。PnP無需額外優化，即可透過圖像再生無縫提取和整合目標浮水印。此方法獨立於目標圖像的來源或浮水印模型，實現通用浮水印偽造。實驗證明，PnP在多種情境下成功偽造浮水印，同時保持最佳視覺效果。這種繞過模型重新訓練並適應任何圖像的能力，擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全性和合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "applications": ["情境一：假設你是一位藝術家，想保護你的AI生成作品不被盜用。但有人利用這項技術，將你的浮水印複製到其他圖像上，讓你難以證明原創性，甚至可能被誤認為抄襲者。", "情境二：新聞媒體使用AI生成圖片來輔助報導。如果有人惡意將浮水印偽造到假新聞圖片上，並嫁禍給該媒體，可能嚴重損害其聲譽和公信力。", "情境三：在學術界，研究人員發表基於AI生成數據的論文。如果他人偽造浮水印，聲稱該數據來自不同的來源，可能導致學術欺詐和錯誤的研究結論。"], "pitch": "各位創投朋友們，想像一下，AI生成的內容正以前所未有的速度爆發，但信任危機也隨之而來。我們的技術揭示了現有浮水印系統的重大漏洞，同時也帶來了巨大的商機！PnP技術不僅能檢測偽造的浮水印，更能進一步開發出更強大、更安全的浮水印系統，保護原創內容，維護數據的真實性。未來，我們可以將這項技術應用於數位版權管理、內容溯源、甚至金融安全等領域。試想一下，每一張AI生成的圖片、每一份重要的數據報告，都擁有一個無法偽造的數位身份證，這將徹底改變我們對數位內容的信任方式。現在投資我們，您將站在AI安全的最前沿，共同打造一個更值得信賴的AI未來！", "audio": "docs/data/audios/2506.06018v1.wav"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:25:05.955123", "title_zh": "鎖定假說：演算法造成的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署，會與使用者形成一種回饋迴路：模型從數據中學習人類的信念，透過生成內容強化這些信念，再吸收這些被強化的信念，然後反覆地回饋給使用者。這種動態類似於同溫層效應。我們假設這種回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性的喪失，並可能鎖定錯誤的信念。我們透過基於代理的LLM模擬和真實世界的GPT使用數據，對此假設進行了形式化並進行了實證檢驗。分析顯示，在新的GPT版本發布後，多樣性出現了突然但持續的下降，這與假設的人機回饋迴路一致。", "applications": ["新聞App總是推播你喜歡的新聞，讓你覺得世界就是你想的那樣，忽略了其他不同的聲音，長期下來，你可能變得更偏激。", "社群媒體的演算法只推薦你追蹤與你意見相似的人，讓你越來越難接觸到不同的觀點，導致同溫層效應越來越嚴重。", "孩子使用AI學習工具，但AI只根據過去的資料生成答案，可能讓孩子學到過時或有偏見的知識，阻礙他們的創新能力。"], "pitch": "各位創投先進，我們正處於AI革命的關鍵時刻，但一個潛在的危機正在浮現：AI正在將我們鎖死在過去的認知中。想像一下，如果未來的AI只能重複過去的觀點，創新將停滯，社會將分裂。我們的研究揭示了這個『鎖定假說』，並提供了應對方案。我們正在開發一種『AI多樣性引擎』，它能主動引入不同的觀點，打破同溫層效應，確保AI成為促進進步的力量，而不是阻礙。這不僅是一項技術，更是一項社會責任。投資我們，就是投資一個更開放、更具創新力的未來。我們預期在三年內，這項技術將成為所有大型語言模型的標準配置，並在教育、媒體、政策制定等領域產生深遠影響。未來的AI，不應該只是過去的鏡子，而應該是通往新世界的窗戶。加入我們，一起開啟這扇窗！", "audio": "docs/data/audios/2506.06166v1.wav"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:26:30.464573", "title_zh": "從多模態基礎模型中的動力學模型引導世界模型", "summary_zh": "本研究探討視覺與語言基礎模型是否具備真實的世界模型（觀察×行動→觀察）和動力學模型（觀察×觀察→行動）。研究發現，微調模型以獲得動力學模型比獲得世界模型更容易。進而，動力學模型可以透過合成數據的弱監督學習和推理時驗證來引導世界模型。首先，動力學模型可以為未標記的影片幀觀察對添加行動標籤，擴展訓練數據。其次，動力學模型可以為世界模型的多個樣本分配獎勵，對其進行評分，從而在推理時有效地引導搜尋。實驗結果顯示，該模型在Aurora-Bench上進行以行動為中心的圖像編輯任務時，性能與最先進的圖像編輯模型相媲美，在真實世界子集上的表現提高了15%。", "applications": ["想像一下，你可以用一句話，例如「把房間變成充滿陽光的沙灘」，然後這個AI就能自動幫你修改照片，讓你的房間看起來就像真的在沙灘上！這就像擁有了魔法PS高手。", "以後玩遊戲，AI能更聰明地理解你的指令。例如，你說「跳到最高的平台上」，AI就能預測你的角色需要如何移動和跳躍，讓遊戲體驗更流暢、更真實。", "在製造業，我們可以透過AI預測機器在不同操作下的反應。例如，輸入「提高機器速度」，AI就能預測機器零件的磨損情況，提前預防故障，降低維修成本。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓機器像人類一樣理解世界，並根據指令改變現實！我們的核心突破在於，我們發現了從動力學模型引導世界模型的有效方法，這讓AI能更準確地預測行動的後果。這項技術的潛力無窮，從圖像編輯、遊戲開發到工業自動化，都能帶來顛覆性的變革。想像一下，一個能根據你的想法創造圖像、控制機器人的AI，這將是一個數十億美元的市場！我們已經在Aurora-Bench基準測試中取得了令人矚目的成果，超越了現有的圖像編輯模型。現在，我們需要您的資金，將這項技術推向市場，成為AI領域的領導者。投資我們，就是投資未來！未來，每個人都可以是創作者，都可以用簡單的語言改變世界！", "audio": "docs/data/audios/2506.06006v1.wav"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "timestamp": "2025-06-09T21:27:56.686311", "title_zh": "AQUATIC-Diff：適用於極小壓縮擴散模型的加法量化", "summary_zh": "本研究針對擴散模型在硬體資源上的高需求問題，提出了一種名為AQUATIC-Diff的加法向量量化方法。不同於以往常用的均勻標量量化，此方法基於碼本，能更有效地壓縮模型，在極低位元量化下達到新的效能巔峰。在ImageNet的LDM-4基準測試中，W4A8設定下sFID值比全精度模型低1.92點，W2A8設定下FID、sFID和ISC指標均達到最佳。更重要的是，我們開發了高效的推論核心，能在各種硬體上實現FLOPs節省，擺脫了對特定硬體支援小整數運算的依賴。", "applications": ["**手機攝影美化：** 將這項技術應用於手機App中，即使是低階手機也能快速生成高品質、風格獨特的照片，讓每個人都能輕鬆成為攝影大師。", "**遊戲角色生成：** 遊戲開發者可以利用這項技術，快速生成大量獨一無二的遊戲角色，節省美術設計時間，並提供玩家更多樣化的選擇。", "**AI藝術創作：** 藝術家可以使用這項技術，在資源有限的設備上進行AI藝術創作，激發無限創意，並將藝術帶入更多人的生活。"], "pitch": "各位創投先進，我們正站在AI圖像生成革命的浪潮之巔！AQUATIC-Diff技術，如同為擴散模型裝上了火箭推進器，使其能在極低的硬體資源下運行，打破了過往高算力需求的瓶頸。想像一下，未來每一台手機都能運行複雜的AI圖像生成模型，人人都能隨時隨地創造獨一無二的內容。這不僅僅是技術突破，更是商業模式的巨大變革！我們可以將此技術授權給手機廠商、遊戲公司、甚至是元宇宙平台，收取授權費用；或者開發基於AQUATIC-Diff的雲端服務，提供更高效、更低成本的AI圖像生成解決方案。隨著元宇宙、NFT等領域的蓬勃發展，對AI圖像生成的需求將呈指數級增長，AQUATIC-Diff必將成為這場盛宴中最耀眼的明星，為各位帶來豐厚的回報！現在投資AQUATIC-Diff，就是投資AI圖像生成的未來！", "audio": "docs/data/audios/2506.05960v1.wav"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:53:33.383616", "title_zh": "夢境樂園：結合模擬器與生成模型的可控世界創造", "summary_zh": "本研究提出「夢境樂園」，一個結合物理模擬器和生成模型的混合世界生成框架。它利用分層世界抽象，將像素級和物件級的語義與幾何資訊編碼為中間表示，連接模擬器和生成模型。這增強了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有和未來預訓練生成模型的直接使用。我們構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗表明，「夢境樂園」在圖像質量上提升了50.8%，可控性增強了17.9%，並具有增強具身智能體訓練的巨大潛力。", "applications": ["遊戲開發者可以利用這項技術快速創建多樣且逼真的遊戲世界，並精確控制場景中的元素，例如調整物體的物理特性或改變環境光照，讓遊戲體驗更豐富。", "建築師和設計師可以創建虛擬的建築模型，並模擬不同天氣或光照條件下的效果，讓客戶在實際建造前就能身歷其境地體驗設計方案。", "電影製作人可以使用這項技術製作特效場景，例如創建逼真的自然災害或科幻世界，並精確控制場景中的每個細節，降低製作成本並提高效率。"], "pitch": "想像一下，我們正站在一個無限可能的起點。Dreamland不僅僅是一個技術突破，它是一個通往全新現實的鑰匙。它將徹底改變遊戲、娛樂、設計乃至AI訓練的未來。我們的混合框架，結合了物理模擬的精確控制與生成模型的逼真渲染，創造出前所未有的可控虛擬世界。這意味著更高效的遊戲開發、更具沉浸感的虛擬體驗，以及更強大的AI智能體。D3Sim數據集是我們的獨家優勢，能加速AI學習並提升性能。市場潜力巨大：遊戲產業對逼真場景的需求、建築設計對可視化效果的追求、AI訓練對大量數據的渴求，都將推動Dreamland的快速成長。我們正在打造的不僅是一個產品，而是一個平台，一個生態系統，一個全新的現實。現在加入我們，一起塑造這個未來，共享這份巨大的商業價值！", "audio": "docs/data/audios/2506.08006v1.wav"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:54:57.509488", "title_zh": "梯度：當市場遇上微調——一種模型優化的分散式方法", "summary_zh": "現有自動機器學習平台在微調大型模型時，往往受限於單一優化策略，無法充分探索所有可能的超參數組合。Gradients平台將超參數優化轉變為一個去中心化的競爭市場，讓獨立的「礦工」競相尋找最佳配置。經濟誘因驅動個人探索，並將其與集體優化目標對齊，從而系統性地挖掘中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型中，相較於其他平台，平均提升了11.8%至42.1%的性能，尤其在複雜推理和檢索任務以及個人化生成方面表現出色。這證明了基於經濟驅動的競爭方法，能有效發現卓越的配置。", "applications": ["想像一下，你是一位行銷人員，想為你的產品創建最吸引人的廣告文案。Gradients就像一個超級優化的廣告文案產生器，能自動找到最有效的詞語和風格，讓你的廣告點擊率飆升。", "如果你是一位醫生，想利用AI診斷罕見疾病。Gradients可以幫助你快速微調AI模型，使其能更準確地識別出疾病的細微特徵，提高診斷的準確性。", "假設你是一位遊戲開發者，想創造一個能根據玩家喜好自動調整難度的遊戲。Gradients可以幫助你優化遊戲AI，讓每個玩家都能享受到獨一無二、高度個人化的遊戲體驗。"], "pitch": "各位投資人，我們相信Gradients將徹底改變AI模型的微調方式。現今，微調過程耗時且昂貴，如同大海撈針。Gradients透過去中心化的市場機制，將這個過程轉變為高效、經濟的競賽。想像一下，一個能自我優化的AI生態系統，就像AI界的App Store，每天都在產生更強大、更精準的模型。這不僅能節省數百萬美元的成本，更能加速AI在各行各業的應用。我們預見，Gradients將成為AI基礎設施的關鍵組成部分，為各行各業提供更強大、更個人化的AI解決方案。投資Gradients，就是投資AI的未來，一個充滿無限可能的未來！", "audio": "docs/data/audios/2506.07940v1.wav"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "timestamp": "2025-06-10T03:56:34.805569", "title_zh": "StableMTL：利用潛在擴散模型，從部分標註的合成數據集中進行多任務學習", "summary_zh": "這項研究提出StableMTL方法，利用擴散模型強大的泛化能力，在只有部分標註的合成數據集上訓練多任務模型，實現零樣本學習。StableMTL將圖像生成器用於潛在回歸，通過任務編碼、逐任務條件化和定制的訓練方案來調整去噪框架。它採用統一的潛在損失，無需仔細平衡各任務的損失，從而實現無縫擴展到更多任務。此外，引入了多流模型和任務注意力機制，將任務間的交互轉化為高效的單向注意力，促進跨任務共享。實驗證明，StableMTL在多個基準測試中優於其他方法。", "applications": ["智慧城市：利用路口監視器畫面，同時辨識車流量、行人數量、違規停車等，提升交通管理效率，並減少人力成本。", "醫療影像分析：從X光片或斷層掃描中，同時檢測多種疾病徵兆，例如腫瘤大小、骨折位置、炎症反應等，輔助醫生進行更精確的診斷。", "電商平台：自動分析商品圖片，同時提取商品屬性（顏色、材質、款式）和場景信息（室內、戶外），提升商品分類和搜尋的準確性，改善使用者體驗。"], "pitch": "想像一下，我們能用AI同時處理多項任務，而且只需要少量標註數據甚至完全不需要！StableMTL就是實現這個願景的關鍵。它像一個超級AI訓練師，能從各種模擬數據中學習，並將知識應用到真實世界。這不僅能大幅降低AI開發成本，還能開啟無限可能。例如，在自動駕駛領域，我們可以同時訓練AI識別交通號誌、行人、障礙物，大幅提升安全性。在醫療診斷方面，AI能同時分析多種病徵，協助醫生做出更精準的判斷。這項技術的潛在市場價值數十億美元，現在投資，就能搶佔AI多任務學習的先機，成為下一個AI獨角獸！", "audio": "docs/data/audios/2506.08013v1.wav"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:37:06.451208", "title_zh": "利用多代理人對話式AI系統支持建築工人的福祉", "summary_zh": "建築業面臨高 शारीरिक 與心理風險，但心理健康支持有限。本研究開發了一套多代理人對話式AI系統，結合領域知識，解決建築業的特定挑戰。系統透過與不同人格的代理人互動，滿足工人基本的心理需求，提供實際問題解決方案與社交互動，從而提升整體福祉。實驗結果顯示，相較於單一代理人系統，我們的系統在可用性、自主性、社交臨場感與信任度方面分別提升了18%、40%、60%與60%。這證明了大型語言模型驅動的AI系統在為建築工人提供領域特定支持方面的潛力。", "applications": ["工地裡，工人阿明心情不好，可以跟AI心理諮詢師聊聊，排解壓力，AI還能提醒他注意安全，避免工傷。", "老王是個水電工，遇到複雜的管線問題，可以問AI專家，AI會一步一步教他怎麼解決，省去查資料的時間。", "新來的工頭小李，對很多建材和工法不熟悉，可以隨時問AI老師，AI會提供相關知識和案例，幫助他快速上手。"], "pitch": "各位投資人，建築業長期面臨人力短缺、工安意外頻傳等問題，而我們的多代理人對話式AI系統，正是解決這些痛點的關鍵。它不僅能提升工人的心理健康與工作效率，更能降低工安事故的發生。想像一下，未來每個工地都配備這樣一套AI系統，它就像一位隨時待命的超級顧問，為工人提供全方位的支持。這將大幅提升建築業的生產力與安全性，創造巨大的商業價值。我們預計，這項技術將能應用於其他高風險行業，例如礦業、製造業等，市場潛力無限。現在投資我們，您將成為引領建築業AI革命的先驅！", "audio": "docs/data/audios/2506.07997v1.wav"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:38:16.837702", "title_zh": "EgoM2P：以自我為中心的視角進行多模態多任務預訓練", "summary_zh": "本研究提出EgoM2P框架，旨在解決以自我為中心的視角下，如何有效理解多模態訊號的挑戰。EgoM2P利用時序感知的多模態tokens，透過遮蔽建模學習，訓練出一個通用的4D理解模型。此模型支援多種任務，包括眼球追蹤、以自我為中心的相機追蹤，以及從單眼視訊進行深度估計，甚至可以生成條件式的以自我為中心的視訊。EgoM2P在多項任務上達到或超越了專用模型的效果，且速度更快。我們將開源EgoM2P，以促進以自我為中心的視覺研究。", "applications": ["導航輔助：想像一下，戴上AR眼鏡，系統能根據你的視線和頭部動作，預測你的意圖，並在視野中即時顯示導航資訊，再也不用低頭看手機了。", "運動訓練：運動員佩戴設備後，系統能分析他們的動作、視線焦點和身體姿態，提供個人化的訓練建議，幫助他們提升表現，例如高爾夫揮桿或籃球投籃。", "遠端協作：工程師可以戴上頭戴式裝置，讓遠端的專家看到他們所看到的，並透過即時的視線追蹤和手勢識別，進行更有效的遠端指導和協作，減少錯誤和提高效率。"], "pitch": "各位投資人，我們正站在AIoT革命的風口浪尖！EgoM2P不僅僅是一個模型，它是一把解鎖未來人機互動的鑰匙。試想，透過我們的技術，AR/VR設備將變得更加智慧、更具沉浸感；機器人將能更精準地理解人類意圖，協作更加順暢；醫療領域，醫生可以透過AR眼鏡進行遠端手術指導，提升醫療水平。EgoM2P的潛力遠不止於此，它將成為元宇宙、智慧工廠、無人駕駛等領域的核心技術。我們正在打造一個全新的互動模式，一個以人為本的智慧世界。現在加入我們，共同開創這個千億級市場！", "audio": "docs/data/audios/2506.07886v1.wav"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "timestamp": "2025-06-10T06:39:36.544597", "title_zh": "自我強制：彌合自迴歸影片擴散中的訓練-測試差距", "summary_zh": "本研究提出「自我強制」訓練方法，解決自迴歸影片擴散模型中長期存在的暴露偏差問題。傳統模型在訓練時依賴真實資料，但在實際應用時卻需根據自身產生的不完美結果生成影片。自我強制透過在訓練期間使用關鍵值（KV）快取進行自迴歸展開，讓模型根據先前自身生成的輸出產生每一幀，從而在影片層級進行整體性監督，直接評估整個生成序列的品質。此外，透過幾步擴散模型和隨機梯度截斷策略，兼顧計算成本和效能。實驗證明，此方法能在單一GPU上實現亞秒級延遲的即時串流影片生成，生成品質甚至超越速度較慢的非因果擴散模型。", "applications": ["想像一下，未來的線上遊戲！遊戲畫面不用事先算好，而是根據你的遊玩方式即時生成，每次玩都有獨一無二的體驗，就像真的身歷其境。", "假設你是個室內設計師，想讓客戶更快看到設計成果。現在只要輸入簡單的描述，就能即時生成不同風格的3D室內設計影片，快速溝通想法，大幅提升效率。", "如果醫院想用AI訓練醫生進行手術模擬，過去需要大量資源建立模型。現在利用這項技術，可以即時生成各種手術場景，讓醫生在逼真的環境下練習，提升手術成功率。"], "pitch": "各位投資人，我們正處於影片生成技術的革命性轉捩點！「自我強制」技術不僅解決了現有模型的瓶頸，更開創了即時、高品質影片生成的全新可能性。想像一下，未來影音內容的生產成本將大幅降低，個人化的互動式影片體驗將無處不在。從遊戲、娛樂、教育到醫療，各行各業都將因此受益。我們的技術擁有極高的商業價值，未來將能授權給各大影音平台、遊戲公司、教育機構，甚至能應用於元宇宙的內容生成。我們預計未來五年內，影片生成市場規模將達到數百億美元，而「自我強制」技術將在這個市場中佔據領先地位，為各位投資人帶來豐厚的回報！現在加入我們，一起打造影片生成的未來！", "audio": "docs/data/audios/2506.08009v1.wav"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:33:25.043974", "title_zh": "τ²-Bench：在雙重控制環境中評估對話式代理", "summary_zh": "現有對話式AI代理的評估基準多為單一控制環境，僅AI代理能使用工具與世界互動，使用者被動提供資訊。本研究提出τ²-Bench，模擬電信領域的雙重控制環境，代理和使用者皆可使用工具在共享動態環境中操作，考驗代理的協調與溝通能力。τ²-Bench包含可程式化任務生成器，能創造多樣化、可驗證的任務，並具備與環境緊密結合的使用者模擬器，提高模擬真實度。實驗顯示，代理在雙重控制環境下的表現明顯下降，突顯了引導使用者的挑戰。τ²-Bench為測試代理的推理能力和引導使用者行為的能力提供了一個可控的平台。", "applications": ["想像一下，未來在家裡設定網路，不再需要看著複雜的說明書。你可以直接跟AI客服對話，AI會一步步引導你操作數據機和路由器，就像朋友在旁邊教你一樣。", "醫院的AI掛號系統，不只幫你預約，還會根據你的症狀，引導你填寫正確的病歷資料，甚至教你如何在家量血壓、準備看診需要的資料，讓你看病更有效率。", "汽車導航不只告訴你怎麼走，還能在你開車遇到問題時，像爆胎了，AI會一步步引導你更換輪胎，確保安全。"], "pitch": "各位投資人，我們正在打造下一代的AI互動模式！傳統AI只能單向提供資訊，但我們的τ²-Bench技術，讓AI能像一位優秀的協作夥伴，與使用者共同完成任務。想像一下，未來的客服中心，AI不再只是回答問題，而是能引導客戶解決複雜的技術問題，大幅降低人力成本，提高客戶滿意度。這項技術的應用範圍極廣，從智慧家庭、遠程醫療到工業自動化，都蘊藏著巨大的商業潛力。我們正在申請專利，並積極尋找合作夥伴，共同開創這個全新的AI市場。現在加入我們，您將站在AI革命的最前線！", "audio": "docs/data/audios/2506.07982v1.wav"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:34:55.861797", "title_zh": "Flow-Anything：從大規模單視角圖像中學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵技術，但現有方法受限於動畫合成數據集的訓練，難以應用於真實世界。為了解決這個問題，我們提出了Flow-Anything，一個大規模數據生成框架，旨在從任何真實世界的單視角圖像中學習光流估計。我們首先利用單眼深度估計網路將單視角圖像轉換為3D表示，再利用物件獨立體積渲染模組和深度感知修復模組來模擬3D表示中的動態物件，從而生成逼真的光流訓練數據集（FA-Flow Dataset）。實驗證明，從大規模真實世界圖像生成光流訓練數據，效果優於最先進的無監督方法和在合成數據集上訓練的有監督方法。我們的模型可以作為基礎模型，提升各種下游影片任務的性能。", "applications": ["**智慧駕駛輔助系統：** 想像一下，你的汽車能更精準地判斷周圍車輛和行人的移動速度和方向，即使在惡劣天氣或光線不足的情況下，也能做出更安全的反應，避免碰撞。", "**運動賽事分析：** 透過分析球員在球場上的光流，可以更精準地追蹤他們的動作，分析戰術的執行效率，甚至預測他們的下一步動作，提供教練和球員更有效的訓練和比賽策略。", "**安全監控系統：** 監控系統可以更準確地檢測異常行為，例如有人跌倒或發生衝突，及時發出警報，保障公共安全。"], "pitch": "各位投資人，我們正在打造的是下一代視覺智能的基石！Flow-Anything 不僅僅是一個光流估計模型，而是一個能夠從海量真實世界圖像中自主學習的 AI 引擎。想像一下，它能賦予機器人更敏銳的視覺感知能力，讓它們在複雜的環境中自由穿梭；它能讓無人機更精準地進行航拍測繪，實現智慧城市管理；它甚至能幫助開發者創造出前所未有的 AR/VR 體驗。我們已經證明了 Flow-Anything 在真實世界數據上的卓越性能，遠超現有技術。現在，我們需要您的支持，將這項技術推向市場，搶占先機，共同開創一個由視覺智能驅動的全新時代！ 我們預期在三年內，Flow-Anything 將成為智慧駕駛、機器人、安防監控等領域的關鍵技術，市場規模將達到數十億美元！", "audio": "docs/data/audios/2506.07740v1.wav"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "timestamp": "2025-06-10T07:36:36.761479", "title_zh": "動態視角合成作為一個反問題", "summary_zh": "本研究將單眼影片的動態視角合成視為一個反問題，並在無需訓練的環境下解決。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，我們實現了高保真度的動態視角合成，而無需更新權重或使用輔助模組。我們首先發現了零終端訊噪比排程對確定性反演造成的根本障礙，並透過引入一種新的雜訊表示法，即K階遞迴雜訊表示法，來解決這個問題。我們推導出此表示法的閉合形式表達式，從而實現VAE編碼和DDIM反轉潛在變量之間的精確有效對齊。為了合成由相機運動產生的新可見區域，我們引入了隨機潛在調製，它對潛在空間執行可見性感知採樣，以完成被遮擋的區域。綜合實驗表明，動態視角合成可以透過雜訊初始化階段的結構化潛在變量操作有效地執行。", "applications": ["**虛擬實境旅遊體驗：** 想像一下，你只需要用手機拍攝一段影片，就能將它轉換成360度的VR體驗，讓你身歷其境地重溫旅行的美好回憶，甚至探索從未去過的地方。", "**電影特效製作：** 電影製作人員可以使用這項技術，從現有的影片素材中創造出全新的視角和場景，節省大量拍攝成本和時間，讓特效更加逼真自然。", "**線上購物：** 顧客可以透過手機影片，從各個角度觀看商品，就像在實體店面一樣，提升購物體驗和購買意願。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它能將任何單眼影片轉換成高品質的3D動態視角，無需複雜的建模或昂貴的設備。試想一下，這項技術能應用於遊戲、電影、VR/AR、電商等各個領域，創造出前所未有的沉浸式體驗。例如，在遊戲中，玩家可以從任何角度觀看自己的角色，甚至可以創造出獨一無二的遊戲視角。在電商領域，顧客可以透過360度視角，全方位了解商品細節，大幅提升購買意願。更令人興奮的是，我們正在探索將這項技術應用於自動駕駛領域，透過合成多個視角，提升感知能力，讓自動駕駛更加安全可靠。我們的團隊擁有深厚的技術積累和創新能力，相信在各位的支持下，我們一定能將這項技術推向市場，創造巨大的商業價值，成為下一代視覺技術的領導者！", "audio": "docs/data/audios/2506.08004v1.wav"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:15:25.013920", "title_zh": "理解隱私感知運算的錯誤敏感性", "summary_zh": "同態加密（HE）允許在加密數據上進行安全計算，無需解密，為保護隱私的計算提供了絕佳機會。醫療保健、金融和政府等領域對數據隱私和安全極為重視，可藉由HE在敏感數據上實現第三方計算和服務。HE的安全性依賴於添加到數據中的雜訊，使相對簡單的問題在計算上變得難以處理。然而，這種以錯誤為中心的機制也帶來了新的挑戰，即HE本身的容錯性和穩健性。本研究深入探討HE應用對位元錯誤的敏感性，並詳細分析了CKKS方案的錯誤特性，為同態加密的穩健性和錯誤敏感性研究奠定基礎。", "applications": ["**醫療數據共享：** 醫院之間可以安全地共享患者的基因組數據，用於疾病研究，而無需暴露患者的個人隱私。", "**金融交易安全：** 銀行可以在加密的交易數據上進行風險評估和欺詐檢測，保護客戶的財務信息。", "**投票系統：** 選舉投票可以在完全加密的狀態下進行，確保選票的保密性，同時允許公開驗證選舉結果的正確性。"], "pitch": "各位創投先進，想像一下，一個數據永遠處於加密狀態，使用過程中也受到保護的世界！這就是同態加密（HE）的願景，也是我們正在實現的目標。HE如同數據安全的聖杯，將徹底改變醫療、金融、政府等對隱私極其敏感的行業。我們的研究不僅深入理解了HE的錯誤敏感性，更為其大規模應用掃清了障礙。試想，未來的AI模型可以在完全加密的醫療數據上進行訓練，藥廠無需接觸任何原始數據即可開發新藥；銀行可以安全地分析數百萬筆交易，預防金融犯罪，同時保護用戶隱私。這不僅僅是技術突破，更是一場數據安全革命！我們團隊的研究成果將為HE的硬體和軟體優化提供關鍵指導，使其更可靠、更高效。隨著數據隱私意識的日益增強，HE市場將迎來爆發式增長。現在投資我們，您將站在這場變革的最前沿，共同打造一個更安全、更隱私的數據未來！預計五年內，基於我們技術的HE解決方案將滲透到各個行業，市場規模將達到數十億美元。不要錯過這個千載難逢的機會！", "audio": "docs/data/audios/2506.07957v1.wav"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:17:50.350602", "title_zh": "基於大型模型的機器聯覺 (SoM)：AI原生智慧多模感測-通訊整合", "summary_zh": "為了支援未來的智慧多功能第六代 (6G) 無線通訊網路，機器聯覺 (SoM) 被提出作為一種新穎的人工智慧 (AI) 原生智慧多模感測-通訊整合範例。本文首次針對 SoM 系統設計的大型模型 (FMs) 進行系統分類，將其分為通用 FMs（尤其是大型語言模型 (LLMs)）和 SoM 領域特定的 FMs（稱為無線基礎模型）。我們提出了基於 LLM 的路徑損耗生成 (LLM4PG) 和散射體生成 (LLM4SG) 方案，以及用於 SoM 機制探索的無線通道基礎模型 (WiCo)、用於 SoM 增強型收發器設計的基於 LLM 的無線多任務 SoM 收發器 (LLM4WM) 和無線基礎模型 (WiFo)，以及用於 SoM 增強型協作感知的無線協作感知基礎模型 (WiPo)。", "applications": ["想像一下，未來自駕車能透過整合視覺、雷達、以及其他車輛分享的資訊，更精準地預測路況，就像人類能透過多種感官來判斷環境一樣，大幅提升行車安全。", "在智慧工廠中，機器人能整合視覺、聽覺、觸覺等多種感測資訊，更靈敏地執行複雜任務，例如辨識瑕疵品、調整生產線參數，提高生產效率和產品品質。", "未來的醫療診斷，醫生可以利用AI整合病人的生理數據、影像資料、以及口述病史，更全面地了解病情，做出更精準的診斷和治療方案。"], "pitch": "各位投資人，我們正在開發一種革命性的技術，名為「機器聯覺」(SoM)。它就像是賦予機器擁有多重感官融合的能力，讓它們能更聰明、更靈敏地應對複雜的現實世界。想像一下，一個具備人類般直覺的AI系統，可以廣泛應用於自駕車、智慧工廠、醫療診斷等領域，徹底顛覆這些產業。我們的技術基於最新的大型模型，突破了傳統AI的局限性，具有更強的泛化能力和適應性。這是一個千載難逢的投資機會，讓我們一起打造一個由智慧機器主導的未來！初期我們將專注在自駕車領域，與Tier 1 供應商合作，快速將技術落地，搶佔市場先機。預計三年內，我們的技術將成為自駕車的標配，五年內將擴展到其他領域，創造巨大的商業價值。不要錯過這個機會，加入我們，共同開創AI的新紀元！", "audio": "docs/data/audios/2506.07647v1.wav"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "timestamp": "2025-06-10T09:19:15.460624", "title_zh": "MADFormer：混合自迴歸與擴散轉換器用於連續圖像生成", "summary_zh": "MADFormer結合了自迴歸（AR）和擴散模型，旨在優化圖像生成。它將圖像分割成空間區塊，利用AR層進行全局條件設定，而擴散層則在區塊內進行迭代局部細化。實驗證明，區塊分割能有效提升高解析度圖像的生成效果，垂直混合AR和擴散層則能在運算資源有限的情況下，顯著提升圖像品質。MADFormer的研究結果為未來混合生成模型的設計提供了實用指導原則，尤其在高解析度圖像生成領域展現了巨大的潛力。", "applications": ["**客製化頭像生成：**使用者只需提供少量資訊，就能生成獨一無二、高解析度的個人頭像，可用於社群媒體或遊戲。", "**老照片修復：**將模糊、破損的老照片透過AI技術進行修復，恢復清晰細節，讓珍貴回憶重現。", "**藝術創作輔助：**藝術家可以利用這項技術，輸入草稿或概念，快速生成多種風格的高品質藝術作品，激發創作靈感。"], "pitch": "各位創投，MADFormer不僅僅是一項技術，它代表著圖像生成領域的未來！想像一下，一個能以極高效率和品質生成圖像的AI引擎，它的應用範圍將無遠弗屆。從遊戲美術資源的快速生成、電影特效的製作，到廣告行銷素材的客製化，甚至在虛擬實境和元宇宙中創造逼真場景，MADFormer都能扮演關鍵角色。更重要的是，我們已經證明，在運算資源有限的情況下，MADFormer也能超越現有技術。這意味著更低的成本、更快的部署速度，以及更廣泛的市場潛力。現在投資MADFormer，您將搶佔AI圖像生成市場的先機，共同開創視覺內容的新紀元！我們預計，在三年內，MADFormer將成為業界標準，市值突破十億美元！", "audio": "docs/data/audios/2506.07999v1.wav"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:24:04.792554", "title_zh": "學生作業自動化AI評分之實施考量", "summary_zh": "本研究探討在K-12教育環境中導入AI評分平台的可行性，與19位教師合作進行試驗。透過平台使用紀錄、問卷和訪談，觀察教師如何運用AI產生的評分標準和回饋。研究發現，教師重視AI快速提供的敘述性回饋，但對自動評分抱持懷疑，強調人工監督的必要性。學生歡迎快速且著重修改建議的回饋，但對完全由AI評分感到質疑。本研究旨在為設計可信賴、以教師為中心的AI評估工具提供參考，在強化回饋的同時，維持教學主導權。", "applications": ["國小老師批改作文不用再熬夜！AI幫忙抓出文法錯誤、提供修改建議，老師只要專注在內容的引導，讓每個孩子都能愛上寫作。", "大學教授出了一份程式作業，AI能快速檢查程式碼的bug，並給予學生改善方向，讓教授有更多時間指導學生更進階的程式技巧。", "線上學習平台能夠根據學生提交的報告，AI立即提供個人化的回饋，讓學生隨時都能獲得學習上的支持，學習效果UP UP！"], "pitch": "各位投資人，想像一下，一個AI能大幅減輕老師負擔、提升學生學習效率的未來！我們的AI評分平台，不僅能快速提供回饋，更能客製化評估標準，協助老師進行更精準的教學。雖然目前我們強調人工監督，但隨著技術不斷演進，未來AI將能獨立完成初步評分，甚至預測學生的學習瓶頸，提供個人化學習方案。這不只是一個評分工具，更是一個教育革新的起點！市場潛力巨大，從K-12到高等教育，甚至企業培訓，都需要這樣一個能提升效率、降低成本的AI助手。現在投資，您將成為教育AI化的領航者，共同打造更美好的教育未來！", "audio": "docs/data/audios/2506.07955v1.wav"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:25:51.149942", "title_zh": "SurgBench：手術影片分析的統一大型基準", "summary_zh": "SurgBench是一個大型手術影片基準測試框架，包含一個預訓練資料集SurgBench-P和一個評估基準SurgBench-E。SurgBench-P涵蓋22種手術程序和11個專科的5300萬幀影像，SurgBench-E提供六個類別的評估，包含階段分類、相機運動、工具識別、疾病診斷、動作分類和器官檢測，共72個細粒度任務。實驗顯示，現有的影片基礎模型難以在不同的手術影片分析任務中推廣，而使用SurgBench-P進行預訓練可以顯著提高性能，並對未見過的手術程序和模態展現卓越的跨領域泛化能力。SurgBench有助於開發更強大的手術影片分析模型，進而實現手術自動化決策、技能評估和術後品質提升。", "applications": ["醫生可以利用SurgBench訓練的模型，在手術過程中即時獲得工具使用建議，減少失誤，提升手術效率。", "醫學院學生可以透過SurgBench建立的虛擬手術環境，反覆練習手術技巧，降低學習曲線，並在安全環境下熟悉各種手術流程。", "病患家屬可以藉由SurgBench分析手術影片，更了解手術過程，並作為術後照護的參考依據，提升對醫療品質的信心。"], "pitch": "各位投資人，我們正在打造手術室的AI大腦！SurgBench不僅是一個龐大的手術影片資料庫，更是一個賦能醫療AI的引擎。想像一下，AI能即時分析手術影片，協助醫生做出更精準的判斷，降低手術風險，提升成功率。這不僅能減少醫療糾紛，更能大幅降低醫療成本。未來，SurgBench甚至能支援遠程手術，讓偏鄉地區也能享有頂尖醫療資源。我們預期，SurgBench將成為手術機器人、醫療影像分析等領域的基石，引領醫療AI的革命，創造數十億美元的市場價值。現在加入，您將成為這場醫療AI革命的領航者！", "audio": "docs/data/audios/2506.07603v1.wav"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T12:28:20.700342", "title_zh": "權重的生成模型：泛化還是記憶？", "summary_zh": "近年來，生成模型在圖像和影片生成領域取得了巨大成功，因此人們開始探索使用它們來合成有效的神經網路權重。這些方法將訓練好的神經網路檢查點作為訓練資料，並旨在於推理過程中生成高性能的神經網路權重。然而，研究發現這些方法在很大程度上是通過記憶來合成權重，產生的權重要么是訓練檢查點的複製品，要么只是簡單的插值。它們無法勝過簡單的基線方法，例如在權重中添加噪聲或採用簡單的權重集成。而且，即使修改與圖像擴散模型中記憶相關的建模因素或應用資料增強，也無法有效緩解這種記憶現象。這項研究對當前生成模型可以建模的資料類型進行了實際評估，並強調需要在新領域中更仔細地評估生成模型。", "applications": ["客製化AI模型：想像一下，你可以根據自己的需求，快速生成一個AI模型，例如，針對特定疾病的醫療影像分析模型，或針對特定風格的藝術創作模型，而不需要從頭開始訓練。", "AI模型的自動修復：當AI模型因為資料偏移而表現不佳時，可以利用生成模型快速生成新的權重，讓模型恢復到最佳狀態，就像AI模型的自動醫生一樣。", "保護AI模型的智慧財產權：透過生成模型，可以將訓練好的AI模型轉換成另一種形式，既保留了模型的功能，又避免了原始權重被直接複製，從而保護模型的智慧財產權。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能讓AI模型的開發和應用變得前所未有的簡單和高效。雖然目前的研究顯示生成模型在權重生成方面存在記憶問題，但這也代表我們有巨大的突破空間！想像一下，如果我們能克服這個難題，就能夠按需生成各種高度客製化的AI模型，徹底顛覆AI產業。例如，我們可以為每個客戶量身打造獨一無二的AI解決方案，從醫療診斷到金融風控，再到藝術創作，應用前景無限廣闊。更重要的是，這項技術還能有效保護AI模型的智慧財產權，讓您投資的AI技術不再容易被複製和盜用。現在投資，您將有機會參與塑造AI的未來，並獲得豐厚的回報！我們需要您的資金來進一步研究和開發，克服當前的技術瓶頸，最終實現AI模型的真正泛化和創造，而不是單純的記憶和複製。這不僅是一項技術投資，更是一項對未來的投資，是對AI無限可能的投資！", "audio": "docs/data/audios/2506.07998v1.wav"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:14:58.562119", "title_zh": "具成本效益的最佳化主動式AI模型評估", "summary_zh": "生成式AI系統的開發需要持續評估、資料獲取和標註，這在資源和時間上都非常昂貴。為了降低成本，快速迭代通常需要依賴合成標註資料，但這可能導致嚴重的偏差。本研究開發了一種新型的、具成本意識的方法，能主動平衡使用低成本但通常不準確的弱評估者（例如基於模型的自動評估器，用於自動評估生成內容的品質）和更昂貴但更準確的強評估者（例如人類）。我們的目標是在總標註預算限制下，產生目標「強」評估的低變異數、無偏估計。藉由主動和預測驅動的統計推論，我們推導出了一系列具成本效益的最佳化策略，用於在弱評估者和強評估者之間分配給定的標註預算，從而最大化統計效率。使用合成和真實世界的數據，我們驗證了這些策略在哪些條件下可以改進現有方法。我們發現，特別是在範例難度差異很大的任務中，我們的策略可以用比標準評估方法低得多的總標註預算，實現相同的估計精度。", "applications": ["**線上課程平台：** 平台可以使用這項技術，自動評估學生提交的作業品質。系統可以先用AI自動評分，針對AI難以判斷或AI評分可信度較低的作業，再交由真人老師評分，節省老師的時間和精力，同時確保評分品質。", "**客服機器人訓練：** 訓練客服機器人需要大量的對話資料。這項技術可以用來判斷哪些對話需要人工客服介入，哪些可以直接由機器人處理。系統可以使用低成本的AI模型初步判斷，再將難以判斷的對話轉交給真人客服，提高客服效率，降低人力成本。", "**內容創作平台：** 平台可以使用這項技術來評估使用者生成內容（例如文章、圖片、影片）的品質。系統可以先用AI自動評估，針對AI難以判斷或AI評分可信度較低的內容，再交由真人編輯審核，確保平台內容品質，提升使用者體驗。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，它將徹底改變AI模型的評估方式。想像一下，您不再需要投入大量資金聘請專家來評估AI模型的品質，也不用擔心使用低品質的合成資料會導致模型出現偏差。我們的技術能夠智慧地平衡使用低成本的AI評估器和高精度的真人評估器，在預算有限的情況下，最大化評估效率。這項技術的應用範圍非常廣泛，從自動駕駛、醫療診斷到金融風控，任何需要高品質AI模型的領域都將受益。更重要的是，隨著AI技術的快速發展，對模型評估的需求也將越來越大，我們的技術將成為市場上的必需品。我們預計，在未來五年內，AI模型評估市場將達到數十億美元的規模，而我們將在這個市場中佔據領先地位。現在加入我們，共同打造AI評估的未來，實現百倍甚至千倍的投資回報！", "audio": "docs/data/audios/2506.07949v1.wav"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:16:23.097350", "title_zh": "MIRA：用於真實世界健康數據的醫療時間序列基礎模型", "summary_zh": "MIRA是一個專為醫療時間序列預測設計的基礎模型。它利用連續時間旋轉位置編碼處理不規則時間間隔，並透過頻率特定的專家混合層來處理不同的採樣率，以及基於神經常微分方程的連續動態外推區塊來模擬潛在狀態的連續軌跡，從而實現精準預測。MIRA在包含超過4540億個時間點的大規模醫療語料庫上進行預訓練，在零樣本和微調基準測試中，其預測誤差平均降低了10%（異地分佈情境）和7%（同地分佈情境）。同時我們也建立了一個全面的基準，為未來醫療時間序列建模的研究奠定基礎。", "applications": ["想像一下，醫院可以利用MIRA精準預測病患的病情變化，提早發現潛在風險，例如心臟病發作或敗血症，讓醫生有更多時間採取預防措施，提高病患的存活率。", "長照中心可以透過MIRA監測長者的生理數據，預測跌倒或失智症惡化的風險，及早提供協助，減輕照顧者的負擔，提升長者的生活品質。", "個人化的健康管理App可以使用MIRA分析使用者的睡眠、運動和飲食數據，預測罹患慢性疾病的風險，提供客製化的健康建議，幫助使用者維持健康的生活方式。"], "pitch": "各位投資人，我們正處於醫療AI的黃金時代！MIRA，不只是一個模型，而是醫療時間序列分析的全新基礎設施。想像一下，一個能夠精準預測疾病進程、優化治療方案的AI，將徹底改變醫療產業。MIRA在大型醫療數據集上的卓越表現已經證明了它的潛力，相較於現有方案，MIRA能更有效地利用醫療數據，解決數據稀缺和隱私限制等難題。這意味著更低的開發成本、更快的部署速度和更廣泛的應用場景。從精準醫療到遠程監護，從藥物研發到保險理賠，MIRA的應用前景無可限量。我們相信，MIRA將成為醫療AI領域的領頭羊，為投資者帶來豐厚的回報。現在加入我們，一起開創醫療AI的新紀元！", "audio": "docs/data/audios/2506.07584v1.wav"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T15:17:45.268193", "title_zh": "重新思考多模態擴散轉換器中的跨模態互動", "summary_zh": "多模態擴散轉換器（MM-DiT）在文本驅動的視覺生成方面取得了顯著進展，但現有模型在文本提示與生成內容的精確對齊方面仍有困難。研究發現，MM-DiT的注意力機制存在兩個主要問題：視覺和文本模態之間的token不平衡導致跨模態注意力被抑制；缺乏時間步感知的注意力權重，阻礙了對齊。為了解決這些問題，研究提出溫度調整跨模態注意力（TACA），一種參數高效的方法，通過溫度縮放和時間步相關調整動態地重新平衡多模態互動。結合LoRA微調，TACA能以極小的計算開銷顯著提高文本-圖像對齊。", "applications": ["**智慧型購物體驗：** 想像一下，你只要用文字描述想要的衣服款式、顏色和材質，AI就能立刻生成穿在你身上的模擬圖，讓你輕鬆找到最適合自己的商品，省去試穿的麻煩。", "**個人化藝術創作：** 不擅長繪畫也沒關係！只要輸入你腦海中的畫面描述，AI就能幫你生成獨一無二的藝術作品，讓每個人都能成為藝術家。", "**遊戲與虛擬實境內容生成：** 遊戲開發者或VR內容創作者可以透過文字快速生成遊戲場景、角色外觀，大幅降低開發成本，並讓玩家擁有更豐富、更客製化的遊戲體驗。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓文字描述直接轉化為栩栩如生的圖像，精準度遠超現有模型。想像一下，未來廣告商可以根據受眾的細微偏好，即時生成高度客製化的廣告素材；建築師可以快速將客戶的口頭想法轉化為逼真的3D模型；甚至是醫療領域，醫生可以透過患者的描述生成病灶圖像，輔助診斷。這項技術的潛力無窮，我們相信它將徹底改變內容創作、設計、行銷等各個領域。現在加入我們，共同打造圖像生成的未來，分享這項技術帶來的巨大商業價值！", "audio": "docs/data/audios/2506.07986v1.wav"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:24:11.283915", "title_zh": "利用C-偶與C-奇量子關聯的$D\\bar{D}$對，對$D^0\\to K^-π^+$ 與 $\\bar{D}^0\\to K^-π^+$ 衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的測量$D^0$與$\\bar{D}^0$介子衰變振幅間強相位差的技術，利用正負電子碰撞產生能量高於$\\\\psi(3770)$產生閾值的量子關聯$D\\bar{D}$對。在這種能量下，$D\\bar{D}$對產生於電荷共軛對稱性的偶數和奇數本徵態。利用此技術，首次使用此類數據樣本確定了$D^0$-$\\bar{D^0}$相對強相位。使用BESIII實驗在質心能量4.13-4.23 GeV下收集的7.13 fb$^{-1}$積分光度的數據集，測得$D^0\\to K^-\\\\pi^+$ 與 $\\bar{D}^0\\to K^-\\\\pi^+$ 衰變之間的強相位差$\\delta^{D}_{K\\\\pi}$為$\\delta^{D}_{K\\\\pi}=\\\\left(192.8^{+11.0 + 1.9}_{-12.4 -2.4}\\\\right)^\\\\circ$。", "applications": ["想像一下，我們能更精準地預測天氣變化，不再只是看氣象雲圖，而是能深入了解大氣中微小粒子的交互作用，提前預知極端氣候的發生。", "未來的醫療診斷將更快速準確。醫生可以透過分析人體內微量物質的衰變模式，早期發現癌症或其他疾病的徵兆，大幅提高治療成功率。", "在材料科學領域，這項技術能幫助我們設計出更堅固、更耐用的新材料。透過精確控制材料內部的微觀結構，製造出能承受極端環境的超級材料。"], "pitch": "各位創投先進，我們帶來的是一項劃時代的技術突破，它將徹底改變我們對亞原子世界的理解。這項針對D介子衰變的研究，不僅驗證了量子理論的精確性，更開啟了通往未知物理學的大門。想像一下，如果我們能掌握控制亞原子粒子的能力，就能夠開發出能量密度極高的新型電池，徹底解決能源危機。我們甚至可以製造出超微型機器人，深入人體修復受損細胞，實現真正的長生不老。這項技術的潛在商業價值是難以估量的，它將引領下一次科技革命，成為未來世界的基石。現在加入我們，一起投資這項偉大的事業，共同開創人類歷史的新紀元！", "audio": "docs/data/audios/2506.07907v1.wav"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:25:34.862297", "title_zh": "超級編碼網路：用於影片理解的多模態編碼器之遞迴關聯", "summary_zh": "本研究提出一個名為「超級編碼網路」(SEN) 的創新架構，旨在提升影片理解能力。SEN透過遞迴關聯多模態編碼器，更深入地融合不同資訊來源，克服了傳統方法僅透過對比學習對齊編碼器的局限性。SEN將預訓練編碼器視為「超級神經元」，利用遞迴關聯區塊(RA)逐步融合多模態資訊，有效編碼更深層次的多模態互動。實驗結果顯示，SEN在追蹤、辨識、聊天和編輯等四個代表性影片任務上均有顯著提升，例如在像素級追蹤上，平均Jaccard指數提高了2.7%，時間一致性(TC)下降了8.8%。", "applications": ["智慧監控：透過分析監視器畫面中的人物動作和場景變化，自動識別異常行為，例如跌倒、打架等，並及時發出警報。", "自動駕駛：結合車載鏡頭和感測器數據，更精確地理解周圍環境，例如識別行人意圖、預測其他車輛的行駛軌跡，從而提升駕駛安全性。", "影音娛樂：讓影片編輯軟體更聰明，例如自動為影片添加字幕、根據劇情生成精彩片段、甚至根據用戶的文字描述修改影片內容。"], "pitch": "各位投資人，我們正在打造影片理解的未來！想像一下，讓AI像人一樣理解影片內容，這將釋放巨大的商業潛力。我們的「超級編碼網路」技術，就像為AI裝上了一雙更敏銳的眼睛和一個更聰明的大腦，它能更深入地理解影片中的每一個細節，從而實現更精準的分析和更智能的應用。這不僅僅是一項技術，更是一個平台，一個可以賦能各行各業的基礎設施。我們預計，未來在智慧安防、自動駕駛、影音娛樂等領域，都將出現基於我們技術的殺手級應用。現在加入我們，一起引領AI影片理解的浪潮，共同分享這千億美元的市場盛宴！", "audio": "docs/data/audios/2506.07576v1.wav"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "timestamp": "2025-06-10T18:26:56.379104", "title_zh": "基於稀疏注意力Transformer與擴散模型的有效地震數據插值", "summary_zh": "本研究提出一種名為Diff-spaformer的新型深度學習框架，用於解決地震數據插值問題。該模型結合了Transformer架構和擴散模型，利用地震先驗提取網絡(SPEN)作為橋梁。通過全層稀疏多頭注意力和前饋傳播捕捉全局信息分佈，並利用擴散模型提供穩健的先驗指導。模型採用負平方歐幾里德距離計算稀疏親和力矩陣，更適合地震數據建模。實驗結果表明，該模型在隨機和連續缺失數據的插值保真度和計算效率方面均有所提升，為複雜地質條件下的高效地震數據重建提供了一種新範例。", "applications": ["石油探勘公司可以利用此技術更精準地重建地震數據，降低勘探成本，並提高油井鑽探的成功率。", "地質災害預測單位可以利用此技術更有效地分析地震波數據，提升地震預警的準確性，減少人員傷亡。", "工程建設公司在進行隧道或橋樑建設時，可以利用此技術更準確地評估地質結構，確保工程安全。"], "pitch": "各位投資人，想像一下，我們能更精準地看穿地底下的秘密！Diff-spaformer技術，就像是為地球做了一次高解析度的CT掃描。它不僅大幅提升地震數據的分析效率，更降低了勘探成本，為能源產業帶來革命性的突破。試想，如果我們能更精準地找到石油、天然氣，甚至地熱能源，這將為全球能源結構帶來多大的改變？此外，這項技術在防災減災領域也潛力無窮，能更準確地預測地震和滑坡等地質災害，保護人民生命財產安全。我們相信，Diff-spaformer將開啟一個全新的地質勘探與防災時代，成為下一個引領科技浪潮的獨角獸！現在加入我們，一起挖掘地球的寶藏，創造無限可能！", "audio": "docs/data/audios/2506.07923v1.wav"}
{"query": "AI", "id": "2506.07906v1", "url": "http://arxiv.org/abs/2506.07906v1", "title": "First observation of quantum correlations in $e^+e^-\\to XD\\bar{D}$ and $C$-even constrained $D\\bar{D}$ pairs", "summary": "The study of meson pairs produced with quantum correlations gives direct\naccess to parameters that are challenging to measure in other systems. In this\nLetter, the existence of quantum correlations due to charge-conjugation\nsymmetry $C$ are demonstrated in $D\\bar{D}$ pairs produced through the\nprocesses $e^+e^-\\to D\\bar{D}$, $e^+e^- \\to D^{*}\\bar{D}$, and $e^+e^- \\to\nD^{*} \\bar{D}^*$, where the lack of charge superscripts refers to an admixture\nof neutral-charm-meson particle and antiparticle states, using $7.13 \\text{\nfb}^{-1}$ of $e^+e^-$ collision data collected by the BESIII experiment between\ncenter-of-mass energies of $4.13-4.23 \\text{ GeV}$. Processes with either\n$C$-even or $C$-odd constraints are identified and separated. A procedure is\npresented that harnesses the entangled production process to enable\nmeasurements of $D^0$-meson hadronic parameters. This study provides the first\nconfirmation of quantum correlations in $e^+e^-\\to X D\\bar{D}$ processes and\nthe first observation of a $C$-even constrained $D\\bar{D}$ system. The\nprocedure is applied to measure $\\delta^{D}_{K\\pi}$, the strong phase between\nthe $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decay amplitudes, which\nresults in the determination of $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$. The potential for measurements of other\nhadronic decay parameters and charm mixing with these and future datasets is\nalso discussed.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:13:24.015493", "title_zh": "首次觀測到 $e^+e^-\\\\\\to XD\\bar{D}$ 和 $C$-宇稱約束 $D\\bar{D}$ 對中的量子關聯", "summary_zh": "本研究利用北京譜儀III（BESIII）實驗收集的正負電子碰撞數據，首次證實在$e^+e^-\\\\to XD\\bar{D}$過程中存在量子關聯，並首次觀測到$C$-宇稱約束的$D\\bar{D}$系統。透過分析$D\\bar{D}$對的產生過程，我們得以研究介子對的量子關聯，進而測量其他系統難以測量的參數。我們還提出了一種利用量子糾纏的產生過程來測量$D^0$介子強子參數的方法，並成功測量了$D^0\\to K^-\\pi^+$和$\\bar{D}^0\\to K^-\\pi^+$衰變振幅之間的強相位差$\\delta^{D}_{K\\pi}$。此研究為未來測量其他強子衰變參數和魅力混合提供了可能性。", "applications": ["【醫療影像增強】想像一下，醫生在看X光片或核磁共振時，影像不夠清晰，難以判斷細微病灶。利用量子關聯技術，我們可以提升影像的解析度，讓醫生能更精準地診斷疾病，及早發現癌症等問題。", "【超安全通訊】現在的網路安全越來越重要，但還是有很多漏洞。如果用量子關聯來加密訊息，就像用一把只有你和接收者才知道的鑰匙，而且一旦有人試圖偷看，鑰匙就會自動銷毀，確保資訊安全。", "【更精準的導航】現在的GPS有時候會不準，尤其是在高樓林立的城市裡。利用量子關聯，我們可以打造更精準的導航系統，無論是自動駕駛還是無人機送貨，都能更安全可靠。"], "pitch": "各位投資人，我們發現了量子力學中一個令人振奮的現象，並將其應用於前所未有的精確測量和資訊技術。想像一下，一個能以前所未有的精度分析亞原子粒子的世界，這不僅僅是科學突破，更是通往新技術的鑰匙。我們已經證明了在介子對中存在量子關聯，這為我們打開了一扇通往高精度測量的大門，在材料科學、醫學成像和安全通訊領域具有顛覆性的潛力。我們的技術能夠實現更安全、無法破解的通訊系統，在網路安全至關重要的時代，這是一項極具價值的資產。此外，它還能推動醫學診斷的發展，實現更早、更準確的疾病檢測。更重要的是，這項技術將會是量子計算發展的重要基石。我們團隊擁有一流的科學家和工程師，正處於這場變革的前沿。現在是加入我們，共同塑造量子技術的未來，並從這項突破性創新中獲得豐厚回報的絕佳時機。我們相信，這項技術的投資回報將遠遠超過您的預期，並將為您的投資組合增加巨大的價值。", "audio": "docs/data/audios/2506.07906v1.wav"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "timestamp": "2025-06-10T21:14:49.230234", "title_zh": "用於H&E到IHC虛擬染色的跨通道感知學習", "summary_zh": "本研究提出一種新的跨通道感知學習（CCPL）策略，旨在解決現有H&E到IHC虛擬染色研究中忽略細胞核與細胞膜之間跨通道關聯性的問題。CCPL首先將HER2免疫組織化學染色分解為細胞核和細胞膜對應的蘇木精和DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，提取雙通道特徵並測量核膜間的相關性。同時，通過特徵蒸餾損失和光密度圖的統計分析，提升模型特徵提取能力並確保染色一致性。實驗結果表明，CCPL有效保留病理特徵，生成高質量虛擬染色圖像，為多媒體醫療數據的自動病理診斷提供強有力支持。", "applications": ["**個性化醫療診斷：** 想像一下，未來醫生可以根據你的H&E染色切片，快速生成各種IHC虛擬染色結果，精準預測你對不同藥物的反應，制定最適合你的治療方案，避免不必要的副作用。", "**遠程病理診斷：** 偏遠地區的醫院可能缺乏昂貴的IHC染色設備，但透過我們的技術，他們只需傳輸H&E染色切片，就能遠程生成IHC虛擬染色圖像，讓專家進行診斷，提升醫療資源的可及性。", "**病理教學與研究：** 醫學生和研究人員可以使用我們的技術，從現有的H&E染色切片中，快速生成大量不同IHC標記的虛擬染色圖像，用於學習、教學和研究，加速病理學的發展。"], "pitch": "各位投資人，我們正在開發一項革命性的病理診斷技術——基於跨通道感知學習的虛擬染色。想像一下，一個無需昂貴試劑和耗時流程，就能從普通H&E染色切片生成各種IHC染色的未來。這不僅能大幅降低病理診斷的成本和時間，還能加速新藥研發，實現個性化醫療。我們的技術基於最先進的AI模型，並已在實驗室中驗證其準確性和可靠性。我們預計，隨著數字病理學的普及，市場對虛擬染色的需求將呈指數級增長。現在加入我們，您將有機會參與一場醫療革命，共同打造一個更精準、更高效、更普惠的醫療未來！我們的目標是成為病理AI領域的領導者，引領下一代診斷技術的發展。", "audio": "docs/data/audios/2506.07559v1.wav"}
{"query": "AI", "id": "2506.07907v2", "url": "http://arxiv.org/abs/2506.07907v2", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between the decay\namplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T00:56:31.677881", "title_zh": "利用C-宇稱偶與奇的量子關聯D零反D零對，對D零->K負π正與反D零->K負π正衰變間強相位差的新穎測量", "summary_zh": "本研究提出一種新穎的強相位差測量技術，利用能量高於ψ(3770)產生閾值的正負電子碰撞產生的量子關聯D零反D零對。此方法利用了電荷共軛對稱性的偶與奇本徵態下產生的D零反D零對。透過此技術，我們首次利用此類數據樣本確定了D零-反D零的相對強相位。使用北京譜儀III實驗在4.13-4.23 GeV質心能量下收集的7.13 fb-1積分光度數據集，測得D零->K負π正與反D零->K負π正衰變之間的強相位差δDKπ為(192.8+11.0+1.9−12.4−2.4)度。", "applications": ["想像一下，我們在機場安檢時，可以更精準地判斷行李中是否有違禁品。這項技術就像是更敏銳的X光，能分辨出極微小的物質差異，提高安檢效率。", "在醫療領域，醫生可以利用類似的原理，更準確地診斷癌症。透過分析癌細胞的微小變化，這項技術有助於早期發現並制定更有效的治療方案。", "在材料科學領域，工程師可以利用這項技術開發更堅固、更耐用的材料。透過精確控制材料的微觀結構，我們可以製造出性能更優越的產品，例如更輕便的汽車或更耐用的橋樑。"], "pitch": "各位創投先進，我們帶來的是一項革命性的技術，它將改寫我們對物質世界的認知和操控方式。這項技術的核心在於精準測量基本粒子間的強相位差，雖然聽起來很學術，但它的應用潛力卻是無可限量的。想像一下，我們可以打造出前所未有的精準感測器，應用於國防安全、醫療診斷、以及新材料開發等領域。更重要的是，這項技術是通往量子計算的鑰匙之一。透過精準控制量子態，我們有機會實現超越現有計算能力的量子電腦，進而引領下一次科技革命。現在投資，您將成為這場革命的先驅，共同開創一個充滿無限可能的未來！我們的團隊擁有頂尖的物理學家和工程師，我們已經完成了初步的實驗驗證，並準備好將這項技術商業化。我們需要您的資金支持，將實驗室成果轉化為實際產品，搶佔市場先機。這不僅是一項投資，更是一次參與改變世界的機會！", "audio": "docs/data/audios/2506.07907v2.wav"}
{"query": "Diffusion Model", "id": "2506.07902v1", "url": "http://arxiv.org/abs/2506.07902v1", "title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling", "summary": "Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.", "authors": ["Sifan Wang", "Zehao Dou", "Tong-Rui Liu", "Lu Lu"], "published_date": "2025-06-09", "timestamp": "2025-06-11T00:57:53.110798", "title_zh": "FunDiff：基於函數空間的擴散模型，用於物理資訊生成建模", "summary_zh": "FunDiff 是一個創新的函數空間生成模型框架，它結合了潛在擴散過程和函數自編碼器架構。這個模型能處理不同離散化的輸入函數，生成可在任意位置評估的連續函數，並能無縫整合物理先驗知識。透過架構約束或基於物理資訊的損失函數，FunDiff 確保生成的樣本滿足基本的物理定律。理論分析表明，FunDiff 在函數空間中實現了最佳的密度估計收斂速度。在流體力學和固體力學等領域的實驗結果證明，FunDiff 能夠生成與目標分佈高度一致且符合物理規律的樣本，並且對雜訊和低解析度資料具有魯棒性。", "applications": ["天氣預報：利用 FunDiff 生成更精確、更穩定的天氣預測模型，能更準確預測颱風路徑、降雨量等，幫助人們提前做好防災準備。", "材料設計：在設計新材料時，FunDiff 可以預測材料在不同條件下的力學性能，加速新材料的開發，例如更輕、更堅固的汽車零件或更耐用的建築材料。", "醫療影像分析：FunDiff 可以生成更真實的醫療影像，幫助醫生進行疾病診斷和治療方案規劃，例如模擬手術過程或預測腫瘤生長情況。"], "pitch": "想像一下，如果我們能像設計積木一樣設計物理世界，會發生什麼？FunDiff 正是實現這一點的關鍵技術！它是一個基於擴散模型的函數空間生成框架，讓AI能夠理解並生成符合物理定律的複雜系統。這意味著，我們可以利用 FunDiff 加速新材料的發現，優化飛行器的設計，甚至預測氣候變化帶來的影響。更重要的是，FunDiff 具有極高的商業價值，它可以應用於各個領域，從航空航天到生物醫學，徹底改變產品設計和科學研究的方式。我們相信，FunDiff 將成為未來物理建模領域的基石，引領下一代工程和科學革命。現在投資 FunDiff，就是投資未來！", "audio": "docs/data/audios/2506.07902v1.wav"}
{"query": "AI", "id": "2506.09050v1", "url": "http://arxiv.org/abs/2506.09050v1", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering", "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:44:35.753035", "title_zh": "ALE-Bench：長時程目標導向演算法工程的基準測試", "summary_zh": "ALE-Bench是一個評估AI在解決複雜優化問題能力的新基準。它基於AtCoder Heuristic Contests的真實任務，這些任務涵蓋包裹遞送、排班、工廠生產計劃和電網平衡等領域。與傳統的短時編碼測試不同，ALE-Bench強調長時間迭代優化。評測顯示，雖然大型語言模型在特定問題上表現出色，但在跨問題一致性和長時程問題解決能力方面仍與人類存在差距。這個基準旨在推動AI在演算法工程領域的未來發展，為解決現實世界的複雜問題提供更強大的AI工具。", "applications": ["想像一下，物流公司可以利用這個技術，自動規劃最佳送貨路線，大幅降低油耗和時間成本，讓你的包裹更快送到。", "醫院可以用它來安排手術排程，確保資源有效分配，縮短病人等待時間，提升醫療服務品質。", "工廠可以運用這項技術，優化生產流程，減少浪費，提高產能，讓產品更快上市。"], "pitch": "各位投資人，我們正處於AI賦能各行各業的黃金時代！ALE-Bench不僅是一個基準測試，更是一個加速AI解決複雜問題的催化劑。想像一下，一個AI可以自動優化全球供應鏈，每年節省數十億美元；一個AI可以精準預測能源需求，實現更智能、更穩定的電網。我們的技術將引領下一代AI演算法的發展，解決傳統方法無法應付的難題。這是一個巨大的市場機會，我們正在尋找具有遠見的投資人，一起打造AI驅動的未來！我們預期，未來五年內，基於ALE-Bench的AI解決方案將滲透到物流、製造、能源、醫療等關鍵領域，創造巨大的經濟效益和社會價值。", "audio": "docs/data/audios/2506.09050v1.wav"}
{"query": "Foundation Model", "id": "2506.09042v1", "url": "http://arxiv.org/abs/2506.09042v1", "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "published_date": "2025-06-10", "timestamp": "2025-06-11T03:46:06.060714", "title_zh": "宇宙駕駛夢：基於世界基礎模型的可擴展合成駕駛數據生成", "summary_zh": "自動駕駛系統的開發需要大量真實世界的數據，但收集和標註這些數據既耗時又昂貴，尤其是捕捉罕見的邊緣案例。為了解決這個問題，我們推出了Cosmos-Drive-Dreams，一個合成數據生成流程，旨在生成具有挑戰性的場景，以促進感知和駕駛策略訓練等下游任務。該流程的核心是Cosmos-Drive，一套基於NVIDIA Cosmos世界基礎模型專為駕駛領域設計的模型，能夠生成可控、高保真、多視角和時空一致的駕駛影片。實驗結果表明，我們生成的數據有助於緩解長尾分佈問題，並提高3D車道檢測、3D物體檢測和駕駛策略學習等下游任務的泛化能力。我們透過NVIDIA的Cosmos平台開源了我們的流程工具包、數據集和模型權重。", "applications": ["想像一下，未來你可以用手機APP客製化你的自動駕駛學習課程。你想在暴雨天開車？想練習在濃霧中停車？只要在APP上設定，系統就能生成對應的模擬場景，讓你安全地提升駕駛技能。", "遊戲開發者可以利用這項技術，快速生成各種逼真的駕駛場景，創造出更沉浸式的賽車遊戲或開放世界遊戲。不再需要花費大量時間和金錢去拍攝真實場景，就能擁有無限可能的遊戲世界。", "交通管理部門可以利用這項技術，模擬各種交通狀況，例如：施工路段、交通事故等，來測試和優化交通控制策略，從而提高道路安全和效率。"], "pitch": "各位創投先進，我們正在打造自動駕駛的「數據印鈔機」！Cosmos-Drive-Dreams不僅能大幅降低自動駕駛數據收集的成本，更能創造出真實世界難以企及的極端場景，例如：超高難度的冰雪路面、突發的動物穿越等等。這意味著，我們能加速自動駕駛技術的成熟，讓無人計程車、無人物流車更快落地。更重要的是，這項技術的應用範圍遠不止於自動駕駛。我們可以將它應用於虛擬實境、遊戲開發、甚至是城市規劃。想像一下，未來的建築師可以在虛擬環境中模擬各種交通流量，優化道路設計；電影導演可以輕鬆創造出史詩級的追逐場景。這是一個千億級別的市場，而我們擁有領先的技術和先發優勢，絕對是您不容錯過的投資機會！", "audio": "docs/data/audios/2506.09042v1.wav"}
{"query": "AI", "id": "2506.09049v1", "url": "http://arxiv.org/abs/2506.09049v1", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T06:18:50.061894", "title_zh": "VIKI-R：透過強化學習協調具體化多智能體合作", "summary_zh": "在動態環境中協調多個具體化智能體是人工智慧的核心挑戰。本研究提出VIKI-R框架，利用視覺語言模型(VLM)進行視覺推理，並透過強化學習協調多智能體合作。VIKI-R包含兩個階段：首先，使用Chain-of-Thought註解的示範數據微調預訓練的VLM；接著，在多層次獎勵訊號下進行強化學習。實驗證明，VIKI-R在所有任務層面都顯著優於基準方法，並能使異質智能體之間產生組合式的合作模式。VIKI-R和VIKI-Bench為具體化人工智慧系統中，多智能體視覺驅動合作提供了一個統一的測試平台和方法。", "applications": ["**智慧家庭管家團隊：** 想像一下，家裡不再只有一個掃地機器人，而是有一個團隊！VIKI-R能讓不同功能的機器人（掃地、擦地、整理）協同合作，更有效率地完成家務。例如，擦地機器人發現地上有髒污，會通知掃地機器人先來處理，再進行擦拭。", "**工廠協作機械手臂：** 在生產線上，不同功能的機械手臂不再各自為政，而是像一個團隊一樣協同工作。VIKI-R能讓它們根據視覺資訊判斷，分工合作組裝產品，提高生產效率和品質。例如，一個機械手臂負責定位零件，另一個負責鎖螺絲，兩者配合無間。", "**救災現場無人機協作：** 災難發生時，多架無人機可以透過VIKI-R協調合作，進行災情評估、搜索倖存者、運送物資等任務。例如，一架無人機負責高空偵察，另一架負責低空搜索，它們可以共享資訊，更快速地找到需要幫助的人。"], "pitch": "各位投資人，我們正站在AI協作的新浪潮之上！VIKI-R不僅僅是一個技術突破，它代表的是一個全新的協作模式。想像一下，未來世界，無數的機器人、無人機，甚至是軟體代理，都能像一個團隊一樣協同工作，解決複雜的問題。VIKI-R正是這個願景的基石。它能讓異質智能體之間產生前所未有的合作模式，大幅提升效率、降低成本。從智慧製造、智慧物流到智慧城市，VIKI-R的應用潛力無可限量。我們相信，VIKI-R將引領下一代AI發展，成為各行各業不可或缺的協作引擎。現在加入我們，共同打造AI協作的未來，贏得百億美元的市場！", "audio": "docs/data/audios/2506.09049v1.wav"}
{"query": "AI", "id": "2506.09002v1", "url": "http://arxiv.org/abs/2506.09002v1", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:15:09.759403", "title_zh": "透過混合程式分析與大型語言模型提升Rust單元測試覆蓋率", "summary_zh": "本研究提出PALM，一種結合程式分析與大型語言模型(LLM)的方法，旨在提升Rust程式的單元測試覆蓋率。PALM首先分析程式碼，找出分支條件並轉換為路徑約束。接著，利用這些約束條件與程式碼上下文資訊，設計提示詞引導LLM生成高效能的單元測試。實驗結果顯示，PALM能在短時間內顯著提升測試覆蓋率，某些情況下整體專案覆蓋率提升超過50%，平均覆蓋率達到75.77%，與人工測試相近。此方法成功整合程式分析與人工智慧，為自動化軟體測試開闢新方向。", "applications": ["想像一下，以後軟體更新再也不怕出包！PALM就像軟體的健康檢查醫生，自動幫忙找出潛在問題，讓你的手機App、電腦程式更穩定，再也不會用到一半突然當機。", "如果你是個遊戲玩家，一定很怕遇到Bug。有了PALM，遊戲開發者可以更快速地測試遊戲，確保遊戲體驗順暢，讓你玩得更開心，不用再忍受惱人的錯誤。", "現在很多家電都連上網路，像是智慧冰箱、智慧電視等等。PALM可以確保這些設備的軟體安全可靠，保護你的隱私，避免被駭客入侵。"], "pitch": "各位投資人，我們正在革新軟體測試的未來！PALM不僅僅是個工具，更是個能大幅降低軟體開發成本、提升軟體品質的革命性技術。想像一下，一個能自動生成高覆蓋率測試的系統，能讓軟體公司節省多少人力和時間？這意味著更快的產品上市速度、更低的維護成本、以及更高的客戶滿意度。隨著AI技術的進步，PALM的潛力是無限的！我們可以將它應用於各種程式語言、各種軟體平台，甚至可以客製化服務，滿足不同產業的需求。我們預計，在五年內，PALM將成為軟體測試領域的領導者，為投資者帶來豐厚的回報！現在加入我們，一起打造更可靠、更安全的軟體世界！", "audio": "docs/data/audios/2506.09002v1.wav"}
{"query": "Foundation Model", "id": "2506.08982v1", "url": "http://arxiv.org/abs/2506.08982v1", "title": "On Finetuning Tabular Foundation Models", "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:17:45.572972", "title_zh": "表格型基礎模型的微調研究", "summary_zh": "本研究探討如何針對表格型資料的基礎模型TabPFNv2進行最佳微調。TabPFNv2原本透過上下文學習，在小型數據集上表現優異，無需調整模型參數。然而，如何微調此類模型以適應特定數據集，以及微調如何改變其內部運作機制，仍是未解之謎。研究發現，完整微調在效率和效果上都是TabPFNv2的最佳選擇。微調後的模型，其測試樣本與訓練樣本之間的相似度計算更準確，進而提升了基於檢索的預測邏輯。實驗證明，微調後的TabPFNv2在包含5萬個物件的數據集上也能有效運作，並在學術數據集上取得領先成果。然而，在具有時間偏移和豐富特徵的數據集上，其穩定性較差。", "applications": ["**個人信用評估：** 銀行或貸款機構可以使用微調後的模型，根據你的交易紀錄、消費習慣等表格數據，更精準地評估你的信用風險，決定是否核准貸款或調整利率。", "**疾病診斷輔助：** 醫生可以輸入病患的各種檢驗數據（例如：血壓、血糖、膽固醇等），讓微調後的模型輔助判斷病患罹患特定疾病的風險，或推薦更精準的檢查項目。", "**產品推薦系統：** 電商平台可以利用微調後的模型，根據你的瀏覽紀錄、購買紀錄、人口統計資料等表格數據，更有效地推薦你可能感興趣的商品，提升銷售額。"], "pitch": "各位投資人，想像一下，未來AI不再需要海量資料訓練，而是像一位經驗豐富的專家，只需少量數據就能快速掌握新領域的知識！我們正在開發的技術，正是基於表格型基礎模型TabPFNv2的微調技術，讓AI在表格數據上實現真正的『即學即用』。這項技術的潛力無窮：在金融領域，它可以精準預測市場趨勢，抓住投資機會；在醫療領域，它可以加速新藥研發，改善診斷效率；在零售領域，它可以優化供應鏈管理，提升客戶滿意度。更重要的是，我們的技術降低了AI應用的門檻，讓中小企業也能輕鬆享受AI帶來的紅利。我們相信，這項技術將引領下一波AI革命，成為各行各業不可或缺的工具。現在加入我們，一起打造AI驅動的未來！", "audio": "docs/data/audios/2506.08982v1.wav"}
{"query": "Diffusion Model", "id": "2506.08809v1", "url": "http://arxiv.org/abs/2506.08809v1", "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions.", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "published_date": "2025-06-10", "timestamp": "2025-06-11T09:19:05.918184", "title_zh": "HiSin：透過解析度導向的漸進式推論實現高效能高解析度正弦圖修復", "summary_zh": "在電腦斷層掃描中，高解析度正弦圖修復至關重要。遺失高頻投影可能導致明顯的偽影和診斷錯誤。我們提出HiSin，一種基於擴散模型的新框架，透過解析度導向的漸進式推論，實現高效正弦圖修復。HiSin在低解析度下逐步提取全局結構，並將高解析度推論延遲到小塊區域，從而實現記憶體高效的修復。同時，它還結合了頻率感知的塊跳過和結構自適應的步驟分配，以減少冗餘計算。實驗結果表明，HiSin降低了高達31.25%的峰值記憶體使用量，並減少了18.15%的推論時間，同時保持了跨數據集、解析度和遮罩條件下的修復準確性。", "applications": ["想像一下，機場安檢時，X光掃描儀因為某個角度被遮擋，導致行李圖像出現盲點。HiSin技術可以自動修復這些盲點，讓安檢人員更清楚地看到行李內部的物品，提高安全性。", "牙醫在進行X光檢查時，如果影像受到干擾或部分缺失，HiSin技術可以填補這些缺失，幫助牙醫更準確地診斷蛀牙或牙齒問題，減少誤判。", "古文物修復人員在使用X光掃描古代陶器或青銅器時，如果掃描數據不完整，HiSin技術可以重建出更完整的文物圖像，幫助他們更好地研究和保護這些珍貴的文化遺產。"], "pitch": "各位投資人，我們帶來的是HiSin技術，它將徹底改變醫學影像、安檢和文物保護等領域。現有的電腦斷層掃描技術在高解析度圖像處理上存在記憶體和計算瓶頸，導致成本高昂且效率低下。HiSin透過創新的解析度導向漸進式推論，大幅降低了資源消耗，同時保持甚至提升了圖像修復的準確性。這意味著更快的掃描速度、更低的硬體成本，以及更精確的診斷結果。想像一下，未來搭載HiSin技術的移動式醫療設備，可以深入偏遠地區，提供即時的、高品質的醫療影像服務。在安檢領域，更清晰的X光圖像將有效提升安全防護能力。在文物保護方面，HiSin將幫助我們解鎖更多歷史的秘密。我們相信，HiSin技術具有巨大的市場潛力，將為投資者帶來豐厚的回報。現在投資HiSin，就是投資未來！", "audio": "docs/data/audios/2506.08809v1.wav"}
{"query": "AI", "id": "2506.08998v1", "url": "http://arxiv.org/abs/2506.08998v1", "title": "On Monotonicity in AI Alignment", "summary": "Comparison-based preference learning has become central to the alignment of\nAI models with human preferences. However, these methods may behave\ncounterintuitively. After empirically observing that, when accounting for a\npreference for response $y$ over $z$, the model may actually decrease the\nprobability (and reward) of generating $y$ (an observation also made by\nothers), this paper investigates the root causes of (non) monotonicity, for a\ngeneral comparison-based preference learning framework that subsumes Direct\nPreference Optimization (DPO), Generalized Preference Optimization (GPO) and\nGeneralized Bradley-Terry (GBT). Under mild assumptions, we prove that such\nmethods still satisfy what we call local pairwise monotonicity. We also provide\na bouquet of formalizations of monotonicity, and identify sufficient conditions\nfor their guarantee, thereby providing a toolbox to evaluate how prone learning\nmodels are to monotonicity violations. These results clarify the limitations of\ncurrent methods and provide guidance for developing more trustworthy preference\nlearning algorithms.", "authors": ["Gilles Bareilles", "Julien Fageot", "Lê-Nguyên Hoang", "Peva Blanchard", "Wassim Bouaziz", "Sébastien Rouault", "El-Mahdi El-Mhamdi"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:23:55.474614", "title_zh": "論AI對齊中的單調性", "summary_zh": "基於比較的偏好學習已成為AI模型與人類偏好對齊的核心。然而，這些方法可能出現違反直覺的行為。本文探討了在考慮對回應y優於z的偏好時，模型實際上可能會降低生成y的機率（以及獎勵）的根本原因。研究針對包含直接偏好優化(DPO)、廣義偏好優化(GPO)和廣義Bradley-Terry(GBT)的通用比較偏好學習框架，分析(非)單調性的成因。在溫和的假設下，證明了這些方法仍然滿足局部成對單調性。並提供了一系列單調性的形式化定義，並確定了保證它們的充分條件，從而提供了一個工具箱來評估學習模型違反單調性的可能性。這些結果闡明了當前方法的局限性，並為開發更值得信賴的偏好學習算法提供了指導。", "applications": ["個人化推薦系統：確保推薦的商品或服務隨著使用者表達更多偏好而變得更好，不會出現使用者表示喜歡A後，系統反而推薦更差的B。", "自動駕駛系統：確保自動駕駛在學習人類駕駛習慣後，不會出現判斷邏輯混亂，例如學會禮讓行人後，反而更容易發生交通事故。", "醫療診斷輔助系統：確保AI在學習醫生的診斷偏好後，不會出現診斷結果不一致或矛盾的情況，例如學會某種疾病的特徵後，反而忽略了該疾病的早期症狀。"], "pitch": "各位創投先進，我們正在解決AI領域一個根本性的問題：如何確保AI的行為符合人類的直覺和期望。我們的研究揭示了現有AI偏好學習方法中存在的「單調性」問題，也就是說，AI有時候會學反了！想像一下，如果你的AI助理在你說喜歡某個餐廳後，反而不再推薦類似的餐廳，這是不是很荒謬？我們的技術能夠保證AI學習過程的單調性，確保AI始終朝着符合人類偏好的方向發展。這不僅能提升AI的可靠性和安全性，更能大幅改善使用者體驗，應用範圍極其廣泛，從個人化推薦、自動駕駛到醫療診斷，都有巨大的市場潛力。我們相信，隨著AI越來越普及，對AI行為可控性的需求也會越來越高，我們的技術將成為AI時代的基礎設施，具有極高的投資價值。現在投資我們，就是投資AI的未來！我們預期在三年內，我們的技術能被廣泛應用於各個領域，並成為AI偏好學習的行業標準，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.08998v1.wav"}
{"query": "Foundation Model", "id": "2506.08955v1", "url": "http://arxiv.org/abs/2506.08955v1", "title": "Segment Concealed Objects with Incomplete Supervision", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models.", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:25:14.129810", "title_zh": "利用不完整監督分割隱藏物體", "summary_zh": "本研究提出一個針對不完整監督隱藏物體分割(ISCOS)的統一方法，旨在解決在不完整標註數據下分割與環境無縫融合的物體。此方法利用視覺基礎模型「Segment Anything Model (SAM)」產生偽標籤，並透過一系列策略來生成、儲存和監督這些偽標籤，以確保穩健的網路訓練。此外，設計了一種混合粒度特徵分組模塊，通過對不同粒度的特徵進行分組和聚合，促進分割連貫性，從而更完整地分割單個和多個物體。實驗結果表明，該方法在多個ISCOS任務中實現了最先進的性能，並且可以作為一個即插即用的解決方案，增強現有模型的性能。", "applications": ["尋找失物：想像一下，你掉了鑰匙或錢包，但它們完美地融入了地毯或沙發中。這項技術可以幫助你快速找到它們，即使它們被巧妙地隱藏起來。", "醫療影像分析：醫生可以利用這項技術更準確地識別X光片或MRI中的微小病灶或異常，即使它們與周圍組織非常相似。", "自動駕駛安全：在惡劣天氣或光線條件下，車輛可以利用這項技術更可靠地檢測到行人、交通標誌或其他障礙物，即使它們被部分遮擋或偽裝。"], "pitch": "各位投資人，我們正處於一個AI視覺革命的風口浪尖！我們的技術，基於不完整監督的隱藏物體分割，不僅解決了業界一大難題，更開啟了無限的商業可能性。試想一下，無人商店的防盜系統，能夠精準識別被遮蔽的商品；國防安全領域，能有效偵測偽裝目標；智慧農業上，可辨識隱藏在作物中的病蟲害。這項技術的核心優勢在於其高適應性和低成本，無需大量人工標註數據，即可實現高精度分割。我們預計，未來五年內，隱藏物體分割技術的市場規模將達到數十億美元。現在加入我們，您將成為這場視覺革命的領航者，共同創造巨大的商業價值！", "audio": "docs/data/audios/2506.08955v1.wav"}
{"query": "Diffusion Model", "id": "2506.08796v1", "url": "http://arxiv.org/abs/2506.08796v1", "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm.", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "published_date": "2025-06-10", "timestamp": "2025-06-11T12:26:24.795768", "title_zh": "流動多樣且高效：透過隨機速度場採樣學習動量流匹配", "summary_zh": "這項研究提出了一種新的修正流模型Discretized-RF，它將傳統修正流中的直線路徑離散化為一系列可變速度場的子路徑，藉此擴展採樣空間，尤其是在接近雜訊分佈時。與直接在資料上疊加雜訊不同，Discretized-RF在子路徑的速度上引入雜訊，改變其方向，從而提升生成結果的多樣性和多尺度雜訊建模能力。實驗證明，透過採樣隨機速度場學習動量流匹配，能夠產生多樣且高效的軌跡，並持續生成高品質且多樣化的結果。簡單來說，就是讓AI生成圖像更豐富、更真實，而且速度更快。", "applications": ["線上遊戲：快速生成多樣化的遊戲角色、場景和道具，降低開發成本，提升遊戲體驗。", "電影特效：生成逼真的特效畫面，例如爆炸、天氣變化等，縮短製作時間，降低製作門檻。", "藝術創作：藝術家可以利用這項技術快速生成各種風格的藝術作品，探索新的創作可能性。"], "pitch": "各位投資人，我們團隊帶來的是一項突破性的AI圖像生成技術，Discretized-RF。想像一下，一個能以閃電般的速度，創造出前所未有、栩栩如生的圖像的世界。傳統AI圖像生成技術的瓶頸在於生成速度慢、多樣性不足，而Discretized-RF完美解決了這些問題。它就像一位天賦異稟的藝術家，能根據您的需求，快速生成各種風格、各種主題的圖像，而且效果驚艷。這項技術的潛力無可限量，從遊戲開發、電影製作到廣告設計、教育娛樂，甚至醫療診斷，都將產生革命性的影響。我們預計，未來五年內，AI圖像生成市場將達到數百億美元的規模，而Discretized-RF將在這個市場中佔據領導地位，為各位帶來豐厚的回報。現在加入我們，共同開創AI圖像生成的新時代！", "audio": "docs/data/audios/2506.08796v1.wav"}
{"query": "AI", "id": "2506.08962v1", "url": "http://arxiv.org/abs/2506.08962v1", "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch.", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:14:30.063849", "title_zh": "研發中：大型語言模型強化的智慧導師系統，用於大學電路分析", "summary_zh": "本研究展示一個AI驅動的智慧導師系統，專為大學電路分析課程的學生提供作業評估與回饋。此系統運用大型語言模型，精心設計提示語，以優化對各種問題的回應。目前已部署於Microsoft Azure平台，並在美國東南部一所大型公立研究型大學的電機與電腦工程學院的電路分析課程中使用。智慧導師不僅提供個人化指導和回饋，還收集學生互動數據，並將其匯總後與課程講師共享，幫助教師即時了解學生的困難點，從而進行更有針對性的課堂教學。初步學生回饋顯示，90.9%的學生對此導師感到滿意。未來，我們將釋出完整分析報告，並探索此智慧導師在更廣泛工程領域的應用。", "applications": ["1. 小明在準備電路學期中考，遇到一個複雜的電路分析題目卡住了。他可以使用這個智慧導師，一步一步引導他解題，就像一位耐心的家教一樣，而且隨時隨地都能使用。", "2. 小華是一位電路學老師，他可以利用這個智慧導師收集到的學生提問數據，了解學生最常遇到的困難，並在課堂上更有針對性地講解這些概念，提升教學效率。", "3. 一家電子公司的新進工程師小李，在設計電路時遇到了一些技術問題。他可以利用這個智慧導師快速查找相關資料和解決方案，提升工作效率，加速產品開發。"], "pitch": "各位投資人，想像一下，未來的教育不再是單向的知識灌輸，而是個人化的學習體驗。我們開發的智慧導師系統，正是這場教育革命的先鋒！它不僅能提供客製化的電路分析教學，還能收集學生的學習數據，為教師提供即時回饋，真正實現了『因材施教』。更重要的是，這個系統可以擴展到其他工程領域，甚至整個 STEM 教育領域！我們預計，未來五年內，智慧導師系統將成為大學、職業學校，甚至是企業培訓的標配。這不僅是一個教育產品，更是一個數據驅動的教育平台，擁有巨大的商業潛力。現在加入我們，一起打造智慧教育的未來！", "audio": "docs/data/audios/2506.08962v1.wav"}
{"query": "Foundation Model", "id": "2506.08949v1", "url": "http://arxiv.org/abs/2506.08949v1", "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS.", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:16:08.821228", "title_zh": "SSS：基於高效提示的半監督SAM-2醫學影像分割", "summary_zh": "在醫學影像領域，如何有效利用大量未標記數據，同時減少對高品質像素級標註的依賴，是一大挑戰。本研究提出名為SSS的半監督學習方法，它基於Vision Foundation Models (如SAM-2) 的強大特徵提取能力，挖掘未標記醫學影像中的潛在知識，有效提升全監督醫學影像分割的性能。透過“弱到強”的一致性正規化框架，引入判別式特徵增強機制，探索不同數據增強策略產生的特徵差異，並結合物理約束與滑動窗口機制生成輸入提示，最終在ACDC和BHSD等多標籤數據集上驗證了該方法的優越性，在BHSD數據集上，SSS的Dice平均得分比現有最佳方法高出+3.65。", "applications": ["【更精準的癌症篩檢】想像一下，未來醫院的電腦能自動分析X光片、斷層掃描等影像，找出潛在的微小腫瘤，讓醫生能及早發現並治療癌症，大幅提高患者的存活率。", "【遠距醫療的福音】偏鄉地區醫療資源匱乏，透過這項技術，即使沒有經驗豐富的放射科醫師，也能利用AI輔助診斷，提供更快速、準確的醫療判斷，減少誤診和延誤治療的風險。", "【個人化的健康管理】結合穿戴式裝置的數據，AI能分析個人的生理影像，預測疾病風險，提供個人化的健康建議，例如：提醒你注意心臟健康、預防骨質疏鬆等。"], "pitch": "各位投資人，醫療影像AI的未來，掌握在我們手中！SSS不僅僅是一項技術，它代表著更精準、更普及、更個人化的醫療服務。試想，全球醫療影像市場規模龐大，而我們的SSS技術，能大幅降低醫療影像分析的成本，提高效率，讓更多人能負擔得起高品質的醫療服務。這意味著巨大的商業潛力！我們可以將SSS整合到現有的醫療設備中，授權給醫院和診所使用，甚至可以開發面向消費者的個人健康管理App。更進一步，隨著AI技術的不斷發展，SSS有望實現全自動化的影像診斷，解放醫療人員的勞動力，讓他們能更專注於患者的照護。現在投資SSS，就是投資醫療的未來！讓我們一起打造一個更健康、更美好的世界！", "audio": "docs/data/audios/2506.08949v1.wav"}
{"query": "Diffusion Model", "id": "2506.08677v1", "url": "http://arxiv.org/abs/2506.08677v1", "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection.", "authors": ["Milica Škipina", "Nikola Jovišić", "Nicola Dall'Asen", "Vanja Švenda", "Anil Osman Tur", "Slobodan Ilić", "Elisa Ricci", "Dubravko Ćulibrk"], "published_date": "2025-06-10", "timestamp": "2025-06-11T15:17:21.611749", "title_zh": "MAMBO：用於乳房X光影像的高解析度生成方法", "summary_zh": "乳房X光檢查是乳癌檢測的金標準。本研究提出MAMBO模型，一種基於擴散模型的新穎方法，能生成高解析度的乳房X光影像。MAMBO整合了局部和全域的上下文資訊，生成逼真度極高的乳房X光片，最高可達3840x3840像素。這項技術能克服AI訓練中數據集不足的難題，可用於增強分類模型的訓練，並擴展到異常檢測。實驗證明，MAMBO在影像生成、超解析度和異常檢測方面具有優異的能力，有潛力提升乳房X光分析的準確性，並更早地檢測到病灶。", "applications": ["想像一下，以後在偏鄉地區，醫生可以透過AI生成的乳房X光影像進行初步篩檢，即使沒有足夠的真實案例，也能提供更及時的醫療服務。", "AI生成的乳房X光影像可以幫助醫學院的學生練習判讀，讓他們在接觸真實病人之前，就能累積更多經驗，提升診斷能力。", "未來，我們可以利用這項技術開發個人化的乳房X光風險評估系統，根據AI生成的不同情境，更精準地預測個人罹患乳癌的風險。"], "pitch": "各位投資人，我們正站在醫療AI革命的浪潮之上！乳癌是女性健康的頭號殺手，而早期發現是關鍵。MAMBO技術突破了數據限制，能生成高解析度的乳房X光影像，這不僅能提升現有AI輔助診斷的準確性，更開創了全新的商業模式。試想一下，我們可以將這項技術授權給醫院、診所，甚至開發遠程醫療平台，讓更多女性受益。此外，我們還能與藥廠合作，利用AI生成的數據加速新藥研發。更令人興奮的是，MAMBO的底層技術具有高度通用性，未來可應用於其他醫學影像領域，例如肺部X光、CT掃描等。這是一個千載難逢的投資機會，讓我們一起打造更健康、更智慧的未來！", "audio": "docs/data/audios/2506.08677v1.wav"}
{"query": "AI", "id": "2506.08945v1", "url": "http://arxiv.org/abs/2506.08945v1", "title": "Who is using AI to code? Global diffusion and impact of generative AI", "summary": "Generative coding tools promise big productivity gains, but uneven uptake\ncould widen skill and income gaps. We train a neural classifier to spot\nAI-generated Python functions in 80 million GitHub commits (2018-2024) by\n200,000 developers and track how fast--and where--these tools take hold. By\nDecember 2024, AI wrote an estimated 30.1% of Python functions from U.S.\ncontributors, versus 24.3% in Germany, 23.2% in France, 21.6% in India, 15.4%\nin Russia and 11.7% in China. Newer GitHub users use AI more than veterans,\nwhile male and female developers adopt at similar rates. Within-developer\nfixed-effects models show that moving to 30% AI use raises quarterly commits by\n2.4%. Coupling this effect with occupational task and wage data puts the annual\nvalue of AI-assisted coding in the United States at $9.6-$14.4 billion, rising\nto $64-$96 billion if we assume higher estimates of productivity effects\nreported by randomized control trials. Moreover, generative AI prompts learning\nand innovation, leading to increases in the number of new libraries and library\ncombinations that programmers use. In short, AI usage is already widespread but\nhighly uneven, and the intensity of use, not only access, drives measurable\ngains in output and exploration.", "authors": ["Simone Daniotti", "Johannes Wachs", "Xiangnan Feng", "Frank Neffke"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:18:05.301258", "title_zh": "誰在使用AI編碼？生成式AI的全球擴散與影響", "summary_zh": "這項研究分析了GitHub上8000萬個提交，追蹤生成式AI在程式碼編寫中的應用。截至2024年底，美國開發者使用AI編寫了約30.1%的Python函數，領先於德國、法國、印度、俄羅斯和中國。研究發現，新用戶比老手更常使用AI，且男女開發者採用率相似。AI使用率達到30%可使季度提交量增加2.4%。美國AI輔助編碼的年價值估計為96億至144億美元，若採用更高生產力估計，可達640億至960億美元。此外，AI還促進了學習與創新，增加了程式設計師使用的新函式庫和組合。總之，AI的使用已相當普遍，但高度不均，且使用強度而非存取權限，才是產出和探索的關鍵。", "applications": ["**加速App開發：** 想像一下，你想要開發一個新的手機App，有了AI編碼工具，可以大幅縮短開發時間，更快將你的創意變成現實，搶佔市場先機。", "**客製化網站架設：** 如果你需要一個專屬的個人或公司網站，但又不懂程式碼，AI編碼工具可以根據你的需求自動生成網站程式碼，讓你可以輕鬆擁有專業級的網站。", "**程式碼錯誤偵錯：** 對於程式設計師來說，最頭痛的就是找bug。AI編碼工具可以快速掃描程式碼，找出潛在的錯誤，節省大量的除錯時間，提升工作效率。"], "pitch": "各位創投夥伴，我們正在見證一場程式碼編寫的革命！這項研究證明，生成式AI不僅已經廣泛應用於程式開發，更帶來了顯著的生產力提升。想像一下，一個程式設計師團隊，在AI的輔助下，可以完成過去數倍的工作量，這意味著更快的產品迭代、更低的開發成本，以及更強大的市場競爭力。我們預計，未來五年內，AI編碼工具將成為所有軟體開發團隊的標配，就像現在的雲端服務一樣。我們正在打造下一代的AI編碼平台，結合最先進的AI技術和人性化的使用者介面，目標是成為這個新興市場的領導者。現在加入我們，共同開創AI編碼的黃金時代！我們相信，這項技術將會徹底改變整個軟體產業，帶來巨大的投資回報。", "audio": "docs/data/audios/2506.08945v1.wav"}
{"query": "Foundation Model", "id": "2506.08936v1", "url": "http://arxiv.org/abs/2506.08936v1", "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead.", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:19:30.928461", "title_zh": "BioLangFusion：DNA、mRNA與蛋白質語言模型的多模態融合", "summary_zh": "BioLangFusion 是一種整合 DNA、mRNA 和蛋白質語言模型的簡單方法，能產生統一的分子表示。它基於分子生物學的中心法則，將不同模態的嵌入向量在具有生物意義的密碼子層級對齊，確保跨模態的直接對應。BioLangFusion 研究了三種融合技術：密碼子層級的嵌入向量串聯、基於多實例學習的熵正則化注意力池化，以及跨模態多頭注意力。這些方法無需額外的預訓練或修改基礎模型，即可輕鬆整合現有的基於序列的基礎模型。在五項分子特性預測任務中，BioLangFusion 優於強大的單模態基準模型，表明即使是預訓練模型的簡單融合，也能以最小的開銷捕獲互補的多重組學信息。", "applications": ["個性化醫療：根據你的基因、mRNA 和蛋白質數據，預測你對特定藥物的反應，從而選擇最適合你的治療方案，避免不必要的副作用。", "疾病早期診斷：通過分析血液或體液中的 DNA、mRNA 和蛋白質的變化，在疾病發展的早期階段就能夠檢測到，及早進行干預。", "新藥開發：加速新藥的篩選和設計過程。通過模擬不同的分子組合，預測它們的藥效和安全性，從而節省大量的實驗時間和成本。"], "pitch": "各位創投先進，我們正在開發 BioLangFusion，這是一項革命性的技術，它能將 DNA、mRNA 和蛋白質數據融合，以前所未有的精度預測生物分子的特性。想像一下，我們能夠像解讀程式碼一樣解讀生命密碼，加速新藥開發、實現精準醫療、甚至預測疾病的發生。這不僅僅是一項技術，而是一個巨大的市場機會。目前藥物開發的成功率極低，耗時長且成本高昂。BioLangFusion 能大幅降低藥物開發的風險和成本，加速新藥上市。更重要的是，我們正在構建一個平台，讓研究人員、藥廠和醫療機構都能夠利用我們的技術，共同推動生命科學的發展。未來的醫療將是預測性的、個性化的，而 BioLangFusion 將是實現這一願景的關鍵。我們相信，BioLangFusion 有潜力成为生物科技领域的下一个独角兽，为投资者带来丰厚的回报。現在加入我們，一起改變世界！", "audio": "docs/data/audios/2506.08936v1.wav"}
{"query": "Diffusion Model", "id": "2506.08632v1", "url": "http://arxiv.org/abs/2506.08632v1", "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning.", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "published_date": "2025-06-10", "timestamp": "2025-06-11T18:20:52.051991", "title_zh": "RoboSwap：一個基於GAN驅動的影片擴散框架，用於無監督機器手臂替換", "summary_zh": "RoboSwap是一個創新的影片編輯框架，旨在解決機器人學習中跨平台泛化的數據稀缺問題。它結合了GAN和擴散模型，無需配對的影片數據，即可將一個影片中的機器手臂替換成另一個。首先，系統會分割出手臂，利用GAN模型進行手臂轉換，然後將轉換後的手臂融合回原始影片背景，並使用擴散模型來增強連貫性、運動真實感和物體互動。實驗證明，RoboSwap在結構連貫性和運動一致性方面優於現有技術，為機器人學習生成可靠的跨平台數據提供了一個強大的解決方案。這將大幅降低機器人學習的數據收集成本，加速相關技術的發展。", "applications": ["遠端醫療手術訓練：醫生可以利用RoboSwap生成不同類型機器手臂在手術中的模擬影片，進行更全面的訓練，無需實際操作各種昂貴的設備。", "工廠自動化流程設計：工程師可以快速模擬不同機器手臂在生產線上的工作情況，優化流程設計，提高生產效率，而無需耗時地進行物理原型測試。", "虛擬實境遊戲開發：遊戲開發者可以使用RoboSwap輕鬆創建各種機器人角色，並模擬它們的動作，豐富遊戲內容，提升玩家的沉浸式體驗。"], "pitch": "各位投資人，想像一下，未來機器人無所不在，從工廠到醫院，從家庭到太空。但要讓它們真正聰明，需要大量的訓練數據。RoboSwap的出現，徹底改變了機器人學習的遊戲規則！它就像機器人界的Photoshop，能以極低的成本，生成各種機器手臂的影片數據，讓AI快速學習。這意味著，我們不再需要花費巨額資金和時間去收集真實數據，就能訓練出更強大的機器人。這項技術的潛在市場規模極其龐大，涵蓋工業自動化、醫療機器人、虛擬實境等多個領域。我們相信，RoboSwap將成為機器人時代的關鍵基礎設施，而現在就是投資這個潛力獨角獸的最佳時機！", "audio": "docs/data/audios/2506.08632v1.wav"}
{"query": "AI", "id": "2506.08935v1", "url": "http://arxiv.org/abs/2506.08935v1", "title": "Can A Gamer Train A Mathematical Reasoning Model?", "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "authors": ["Andrew Shin"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:12:34.399500", "title_zh": "電競玩家能訓練出數學推理模型嗎？", "summary_zh": "本研究證明，即使使用一張普通的電競級GPU，也能訓練出強大的數學推理模型。透過結合強化學習和記憶體優化技術，我們在僅有16GB記憶體的RTX 3080 Ti上，訓練出一個15億參數的模型，其數學推理能力甚至超越了許多更大的模型。這項成果挑戰了「頂尖數學推理需要龐大運算資源」的傳統觀念，有助於普及高效能AI研究，讓更多人能參與其中，降低AI開發門檻。", "applications": ["輔助學生學習：學生在家也能利用一般電腦，執行AI數學解題模型，獲得更個人化的學習輔導。", "金融風險評估：小型金融機構或個人投資者，能用更低成本的AI模型進行風險分析，做出更明智的決策。", "科學研究加速：研究人員在資源有限的情況下，也能快速訓練AI模型，加速科學發現的過程。"], "pitch": "想像一下，過去需要Google等級的超級電腦才能做到的數學推理，現在一台電競主機就能搞定！這項技術打破了AI發展的硬體限制，讓AI不再是少數科技巨頭的專利。我們正在打造一個AI民主化的未來，讓各行各業、甚至個人，都能輕鬆駕馭AI的力量。這不僅能加速科學研究、提升教育品質，更能催生出無數創新應用。例如，我們預期未來每個人都能擁有自己的AI數學助理，解決生活中的各種難題。現在投資我們，您將站在AI革命的最前線，共同開創這個無限可能的未來！", "audio": "docs/data/audios/2506.08935v1.wav"}
{"query": "Foundation Model", "id": "2506.08902v1", "url": "http://arxiv.org/abs/2506.08902v1", "title": "Intention-Conditioned Flow Occupancy Models", "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:14:07.747235", "title_zh": "意圖條件化流佔據模型", "summary_zh": "本研究提出一種名為「意圖條件化流佔據模型」(InFOM) 的強化學習預訓練方法。InFOM利用流匹配技術，預測智能體在未來可能訪問的狀態分布（佔據度量）。模型中加入潛在變數捕捉使用者意圖，提升表達能力並實現廣義策略改進。實驗結果顯示，InFOM在36個基於狀態和4個基於圖像的基準測試任務中，相較於其他預訓練方法，平均回報提升1.8倍，成功率提升36%。InFOM為強化學習領域的大規模預訓練提供了一個有效途徑，有助於解決樣本效率和穩健性等核心挑戰。", "applications": ["智慧家庭：想像一下，你的智慧家庭不再只是被動執行指令，而是能預測你的需求。InFOM技術能讓智慧家庭系統學習你的生活習慣（意圖），提前調整燈光、溫度，甚至準備咖啡，打造更舒適便利的生活。", "自動駕駛：自動駕駛汽車可以利用InFOM預測其他車輛或行人的行為模式，例如預測行人是否打算穿越馬路。這能提升自動駕駛系統的安全性，減少事故發生。", "個人化醫療：InFOM可以用於預測病患的健康狀態發展趨勢。透過分析病患的病史、生活習慣等資訊（意圖），預測未來可能出現的健康問題，以便及早介入治療，達到個人化醫療的效果。"], "pitch": "各位投資人，我們正站在AI發展的下一個浪潮前沿！InFOM技術，意圖條件化流佔據模型，是強化學習領域的革命性突破，它將徹底改變機器與環境互動的方式。想像一下，不再需要耗費大量時間和資源訓練AI適應新環境，InFOM讓AI具備預測未來、理解使用者意圖的能力，從而實現更高效、更智能的決策。這意味著什麼？\n\n首先，在自動駕駛領域，InFOM將使汽車不僅能「看到」周圍環境，更能「預測」其他車輛和行人的意圖，大幅提升安全性，加速自動駕駛的普及。\n\n其次，在智慧製造領域，InFOM能讓機器人預測生產線的潛在問題，優化生產流程，降低成本，提高效率，助力企業實現智慧轉型。\n\n更重要的是，InFOM的潛力遠不止於此。它還可以應用於金融、醫療、能源等各個領域，例如預測股市走勢、診斷疾病、優化能源分配等等。這是一個數十億美元的市場，而我們擁有領先的技術和強大的團隊，有信心在這個市場中佔據主導地位。\n\n現在正是投資InFOM的最佳時機！我們需要您的資金，加速技術開發，擴大市場規模，共同打造一個更智能、更美好的未來！", "audio": "docs/data/audios/2506.08902v1.wav"}
{"query": "Diffusion Model", "id": "2506.08617v1", "url": "http://arxiv.org/abs/2506.08617v1", "title": "Diffusion model for analyzing quantum fingerprints in conductance fluctuation", "summary": "A conditional diffusion model has been developed to analyze intricate\nconductance fluctuations called universal conductance fluctuations or quantum\nfingerprints appearing in quantum transport phenomena. The model reconstructs\nimpurity arrangements and quantum interference patterns in nanometals by using\nmagnetoconductance data, providing a novel approach to analyze complex data\nbased on machine learning. In addition, we visualize the attention weights in\nthe model, which efficiently extract information on the non-local correlation\nof the electron wave functions, and the score functions, which represent the\nforce fields in the wave-function space.", "authors": ["Naoto Yokoi", "Yuki Tanaka", "Yukito Nonaka", "Shunsuke Daimon", "Junji Haruyama", "Eiji Saitoh"], "published_date": "2025-06-10", "timestamp": "2025-06-11T21:15:24.879224", "title_zh": "用於分析電導波動中量子指紋的擴散模型", "summary_zh": "本研究開發了一種條件擴散模型，用於分析量子傳輸現象中複雜的電導波動，又稱通用電導波動或量子指紋。該模型利用磁電導數據重建奈米金屬中的雜質排列和量子干涉圖樣，為基於機器學習的複雜數據分析提供了一種新穎方法。此外，我們可視化了模型中的注意力權重，有效地提取了電子波函數的非局部相關信息，以及表示波函數空間中力場的得分函數。", "applications": ["想像一下，我們可以利用這個技術來分析演唱會場地的音響效果。透過測量不同位置的聲音波動，就能重建場地內部的聲波干涉模式，從而優化音響設備的擺放，讓每個角落的聽眾都能享受最佳音質。", "這項技術也能應用於醫療領域，分析腦電圖（EEG）數據。透過重建大腦中的神經活動模式，醫生可以更精準地診斷癲癇等神經系統疾病，並制定更有效的治療方案。", "在材料科學領域，我們可以利用這個模型分析材料表面的電導波動，從而了解材料內部的微觀結構和缺陷分布。這有助於開發更高性能的半導體材料和更耐用的複合材料。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，它能解鎖微觀世界的秘密，並將其轉化為巨大的商業價值！我們的擴散模型不僅能分析量子指紋，更像是一把萬能鑰匙，可以解開複雜系統中隱藏的模式。想像一下，我們可以利用它來開發新一代的材料，其性能超越現有材料的極限，例如超導材料、更高效的太陽能電池等。在醫療領域，這項技術將帶來精準醫療的革命，幫助我們更早、更準確地診斷疾病。更重要的是，這項技術具有無限的潛力，可以應用於金融、氣象預測等各個領域。現在投資，您將成為這場科技革命的先驅，共同開創一個全新的時代！我們預計在五年內，這項技術將帶來數十億美元的市場價值，並改變我們生活的方方面面。", "audio": "docs/data/audios/2506.08617v1.wav"}
{"query": "AI", "id": "2506.09002v2", "url": "http://arxiv.org/abs/2506.09002v2", "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "published_date": "2025-06-10", "timestamp": "2025-06-12T00:56:04.097589", "title_zh": "透過混合程式分析與大型語言模型提升Rust單元測試覆蓋率", "summary_zh": "本研究提出PALM，一種利用大型語言模型（LLMs）提升單元測試覆蓋率的方法。PALM透過程式分析找出函數中的分支條件，將其轉化為路徑約束，並結合相關上下文資訊，引導LLMs生成單元測試。實驗結果顯示，PALM在短時間內能顯著提升測試覆蓋率，在某些情況下，整體專案覆蓋率提高超過50%，平均覆蓋率達到75.77%，與人工測試相當。此方法展現了LLMs在自動化測試生成方面的潛力，並已成功向開源專案提交多個測試案例，大幅提升程式碼品質與可靠性。", "applications": ["自動駕駛系統：確保自動駕駛程式碼在各種路況下的安全性，降低事故風險。", "醫療設備軟體：驗證醫療設備軟體的準確性，避免因程式錯誤導致的誤診或治療失誤。", "金融交易系統：測試金融交易系統的穩定性，防止因程式漏洞造成的財務損失。"], "pitch": "各位投資人，我們正在打造軟體測試的未來！現今軟體開發週期中，測試往往是耗時且昂貴的環節。PALM，我們的創新技術，結合了程式分析和大型語言模型，能夠以前所未有的效率和準確性自動生成單元測試。想像一下，開發者不再需要花費大量時間編寫測試程式，而是可以將精力集中在核心功能的開發上。這不僅能大幅縮短開發週期，降低成本，更能確保軟體的品質和安全性。未來，PALM將成為所有軟體開發團隊不可或缺的工具，從小型新創公司到大型企業，都將受益於PALM帶來的效率提升和風險降低。我們預計，隨著AI技術的持續發展，PALM的應用範圍將不斷擴大，甚至可以應用於程式碼自動修復，成為真正的AI程式碼醫生。現在加入我們，共同開創軟體測試的新紀元！", "audio": "docs/data/audios/2506.09002v2.wav"}
{"query": "Foundation Model", "id": "2506.09022v2", "url": "http://arxiv.org/abs/2506.09022v2", "title": "Do Multiple Instance Learning Models Transfer?", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "published_date": "2025-06-10", "timestamp": "2025-06-12T00:57:13.228162", "title_zh": "多重實例學習模型可以轉移嗎？", "summary_zh": "多重實例學習（MIL）是計算病理學中，從大型組織圖像生成有意義的切片層級嵌入的基石方法。然而，MIL常在小型、弱監督的臨床數據集上遇到困難。本研究系統性地評估了預訓練MIL模型的遷移學習能力，結果顯示，即使在與目標任務不同的器官上訓練，預訓練模型仍始終優於從頭開始訓練的模型。在泛癌數據集上進行預訓練，更能實現跨器官和任務的強大泛化，勝過切片基礎模型，同時使用更少的預訓練數據。這突顯了MIL模型的強大適應性，並展示了利用遷移學習提高計算病理學性能的好處。我們提供了一個資源，標準化了MIL模型的實現和常用計算病理學任務的預訓練模型權重集合。", "applications": ["**癌症診斷輔助：** 想像一下，醫生可以透過AI快速分析病理切片，找出潛在癌細胞，提高診斷準確性，減少誤判，讓病人及早接受治療。", "**新藥開發加速：** 藥廠可以利用AI分析大量病理圖像，找出對特定疾病有效的藥物標靶，加速新藥開發流程，讓更多病人受益。", "**個人化醫療：** 透過AI分析病人的病理切片，預測疾病發展趨勢，制定更精準的治療方案，實現個人化醫療，提高治療效果。"], "pitch": "各位創投夥伴，我們正在開發一項革命性的AI技術，它能大幅提升癌症診斷的準確性和效率。我們的多重實例學習模型，就像一位經驗豐富的病理學家，能從海量的病理圖像中快速找出關鍵資訊，協助醫生做出更明智的決策。這項技術不僅能降低醫療成本，更能挽救無數生命。想像一下，未來每個醫院都能配備這樣一套AI系統，讓癌症不再是絕症，而是可以有效控制的疾病。我們相信，這項技術將顛覆醫療產業，創造巨大的商業價值。現在正是投資的絕佳時機，讓我們一起攜手打造更健康的未來！", "audio": "docs/data/audios/2506.09022v2.wav"}
{"query": "AI", "id": "2506.09988v1", "url": "http://arxiv.org/abs/2506.09988v1", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:44:10.759850", "title_zh": "EditInspector：一個用於評估文本引導圖像編輯的基準", "summary_zh": "隨著生成式AI的快速發展，文本引導的圖像編輯越來越普及。然而，如何驗證這些編輯的品質成為一個重要問題。我們推出EditInspector，一個基於人類標注的新基準，用於全面評估文本引導的圖像編輯。EditInspector可以評估編輯的準確性、偽影檢測、視覺品質、與場景的融合度、常識一致性以及描述編輯變化的能力。研究發現，現有模型在全面評估編輯方面存在困難，並且在描述變化時容易產生幻覺。為了解決這些問題，我們提出了兩種新方法，在偽影檢測和差異字幕生成方面優於現有模型。", "applications": ["想像一下，你可以用文字描述想要修改的照片，例如『把天空變成夕陽』，這個技術就能自動完成，讓你的照片更具藝術感，輕鬆在社群媒體上獲得更多讚。", "室內設計師可以利用這個技術，快速預覽不同風格的家具擺設在客戶家中的效果，例如『把沙發換成L型皮沙發』，讓客戶更直觀地了解設計方案，提高成交率。", "新聞媒體可以利用這個技術，快速生成示意圖，例如『在街道上增加人群』，更生動地呈現新聞事件，提升報導的吸引力。"], "pitch": "各位投資人，我們團隊開發的EditInspector技術，正迎合了生成式AI圖像編輯爆發性成長的需求。現有圖像編輯工具缺乏有效的品質驗證機制，導致使用者體驗不佳，甚至產生誤導。EditInspector提供了一個客觀、全面的評估標準，能有效提升圖像編輯的可靠性和可用性。我們的技術不僅能應用於現有的圖像編輯軟體，更可整合至廣告設計、電商產品展示、遊戲開發等領域，市場潛力巨大。想像一下，未來使用者可以透過簡單的文字指令，精準控制圖像內容，創造出無限的商業價值。我們預計，EditInspector將成為圖像編輯領域的黃金標準，為投資者帶來豐厚的回報。我們誠摯邀請您加入，共同開創圖像編輯的新紀元！", "audio": "docs/data/audios/2506.09988v1.wav"}
{"query": "Foundation Model", "id": "2506.09982v1", "url": "http://arxiv.org/abs/2506.09982v1", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "published_date": "2025-06-11", "timestamp": "2025-06-12T03:45:31.224163", "title_zh": "AnimateAnyMesh：用於文本驅動通用網格動畫的前饋式4D基礎模型", "summary_zh": "AnimateAnyMesh是一個創新的前饋式框架，能根據文字描述快速生成任意3D模型的動畫。它利用獨特的DyMeshVAE架構，有效壓縮和重建動態網格序列，同時分離空間和時間特徵。透過修正流訓練策略，模型能產生高品質、符合文字描述的動畫。研究團隊更釋出包含超過400萬個動態網格序列的DyMesh資料集。實驗證明，AnimateAnyMesh在幾秒內就能生成語義準確、時間連貫的網格動畫，在品質和效率上都超越現有方法，使4D內容創作更易於使用。", "applications": ["遊戲開發：遊戲設計師只需輸入文字描述，即可快速生成角色動畫，例如「殭屍緩慢行走」、「英雄快速奔跑」等，大幅縮短開發時間。", "虛擬實境（VR/AR）：使用者可以透過文字指令，讓虛擬化身做出各種動作，例如「開心跳舞」、「悲傷哭泣」，增加互動的真實感和趣味性。", "教育應用：學生可以輸入文字描述，讓3D模型展示特定動作或物理現象，例如「心臟跳動」、「行星繞太陽運轉」，幫助理解抽象概念。"], "pitch": "各位投資人，想像一下，未來不再需要昂貴的動畫工作室和專業動畫師，只需輸入簡單的文字描述，就能創造出逼真的3D動畫！AnimateAnyMesh正是實現這一願景的關鍵技術。它不僅能大幅降低動畫製作成本，更能開啟無限的商業可能性。我們可以將它應用於遊戲、VR/AR、教育、廣告等各個領域。例如，與電商平台合作，讓消費者透過文字指令，看到商品在不同情境下的動態展示；與影視公司合作，快速生成特效動畫，降低製作成本。更進一步，我們可以將這項技術與AI助手整合，讓使用者透過語音指令，輕鬆創造個性化的3D動畫內容。這不僅僅是一個動畫工具，更是一個革命性的4D內容創作平台，具有巨大的市場潛力。現在投資，您將成為這場變革的領跑者，共同開創4D內容的黃金時代！", "audio": "docs/data/audios/2506.09982v1.wav"}
{"query": "AI", "id": "2506.09985v1", "url": "http://arxiv.org/abs/2506.09985v1", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:19:17.943286", "title_zh": "V-JEPA 2：自我監督式影片模型實現理解、預測與規劃", "summary_zh": "V-JEPA 2 是一種透過觀看大量網路影片和少量機器人互動資料，來學習理解世界並採取行動的自我監督式模型。它在動作理解和人類行為預測方面表現出色，甚至超越了專門為這些任務設計的模型。透過與大型語言模型的結合，V-JEPA 2 在影片問答方面也達到了領先水平。更重要的是，它能利用從機器人收集的少量無標籤影片，進行機器人路徑規劃，並在不同實驗室的機器人手臂上實現物件的拾取和放置，無需額外訓練或獎勵。這證明了透過自我監督學習，模型能夠理解物理世界並制定相應的行動計畫。", "applications": ["智慧家庭：想像一下，你的智慧冰箱能透過觀察你打開冰箱的動作，預測你接下來想做什麼，並提前準備好食材，甚至幫你規劃更健康的飲食。", "自動駕駛：讓自動駕駛系統不僅能識別紅綠燈和行人，還能預測其他車輛或行人的意圖，例如判斷行人是否打算穿越馬路，從而做出更安全、更流暢的駕駛決策。", "遠程醫療：醫生可以透過觀看病患在家中的影片，更準確地判斷病患的病情，並提供更個性化的治療建議，尤其對於行動不便或居住偏遠地區的病患來說，這將極大地提升醫療服務的可及性。"], "pitch": "各位投資人，我們相信 V-JEPA 2 代表了人工智慧發展的下一個重大突破。它不僅僅是一個影片分析工具，而是一個能夠像人類一樣理解世界、預測未來並制定行動計畫的通用智能模型。試想一下，將 V-JEPA 2 應用於工業自動化，它可以讓機器人自主完成複雜的組裝和維修任務，大幅降低生產成本，提高效率。在娛樂產業，它可以生成逼真且互動性強的虛擬世界，為用戶帶來前所未有的沉浸式體驗。更令人興奮的是，V-JEPA 2 有望成為通用人工智慧的基石，催生出我們今天難以想像的全新應用。我們正在尋找具有遠見卓識的合作夥伴，共同將 V-JEPA 2 的潛力轉化為現實，引領下一波人工智慧浪潮。現在投資，您將站在這場革命的最前沿！", "audio": "docs/data/audios/2506.09985v1.wav"}
{"query": "Foundation Model", "id": "2506.09883v1", "url": "http://arxiv.org/abs/2506.09883v1", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "published_date": "2025-06-11", "timestamp": "2025-06-12T06:20:43.147567", "title_zh": "透過幾何蒸餾微調具備3D感知能力的視覺語言模型", "summary_zh": "本研究提出幾何蒸餾，一種輕量級、無需標註的微調框架，將人類啟發的幾何線索注入預訓練的視覺語言模型（VLMs），無需修改其架構。透過從現成的3D基礎模型（如MASt3R、VGGT）中提取稀疏對應關係、相對深度關係和密集成本體積，我們的模型塑造出具備幾何感知能力的表示，同時保持與自然圖像文本輸入的相容性。在3D視覺語言推理和3D感知基準測試中，我們的模型始終優於先前的方法，以顯著降低的計算成本實現了改進的3D空間推理。這為2D訓練的VLMs與3D理解的橋樑提供了一個可擴展且高效的路徑，開闢了在空間定位多模態任務中更廣泛的應用。", "applications": ["**智慧導航：**想像一下，你的手機不只告訴你該往哪裡走，還能真正『看懂』周圍環境，避開障礙物、判斷路面狀況，甚至在你快要撞到東西時發出警告。這就像有個超強的3D導航員，讓你走路、開車更安全。", "**虛擬試穿/試妝：**網購衣服或化妝品時，總是擔心不合適？有了這項技術，你可以用手機鏡頭即時『試穿』衣服、『試用』口紅，3D感知能力讓效果更逼真，就像真的穿在你身上一樣，大幅降低退貨率。", "**智慧家庭：**掃地機器人不再笨手笨腳，它能真正理解家裡的空間佈局，精準避開障礙物、清掃死角。你甚至可以用語音指令，讓它去特定位置（例如『客廳沙發底下』）清掃，讓生活更便利。"], "pitch": "各位投資人，我們正在打造下一代AI的『空間智慧』！現有的視覺語言模型雖然強大，但缺乏對3D世界的理解，就像一個色盲的畫家。我們的幾何蒸餾技術，能以極低的成本賦予這些模型『3D視覺』，讓它們真正『看懂』周圍的世界。想像一下，這項技術能應用在自動駕駛、機器人、AR/VR等領域，徹底改變人機互動的方式。隨著元宇宙的發展，對3D空間理解的需求將會爆發式增長，我們的技術將成為這個新世界的基石。現在投資我們，就是投資AI的未來，一個充滿『空間智慧』的未來！我們預期在五年內，搭載我們技術的產品將佔據智慧導航、虛擬試穿和智慧家庭市場的領先地位，帶來數十億美元的營收。", "audio": "docs/data/audios/2506.09883v1.wav"}
{"query": "AI", "id": "2506.09977v1", "url": "http://arxiv.org/abs/2506.09977v1", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:14:46.718441", "title_zh": "人們如何修正不一致的信念？透過使用者研究檢驗人類的信念修正", "summary_zh": "本研究探討人類在接收到新資訊時如何修正自身信念，這對於開發能有效模擬人類推理的AI系統至關重要。研究透過三項使用者研究發現，人們傾向於基於解釋進行信念修正，即使這可能導致與傳統信念變更理論不符的結果。無論解釋是由系統提供還是由使用者自行產生，人們都展現出對看似非最小修正的強烈偏好。這些發現暗示，AI系統在模擬人類推理或與人類互動時，應考慮基於解釋的、可能非最小的信念修正操作，以更好地與人類認知過程對齊。", "applications": ["AI心理諮商：當AI偵測到使用者信念與事實不符時，不是直接更正，而是提供解釋，引導使用者自行修正，減少反彈。", "個人化教育：根據學生的既有知識和信念，提供客製化的學習內容和解釋，幫助他們更有效地理解新概念，並修正錯誤觀念。", "智能客服：在處理客戶投訴時，智能客服不僅提供解決方案，更重要的是解釋問題發生的原因，以及解決方案的合理性，讓客戶更容易接受。"], "pitch": "各位投資人，想像一下，我們正在打造的不僅僅是AI，而是真正理解人類思考方式的AI！傳統AI在面對矛盾資訊時，往往依賴邏輯運算，忽略了人類更看重『解釋』的心理。我們的技術，基於使用者研究，讓AI能夠像人一樣，透過提供或引導產生解釋，來修正信念。這意味著什麼？更人性化的AI互動、更有效的知識傳遞，以及更強大的決策支持系統！試想，未來的AI醫療診斷，不僅告訴你得了什麼病，還會解釋病因和治療方案的依據，讓你安心接受治療。在金融投資領域，AI不僅提供投資建議，還會解釋背後的邏輯，讓你更放心地參與市場。這是一個百億美元級別的市場，我們擁有領先的技術和使用者數據，現在加入我們，一起打造真正懂你的AI，引領AI的下一個時代！", "audio": "docs/data/audios/2506.09977v1.wav"}
{"query": "Foundation Model", "id": "2506.09881v1", "url": "http://arxiv.org/abs/2506.09881v1", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:16:21.178865", "title_zh": "利用深度與語言實現開放詞彙領域泛化語義分割", "summary_zh": "本研究提出一個名為Vireo的全新單階段框架，旨在解決開放詞彙領域泛化語義分割（OV-DGSS）問題。OV-DGSS的目標是在未見過的領域中，為未見過的類別生成像素級別的遮罩，這對於在惡劣條件下的自動駕駛等真實場景至關重要。Vireo基於凍結的視覺基礎模型（VFMs），並通過深度VFMs整合場景幾何信息，以提取領域不變的結構特徵。為了彌合領域轉移下視覺和文本模態之間的差距，Vireo提出了GeoText提示、粗略遮罩先驗嵌入（CMPE）和領域-開放詞彙向量嵌入頭（DOV-VEH）等關鍵組件。實驗結果表明，Vireo在領域泛化和開放詞彙識別方面均優於現有方法，為多樣化和動態環境中的穩健視覺理解提供了一個統一且可擴展的解決方案。", "applications": ["自動駕駛系統：讓汽車即使在雨天、霧天或不同國家/地區，也能準確辨識行人、車輛、交通號誌等，提升行車安全。", "智慧城市監控：即使在光線不足或有遮蔽物的情況下，也能辨識異常事件（例如：跌倒、打架），及時發出警報。", "醫療影像分析：協助醫生辨識X光、CT掃描等影像中未曾見過的病灶或組織結構，提高診斷準確性。"], "pitch": "各位投資人，我們正處於AI視覺革命的前沿！Vireo不僅僅是一個語義分割模型，它代表著AI理解世界方式的根本性突破。想像一下，一個AI能夠像人類一樣，在任何環境下、看到任何東西都能理解。這就是Vireo的潛力！\n\n自動駕駛只是起點。我們的技術可以應用於智慧城市、醫療診斷、工業自動化，甚至太空探索！Vireo讓機器人能夠在完全陌生的環境中工作，讓AI能夠分析前所未有的數據，帶來無限可能。\n\n現有的AI在面對新環境或新物體時往往束手無策，但Vireo克服了這個瓶頸。我們相信，Vireo將成為未來AI視覺領域的基石，帶來巨大的商業價值。現在投資Vireo，就是投資AI的未來！我們預計在五年內，Vireo將成為各行業AI視覺解決方案的標準，為投資者帶來豐厚的回報。讓我們一起開創AI視覺的新時代！", "audio": "docs/data/audios/2506.09881v1.wav"}
{"query": "Diffusion Model", "id": "2506.09932v1", "url": "http://arxiv.org/abs/2506.09932v1", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "published_date": "2025-06-11", "timestamp": "2025-06-12T09:17:45.365176", "title_zh": "HadaNorm：透過中心化轉換的擴散轉換器量化", "summary_zh": "擴散模型在圖像生成領域表現出色，但其高記憶體和計算需求限制了在資源有限設備上的部署。後訓練量化(PTQ)通過降低矩陣運算的位寬提供了解決方案。然而，標準PTQ方法難以處理異常值，並且實現更高的壓縮通常需要在量化之前轉換模型權重和激活。本研究提出了一種新穎的線性轉換HadaNorm，它通過在應用Hadamard轉換之前對激活特徵通道進行歸一化，有效地緩解異常值，從而實現更激進的激活量化。實驗證明，HadaNorm始終如一地減少了轉換器模塊各個組件的量化誤差，與最先進的方法相比，實現了卓越的效率-性能權衡。", "applications": ["手機修圖App：讓低階手機也能流暢使用AI修圖功能，快速生成高品質圖片，不再卡頓。", "智慧監控系統：在攝影機端直接進行AI分析，即時辨識異常狀況，無需將大量影像傳回雲端，節省頻寬和電力。", "AI藝術創作：讓藝術家在平板電腦上就能進行複雜的AI繪圖和生成，隨時隨地激發創作靈感，不受硬體限制。"], "pitch": "各位創投大家好，我們正在打造AI普及化的未來！HadaNorm技術，讓原本只有高階伺服器才能運行的AI圖像生成模型，現在可以在手機、物聯網裝置上流暢運行。想像一下，未來每個人都能用手機輕鬆生成高品質的客製化圖像，智慧監控系統可以更快速、更節能地保障安全，AI藝術創作不再受限於昂貴的硬體設備。這不僅僅是技術突破，更是AI應用的大爆發！我們將授權HadaNorm技術給各大晶片廠、手機廠商、安防企業，打造AI生態系統。初期鎖定手機修圖、智慧監控、AI藝術創作三大市場，預計三年內達到數億美元的營收。現在投資我們，您將參與AI普及化的浪潮，共同打造一個更智慧、更便捷的未來！", "audio": "docs/data/audios/2506.09932v1.wav"}
{"query": "AI", "id": "2506.09975v1", "url": "http://arxiv.org/abs/2506.09975v1", "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:31:55.244924", "title_zh": "當偵測失效時：微調模型生成如人類般的社群媒體文本的力量", "summary_zh": "本研究探討偵測AI生成的社群媒體文本的難題。由於社群媒體文本短小、非正式且充滿獨特性，使得偵測更加困難。然而，這項研究至關重要，因為社群媒體是網路影響力活動的重要攻擊媒介。研究團隊創建了一個包含超過50萬個AI生成社群媒體貼文的數據集，涵蓋11個爭議性議題。結果顯示，在攻擊者不公開其微調模型的情況下，偵測效果大幅下降。人類研究也證實了這一點。這意味著現有的偵測演算法很容易受到微調LLM的攻擊。這項研究結果對所有偵測領域都具有重要意義，因為微調是一種普遍適用且實際的LLM使用案例。", "applications": ["想像一下，新聞媒體可以用AI來分析大量社群媒體上的輿情，快速掌握民眾對特定議題的看法，並即時調整報導方向，更貼近民意。", "公司可以利用這項技術，辨識出競爭對手是否使用AI生成內容來進行惡意行銷或散播不實訊息，及早採取應對措施，維護品牌聲譽。", "政府單位可以監測網路上的極端言論或煽動性內容，預防潛在的社會動亂，並採取相應的維穩措施，確保社會安定。"], "pitch": "各位投資人，我們正站在一個AI軍備競賽的風口浪尖！AI生成內容的能力日益強大，但我們偵測這些內容的能力卻遠遠落後。這項研究揭示了微調模型在規避偵測方面的驚人力量，這意味著網路上的假訊息和輿論操縱將變得更加難以辨識。我們的解決方案不僅僅是一個偵測工具，更是一個預警系統，能夠幫助企業、政府和個人在資訊戰中保持領先。想像一下，我們可以利用這項技術打造一個『AI內容真實性評估平台』，為新聞媒體、社群平台和廣告商提供可信賴的內容驗證服務。隨著AI生成內容的普及，對真實性驗證的需求將會爆炸性增長，這將是一個數十億美元的市場！現在投資我們，就是投資於未來的資訊安全，成為這場AI軍備競賽的領跑者！我們甚至可以將這項技術應用於Deepfake影片的檢測，保護公眾人物和企業免受惡意攻擊。未來，我們還可以開發更先進的AI防禦系統，主動干擾和瓦解AI生成的假訊息攻勢，建立一個更健康、更真實的網路環境。", "audio": "docs/data/audios/2506.09975v1.wav"}
{"query": "Foundation Model", "id": "2506.09855v1", "url": "http://arxiv.org/abs/2506.09855v1", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:33:08.220647", "title_zh": "基於基礎模型的深度強化學習於 RIS 輔助無線通訊", "summary_zh": "本研究提出一個創新的框架，利用預訓練的開源基礎模型（FM），即大型無線模型（LWM），來處理無線通道並產生多功能且上下文相關的通道嵌入。這些嵌入用於聯合優化基地台波束成形和 RIS 配置。具體來說，我們設計了一個深度強化學習（DRL）模型來自動選擇基地台波束成形向量和 RIS 相移矩陣，旨在最大化頻譜效率（SE）。結果表明，用於無線電訊號理解的預訓練 FM 可以進行微調，並與 DRL 集成，從而在無線網路中做出有效的決策。模擬結果顯示，所提出的方法優於基於 DRL 的方法和基於波束掃描的方法，分別實現了 9.89% 和 43.66% 的頻譜效率提升。", "applications": ["想像一下，在人潮擁擠的體育場或演唱會現場，手機訊號不再卡頓。這項技術就像是個聰明的交通警察，能動態調整無線訊號的方向，確保每個人都能順暢地直播、傳照片或打卡。", "在智慧家居中，這項技術可以優化 Wi-Fi 訊號，讓家裡的每個角落都有穩定的網路連線，無論你在哪個房間，都能享受流暢的影音體驗，再也不用煩惱訊號死角。", "在自動駕駛汽車領域，這項技術能確保車輛之間、車輛與基礎設施之間的通訊暢通無阻。即使在複雜的城市環境中，也能提供可靠的導航和安全警示，提升行車安全。"], "pitch": "各位投資人，我們正在開發一種革命性的無線通訊技術，它能大幅提升網路效率，降低營運成本，並開啟全新的商業模式。想像一下，透過我們的技術，電信營運商能以更少的基地台覆蓋更大的範圍，提供更穩定的網路服務，從而節省數十億美元的基礎建設成本。更重要的是，這項技術為5G、6G甚至未來的無線通訊技術奠定了基礎，它將是智慧城市、物聯網、自動駕駛等領域的關鍵推動力量。我們預計，在未來五年內，這項技術的市場規模將達到數百億美元，而我們將成為這個市場的領導者。現在加入我們，共同打造無線通訊的未來！", "audio": "docs/data/audios/2506.09855v1.wav"}
{"query": "Diffusion Model", "id": "2506.09740v1", "url": "http://arxiv.org/abs/2506.09740v1", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T12:34:17.753186", "title_zh": "ELBO-T2IAlign：一種基於ELBO的通用方法，用於校準擴散模型中的像素級文字-圖像對齊", "summary_zh": "本研究發現，現有擴散模型在處理小型、遮擋或罕見物體時，文字與圖像的像素級對齊存在偏差。為了解決這個問題，我們提出了一種名為ELBO-T2IAlign的創新方法。它基於證據下界(ELBO)，無需訓練，即可有效校準擴散模型中的文字-圖像對齊。此方法通用性強，適用於各種擴散模型架構，並已在圖像分割和生成等基準數據集上驗證了其有效性。這項技術有助於提升圖像編輯、合成圖像生成等下游任務的性能。", "applications": ["想像一下，你可以用文字精準地編輯照片，比如把照片裡的小狗換成指定品種，而且細節完美無瑕，就像真的在那裡一樣。", "以後設計師可以透過輸入文字描述，快速生成各種風格的產品原型圖，省去大量手繪和建模的時間，加速產品開發流程。", "醫療影像分析領域，醫生可以透過文字引導，更精確地標記和分析X光片或MRI圖像中的病灶，提高診斷準確性。"], "pitch": "各位投資人，我們帶來的是一項革命性的圖像生成技術——ELBO-T2IAlign。它能精準校準文字與圖像的像素級對應關係，解決了AI圖像生成領域長期存在的對齊問題。這意味著，我們能創造出前所未有、高度精確且符合使用者意圖的圖像。想像一下，未來的廣告行銷、遊戲開發、甚至是虛擬實境內容，都將因為這項技術而產生質的飛躍。更重要的是，這項技術是免訓練且通用的，能快速整合到現有的AI圖像生成平台中，具有極高的商業價值和市場潛力。我們預期，這項技術將引領下一代AI圖像革命，為投資者帶來豐厚的回報。", "audio": "docs/data/audios/2506.09740v1.wav"}
{"query": "AI", "id": "2506.09968v1", "url": "http://arxiv.org/abs/2506.09968v1", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:14:32.336141", "title_zh": "SRLAgent：透過遊戲化與大型語言模型輔助強化自我調節學習技能", "summary_zh": "大學生面臨日益增加的學術壓力與獨立性，自我調節學習(SRL)至關重要。SRLAgent是一個由大型語言模型(LLM)輔助的系統，透過遊戲化和適應性支援來培養SRL技能。基於Zimmerman的三階段SRL框架，SRLAgent讓學生在互動遊戲環境中進行目標設定、策略執行和自我反思。系統提供由LLM驅動的即時回饋和鷹架式輔助，以支持學生的獨立學習。實驗結果顯示，使用SRLAgent的學生在SRL技能方面有顯著提升，且參與度更高。這項研究突顯了在遊戲化環境中嵌入SRL鷹架和即時AI支援的價值，為旨在促進更深層次的學習和後設認知技能發展的教育科技提供了設計啟示。", "applications": ["想像一下，國中生小明總是拖延作業，有了SRLAgent，系統會像個貼心的教練，透過遊戲化的方式引導他設定目標、規劃時間，並在完成任務後給予即時回饋，幫助他擺脫拖延症，成為時間管理高手。", "大學生小華準備期末考，SRLAgent能根據他的學習進度，推薦最有效的複習策略，並提供客製化的練習題，就像一位24小時隨時待命的AI家教，幫助他更有信心地迎接考試。", "社會新鮮人小美剛進入職場，SRLAgent可以幫助她分析自己的優勢和劣勢，設定職業目標，並提供學習資源和建議，讓她快速適應職場環境，提升工作效率。"], "pitch": "各位投資人，我們正站在教育科技革命的風口浪尖！SRLAgent不僅僅是一個學習工具，它是一個個性化的AI學習夥伴，能有效提升學生的自我調節學習能力，解決長期以來教育界難以攻克的難題。試想，如果每個學生都能擁有像SRLAgent一樣的AI教練，學習效率將大幅提升，升學率、就業率都將水漲船高！這意味著更優秀的人才，更強大的國家競爭力！SRLAgent的潛在市場極其龐大，從K12教育到高等教育，再到企業培訓，無處不在。我們可以與學校、教育機構、企業合作，提供訂閱服務、客製化課程，甚至開發針對特定領域的專業學習模組。更進一步，我們可以將SRLAgent與VR/AR技術結合，打造沉浸式學習體驗，讓學習變得更加有趣和有效。我們預計，SRLAgent將在未來五年內成為教育科技領域的獨角獸，為投資者帶來豐厚的回報！現在加入我們，共同打造教育的未來！", "audio": "docs/data/audios/2506.09968v1.wav"}
{"query": "Foundation Model", "id": "2506.09784v1", "url": "http://arxiv.org/abs/2506.09784v1", "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:16:12.814824", "title_zh": "基於凍結基礎模型的精準且高效的零樣本6D姿態估計", "summary_zh": "本研究提出FreeZeV2，一種無需訓練的方法，利用幾何和視覺基礎模型，實現對未見物體的精準6D姿態估計。FreeZeV2透過稀疏特徵提取、特徵感知評分機制和模組化設計，顯著提升了準確性和效率。相較於前代FreeZe，FreeZeV2在準確度提升5%的同時，速度提升了8倍。結合多個分割模型後，準確度更提升8%，速度仍快2.5倍。FreeZeV2在BOP Challenge 2024中榮獲最佳整體方法獎。這項技術為機器人學和擴增實境等領域帶來了革命性的突破，降低了對大量訓練資料的依賴，實現了更快速、更準確的物體識別和操作。", "applications": ["**智慧家庭助手：** 想像一下，你的機器人吸塵器能精準辨識地上的玩具、襪子，甚至能分辨不同品牌的清潔劑，並針對性地進行清潔，而不需要事先學習每一樣物品的形狀。", "**自動駕駛汽車：** 未來，自動駕駛汽車能更準確地識別道路上的行人、腳踏車騎士，甚至是散落在地上的障礙物，即使這些物體是汽車從未見過的，也能迅速做出反應，提高行車安全。", "**醫療手術輔助：** 醫生可以利用AR眼鏡，在手術過程中即時看到手術器械在病人體內的精確位置和角度，即使是新型的手術器械，也能立即上手，提高手術的精準度和成功率。"], "pitch": "各位投資人，想像一下，我們正在打造的是一個「零學習」的視覺AI引擎！傳統的AI需要海量資料訓練，耗時耗力，但FreeZeV2徹底顛覆了這個模式。它就像一個天生的偵探，即使面對完全陌生的物體，也能瞬間掌握其空間姿態。這意味著什麼？意味著我們將大幅降低AI部署的成本和門檻，加速AI在各行各業的應用。從智慧製造到智慧醫療，從自動駕駛到AR/VR，FreeZeV2將成為這些領域的關鍵賦能者。更重要的是，我們已經在國際頂級競賽中證明了FreeZeV2的領先地位。現在投資我們，您將擁抱一個潛力無限的未來，共同開創一個「萬物皆可識」的AI新時代！我們的目標是讓每一台機器、每一個設備，都擁有像人類一樣的視覺感知能力，而這一切，都將從FreeZeV2開始！", "audio": "docs/data/audios/2506.09784v1.wav"}
{"query": "Diffusion Model", "id": "2506.09681v1", "url": "http://arxiv.org/abs/2506.09681v1", "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "published_date": "2025-06-11", "timestamp": "2025-06-12T15:17:39.832275", "title_zh": "以瓦瑟斯坦距離評估去噪擴散模型的品質：雜訊分數與最佳界限", "summary_zh": "本研究探討去噪擴散機率模型（DDPMs）在生成新樣本方面的能力。DDPMs透過估計分數函數，將布朗運動映射到擴散過程，進而產生新的樣本。研究首先證實DDPMs對於分數評估中的常數變異數雜訊具有穩健性。接著，在瓦瑟斯坦-2距離中建立了有限樣本保證，展現了DDPMs對於雜訊分數估計的穩健性，並實現了比先前已知結果更快的收斂速度。此外，觀察到所得速率與高斯情況下的速率相符，意味著它們具有最佳性。簡單來說，就是這個模型在有雜訊干擾的情況下，依然可以準確且快速地生成高品質的資料。", "applications": ["AI繪圖軟體：即使使用者輸入的草稿或提示不夠清晰，也能生成精美的圖像，提升使用者體驗。", "醫療影像處理：在X光、MRI等影像中，去除雜訊干擾，幫助醫生更準確地診斷疾病，提高醫療效率。", "語音辨識系統：在嘈雜環境中，提高語音辨識的準確性，例如在工廠、車間等環境中，讓機器人能夠聽懂人類的指令。"], "pitch": "想像一下，一個AI模型可以像經驗豐富的藝術家一樣，即使在光線不足或畫布粗糙的情況下，也能創作出令人驚豔的作品。這就是我們技術的核心價值：在充滿雜訊的現實世界中，依然能精準地生成高品質的數據。這項技術的應用範圍極其廣泛，從AI藝術創作、醫療影像分析到自動駕駛等領域，都能帶來革命性的提升。我們預計，未來五年內，這項技術將成為AI領域的基礎設施，賦能各行各業。現在投資我們，您將成為這場AI革命的早期參與者，共同塑造AI的未來！我們不僅僅是在開發一個模型，我們是在打造一個平台，一個可以無限擴展的AI生態系統，一個可以讓AI真正理解並適應現實世界的引擎。這不僅僅是一筆投資，這是一個改變世界的機會！", "audio": "docs/data/audios/2506.09681v1.wav"}
{"query": "AI", "id": "2506.09947v1", "url": "http://arxiv.org/abs/2506.09947v1", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:12:46.478204", "title_zh": "KI4Demokratie：一個基於人工智慧的平台，用於監測和促進民主論述", "summary_zh": "社群媒體日益助長極端主義，特別是右翼極端主義，並加速反民主敘事的傳播。雖然人工智慧和資料科學常被用於操縱政治觀點，但我們迫切需要能夠有效監測、且不侵犯言論自由的工具。KI4Demokratie 是一個基於人工智慧的平台，旨在協助記者、研究人員和政策制定者，監測可能破壞民主價值的右翼言論。它運用機器學習模型分析每日收集的大量德國線上數據，提供德國數位領域趨勢的全面視角。初步分析顯示，追蹤有組織的極端主義行為既複雜又充滿希望，尤其是在關鍵事件期間。", "applications": ["新聞媒體可以使用這個平台來追蹤網路上仇恨言論的趨勢，並報導相關新聞，提醒大眾注意潛在的社會風險。", "教育機構可以利用這個平台來分析學生在社群媒體上的言論，了解他們的價值觀和意識形態，並提供適當的引導和教育。", "政府部門可以透過這個平台來監測網路上的不實訊息，並及時澄清，避免謠言擴散，維護社會穩定。"], "pitch": "各位創投先進，想像一下，一個能夠即時洞察社會輿論、預測潛在危機的人工智慧引擎。KI4Demokratie 不僅僅是一個監測平台，它更是一個社會風險預警系統。在假新聞氾濫、社群媒體影響力日增的時代，它能幫助我們捍衛民主價值，確保資訊的真實性。我們預見，未來企業可以利用它來監測品牌聲譽，政府可以利用它來應對突發事件，甚至個人也能用它來過濾不實資訊。這是一個潛力無限的市場，讓我們一起投資 KI4Demokratie，打造一個更透明、更健康的數位社會！", "audio": "docs/data/audios/2506.09947v1.wav"}
{"query": "Foundation Model", "id": "2506.09748v1", "url": "http://arxiv.org/abs/2506.09748v1", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:14:11.168015", "title_zh": "基於語義與結構約束的無人機絕對視覺定位分層圖像匹配", "summary_zh": "本研究提出一種用於無人機絕對視覺定位的分層跨源圖像匹配方法，旨在解決在無全球導航衛星系統（GNSS）信號下無人機定位的挑戰。該方法整合了語義感知和結構約束的粗略匹配模塊以及輕量級的精細匹配模塊。粗略匹配利用視覺基礎模型提取的語義特徵，在語義和結構約束下建立區域級的對應關係。精細匹配則提取精細特徵，建立像素級的對應關係。實驗結果表明，該方法在多種挑戰性條件下均表現出卓越的準確性和魯棒性，驗證了其有效性。此技術無需依賴相對定位技術，僅通過圖像檢索模塊和提出的分層圖像匹配模塊即可實現。", "applications": ["無人機快遞：在GNSS信號不佳的城市峽谷或室內環境中，無人機可以精確定位並完成包裹投遞。", "災難救援：在地震或洪水等災難發生後，無人機可以利用該技術繪製受災地圖，協助救援人員快速找到受困者，即使GNSS信號中斷。", "智慧農業：無人機可以精確監測農作物生長狀況，例如判斷哪一區域的作物需要額外施肥或灌溉，從而提高農業生產效率。"], "pitch": "各位投資人，我們帶來的是一項革命性的無人機定位技術，它將徹底改變無人機的應用場景。想像一下，在GNSS信號受限的環境中，我們的無人機依然能夠精準定位、自主導航，這意味著什麼？更高效的物流配送、更安全的災難救援、更精準的農業管理，這僅僅是開始！我們的分層圖像匹配技術，克服了傳統方法在跨源圖像差異和時間變化上的局限性，實現了前所未有的準確性和魯棒性。這項技術不僅能應用於現有的無人機市場，更將開啟全新的商業模式。例如，我們可以與保險公司合作，提供災後快速評估服務；與電信公司合作，進行基站信號覆蓋檢測；甚至與軍方合作，提供戰場偵察和情報收集。未來，隨著無人機技術的普及，我們的定位技術將成為各行各業的剛需。現在投資，您將搶佔市場先機，共同打造一個由精準定位技術賦能的無人機新時代！", "audio": "docs/data/audios/2506.09748v1.wav"}
{"query": "Diffusion Model", "id": "2506.09644v1", "url": "http://arxiv.org/abs/2506.09644v1", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "published_date": "2025-06-11", "timestamp": "2025-06-12T21:15:19.631579", "title_zh": "DGAE：擴散引導自編碼器，用於高效潛在表示學習", "summary_zh": "DGAE是一種新型自編碼器，旨在提升圖像和影片生成模型的效率。它透過擴散模型引導解碼器，即使在高壓縮比下也能有效恢復資訊，解決了傳統自編碼器在高壓縮時的效能下降問題。DGAE不僅提升了空間壓縮能力，更將潛在空間的維度縮小了2倍，實現了更高效、更精簡的表示。實驗證明，DGAE在ImageNet-1K圖像生成上表現出色，並且能加速擴散模型的收斂，為生成模型帶來了顯著的性能提升。", "applications": ["1. 手機修圖App：使用者可以快速壓縮照片大小，節省儲存空間，同時確保照片品質不受太大影響，上傳分享更方便。", "2. 視訊會議軟體：在網路不佳的情況下，DGAE可以幫助壓縮視訊資料，確保視訊通話的流暢性，減少延遲和卡頓。", "3. 遊戲開發：遊戲開發者可以使用DGAE來壓縮遊戲素材，例如角色模型和場景貼圖，降低遊戲的檔案大小，讓玩家更容易下載和安裝。"], "pitch": "各位投資人，我們正在開發一項革命性的圖像壓縮技術——DGAE，它能以更小的資料量，產生更高品質的圖像。想像一下，未來的AI繪圖只需要極少的運算資源，就能生成逼真的影像，這將徹底顛覆遊戲、影視、廣告等產業。DGAE不僅能節省儲存空間和傳輸頻寬，還能加速AI模型的訓練速度，降低成本。更令人興奮的是，DGAE的潛力遠不止於此，它甚至能應用於醫療影像分析，協助醫生更快更準確地診斷疾病。我們相信，DGAE將成為未來AI時代的基礎設施，擁有巨大的商業價值和社會影響力。現在加入我們，一起開創圖像處理的新紀元！", "audio": "docs/data/audios/2506.09644v1.wav"}
{"query": "AI", "id": "2506.09940v1", "url": "http://arxiv.org/abs/2506.09940v1", "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:56:26.588556", "title_zh": "資訊不對稱與知識可轉移性下線上策略決策的樣本複雜度", "summary_zh": "本研究探討在資訊不對稱和知識轉移挑戰下，如何有效學習策略決策。在多代理系統中，代理人基於私有資訊採取行動以最大化獎勵，這導致了複雜的混淆變數。同時，由於難以在目標環境中進行實驗，知識轉移也構成重大挑戰。我們提出了一種樣本高效的演算法，能準確識別資訊不對稱下的系統動態，並有效應對強化學習中的知識轉移。該方法在線上策略互動模型下，證明能以O(1/epsilon^2)的樣本複雜度學習到epsilon-最佳策略。", "applications": ["股票市場預測：利用過去的交易數據（知識轉移）來預測未來股價，即使不同投資者擁有不同的資訊（資訊不對稱），也能幫助散戶做出更明智的投資決策。", "醫療診斷：醫生根據病人的病歷、檢查報告等資訊（資訊不對稱）來判斷病情。透過學習大量病例數據（知識轉移），可以輔助醫生做出更精準的診斷，減少誤診率。", "交通流量控制：在不同路段的車流量資訊不對稱的情況下，透過分析歷史交通數據（知識轉移），可以即時調整紅綠燈時間，優化整體交通流量，減少交通堵塞。"], "pitch": "各位創投先進，我們正在開發一項突破性的AI技術，能解決資訊不對稱和知識轉移這兩大難題，這在現實世界中無所不在。想像一下，在金融市場，我們的演算法能夠比傳統模型更精準地預測股價，為投資者帶來超額回報。在醫療領域，我們的技術可以協助醫生做出更準確的診斷，挽救更多生命。更令人興奮的是，我們的技術具有高度的可擴展性，可以應用於智慧城市、供應鏈管理、甚至國防安全等領域。我們已經證明了我們的演算法具有極高的樣本效率，這意味著我們可以用更少的數據更快地學習，降低開發成本。我們相信，這項技術將徹底改變決策方式，創造巨大的商業價值。現在加入我們，一起塑造AI的未來！", "audio": "docs/data/audios/2506.09940v1.wav"}
{"query": "Foundation Model", "id": "2506.09638v1", "url": "http://arxiv.org/abs/2506.09638v1", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:57:44.158129", "title_zh": "FedVLMBench：視覺語言模型聯邦式微調基準測試", "summary_zh": "本研究推出FedVLMBench，首個針對視覺語言模型（VLM）聯邦式微調的系統性基準測試。它整合了兩種主流VLM架構、四種微調策略、五種聯邦學習算法，以及六個涵蓋多領域單任務和多任務場景的多模態數據集。實驗結果揭示了VLM架構、微調策略、數據異質性以及多任務聯邦優化之間的相互作用。研究發現，在聯邦學習中，對於基於編碼器的VLM，具有並行連接器和LLM調整的雙層多層感知器連接器是最佳配置。此外，當前的聯邦學習方法在以視覺為中心的任務中，比以文本為中心的任務對數據異質性更敏感。FedVLMBench為研究社群提供必要的工具、數據集和經驗指導，為推進多模態基礎模型的隱私保護聯邦式訓練提供標準化平台。", "applications": ["遠距醫療影像分析：在不洩漏病人隱私的情況下，讓不同醫院的醫療影像數據協同訓練AI模型，提升疾病診斷的準確性。", "個性化教育內容推薦：整合不同地區學生的學習數據，在保護學生隱私的前提下，為每位學生提供更精準的學習資源推薦。", "智慧城市交通流量預測：結合不同來源的交通數據（例如：監視器影像、感測器數據），在保護數據提供者隱私的情況下，提升交通流量預測的準確性，優化交通管理。"], "pitch": "各位投資人，我們帶來的是FedVLMBench，一個革命性的平台，它將引領視覺語言模型（VLM）進入聯邦學習的新時代！想像一下，一個能夠在保護隱私的前提下，整合全球醫療影像數據，開發出更精準的疾病診斷AI的未來；一個能夠匯集所有線上學習平台數據，打造真正個性化教育體驗的未來；一個能夠即時分析全球交通數據，實現智慧交通調度的未來。FedVLMBench正是實現這些願景的關鍵。它不僅是一個基準測試，更是一個創新的催化劑，將加速VLM在各個垂直領域的應用。隨著數據隱私意識的日益增強，聯邦學習的需求將呈指數級增長。FedVLMBench將成為這個市場的領導者，為我們帶來巨大的商業價值和社會影響力。現在加入我們，一起塑造這個隱私保護的AI驅動的未來！", "audio": "docs/data/audios/2506.09638v1.wav"}
{"query": "Diffusion Model", "id": "2506.09538v1", "url": "http://arxiv.org/abs/2506.09538v1", "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "published_date": "2025-06-11", "timestamp": "2025-06-13T00:59:22.934110", "title_zh": "AngleRoCL：適用於物理視角不變之T2I對抗性貼片的角度穩健概念學習", "summary_zh": "本研究探討了文字生成圖像(T2I)模型所產生的對抗性貼片在不同視角下的攻擊效果，發現這些貼片在角度穩健性方面存在問題。研究表明，文字提示顯著影響貼片的角度穩健性，且特定任務的語言指令無法有效提升。為此，我們提出AngleRoCL，一種簡單且靈活的方法，學習一個可泛化的概念，代表生成角度穩健貼片的能力。此概念可融入文字提示中，引導T2I模型生成對視角變化具有抵抗力的貼片。實驗結果顯示，AngleRoCL顯著提升了T2I對抗性貼片的角度穩健性，在多個角度下仍保持高攻擊成功率，攻擊效果平均相對提升超過50%。本研究推進了對物理角度穩健貼片的理解，並深入探討了文字概念與T2I生成內容中物理屬性之間的關係。", "applications": ["智慧交通：想像一下，在自動駕駛汽車的攝影機上貼上這種對抗性貼片，可以讓汽車誤判交通號誌，例如將停止標誌誤認為是限速標誌，從而導致交通意外。AngleRoCL可以幫助我們測試並增強自動駕駛系統的安全性，使其更能抵抗這種惡意攻擊。", "安全監控：在機場或重要設施的監控攝影機上貼上這種貼片，可以讓系統無法正確辨識特定人物或物體，從而影響安全預警。AngleRoCL技術有助於評估和改善監控系統的漏洞，確保公共安全。", "廣告干擾：在戶外廣告看板上貼上這種貼片，可以讓手機掃描時顯示錯誤的廣告內容，或者導向惡意網站。AngleRoCL的研究可以幫助我們開發更強大的圖像辨識系統，避免用戶受到欺騙或損害。"], "pitch": "各位創投先進，我們團隊帶來的是AngleRoCL技術，一項革命性的AI安全防禦方案。當前AI圖像辨識系統，特別是自動駕駛、安全監控等領域，面臨著嚴峻的對抗性攻擊威脅。駭客只需透過精心設計的貼片，就能輕易欺騙AI系統，造成難以估計的損失。AngleRoCL的獨特之處在於，它能讓AI系統在各種角度、光線下，都能精準辨識物體，有效抵禦這些惡意攻擊。想像一下，未來無人機送貨普及，AngleRoCL能確保無人機不會被惡意貼片誤導，安全抵達目的地；智慧工廠中，AngleRoCL能保護生產線上的機器手臂，避免因圖像辨識錯誤而發生事故。這不僅僅是一項技術，更是一份安全保障，一個潛力無限的市場。我們預計，隨著AI應用的普及，AngleRoCL將成為AI安全領域的關鍵基礎設施，市場規模將達到數十億美元。現在加入，您將成為AI安全革命的領航者！", "audio": "docs/data/audios/2506.09538v1.wav"}
{"query": "AI", "id": "2506.10975v1", "url": "http://arxiv.org/abs/2506.10975v1", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "published_date": "2025-06-12", "timestamp": "2025-06-13T03:44:41.947637", "title_zh": "GenWorld：邁向偵測AI生成的真實世界模擬影片", "summary_zh": "隨著影片生成技術蓬勃發展，AI生成影片的偵測需求日益增加。然而，缺乏高品質的真實世界數據集阻礙了可靠偵測器的發展。為此，我們提出GenWorld，一個大規模、高品質的真實世界模擬數據集，用於AI生成影片偵測。GenWorld專注於模擬真實世界場景的影片，這些影片因其真實感和潛在影響力而意義重大。我們利用多種先進的影片生成模型提供逼真且高品質的偽造影片，並包含來自不同生成器和各種提示模態（例如，文本、圖像、影片）的影片，從而學習更具泛化性的特徵。我們也提出SpannDetector模型，利用多視角一致性作為真實世界AI生成影片偵測的強大標準，實驗證明其效果優異。GenWorld將推動AI生成影片偵測領域的發展。", "applications": ["新聞媒體可以使用這項技術來驗證收到的影片素材是否為真實拍攝，避免誤報或傳播假新聞，維護新聞的可信度。", "保險公司可以利用AI判斷車禍或意外事故的影片是否經過篡改，防止詐保案件發生，減少不必要的損失。", "教育機構可以運用此技術，檢測學生提交的影片作業是否為AI生成，確保學術誠信，鼓勵學生獨立思考和創作。"], "pitch": "各位創投先進，想像一下，在AI影片真假難辨的時代，誰能掌握辨識真偽的鑰匙，誰就能掌握資訊安全的主導權！GenWorld不僅是一個數據集，更是一座金礦！我們開發的SpannDetector模型，能有效辨識由世界模型（如Cosmos）生成的高品質偽造影片，這代表我們能領先市場，提供最可靠的AI影片驗證服務。試想，未來所有的新聞媒體、社群平台、政府機關，甚至是個人，都需要我們的技術來驗證影片的真實性。這是一個數十億美元的潛在市場！更進一步，我們可以將此技術應用於國防安全、金融詐欺防範等領域，其價值難以估量。現在投資GenWorld，就是投資未來，讓我們一起打造一個更真實、更安全的數位世界！", "audio": "docs/data/audios/2506.10975v1.wav"}
{"query": "AI", "id": "2506.10953v1", "url": "http://arxiv.org/abs/2506.10953v1", "title": "Build the web for agents, not agents for the web", "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "published_date": "2025-06-12", "timestamp": "2025-06-13T06:18:48.315754", "title_zh": "為代理人打造網路，而非為網路打造代理人", "summary_zh": "近年來，大型語言模型（LLM）和多模態模型的發展，激發了開發網路代理人的濃厚興趣。這些AI系統能在網路環境中自主導航和完成任務。然而，由於人類設計的介面與LLM能力之間存在根本不匹配，現有方法面臨巨大挑戰。本論文倡導網路代理人研究的範式轉移：我們應該開發一種專為代理能力優化的新型互動範式，而不是強迫網路代理人適應為人類設計的介面。為此，我們引入了「代理人網路介面」（AWI）的概念，並確立了六項設計原則，強調安全性、效率和標準化。這種重新定義旨在克服現有介面的根本局限性，為更高效、可靠和透明的網路代理人設計鋪平道路。", "applications": ["智能客服：網路代理人可以自動處理線上客服請求，解答常見問題、協助用戶查找資料，甚至完成簡單的交易，大幅降低企業的人力成本。", "自動化購物：代理人可以根據使用者設定的條件（例如價格、品牌、規格）自動搜尋、比較和購買商品，省去使用者瀏覽大量網頁的時間。", "行程規劃：代理人可以整合各種網站資訊，例如航班、飯店、景點等，自動規劃最佳旅遊行程，並協助預訂機票和住宿。"], "pitch": "各位投資人，我們正處於AI革命的風口浪尖！想像一下，一個由AI代理人驅動的網路世界，在這個世界裡，複雜的網路互動變得簡單、高效、自動化。我們提出的「代理人網路介面」（AWI）正是實現這個願景的關鍵。現有的網路介面是為人類設計的，AI代理人要適應這些介面，效率低下、錯誤頻發。AWI則反其道而行，為AI代理人量身打造，讓它們能夠更安全、更高效地完成各種網路任務。這不僅僅是技術上的突破，更是一場商業模式的變革。試想，未來的電商平台、旅遊網站、金融服務，都將基於AWI構建，AI代理人將成為使用者與網路世界互動的主要介面。我們預計，AWI將催生一個數十億美元的市場，而我們正是這個市場的開拓者。現在加入我們，一起打造AI驅動的未來網路！", "audio": "docs/data/audios/2506.10953v1.wav"}
{"query": "AI", "id": "2506.10934v1", "url": "http://arxiv.org/abs/2506.10934v1", "title": "Dynamic Epistemic Friction in Dialogue", "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:13:43.255960", "title_zh": "[翻譯失敗] Dynamic Epistemic Friction in Dialogue", "summary_zh": "摘要翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "applications": ["應用場景1：翻譯失敗，請參考原文", "應用場景2：翻譯失敗，請參考原文", "應用場景3：翻譯失敗，請參考原文"], "pitch": "推銷內容翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "audio": "docs/data/audios/2506.10934v1.wav"}
{"query": "Foundation Model", "id": "2506.10914v1", "url": "http://arxiv.org/abs/2506.10914v1", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:14:54.990913", "title_zh": "基於先驗數據擬合網路的因果推論基礎模型", "summary_zh": "本研究提出CausalFM，一個基於先驗數據擬合網路(PFN)的因果推論基礎模型框架。PFN是一種Transformer模型，先使用來自預先指定的先驗分佈的合成數據進行預訓練，然後通過上下文學習實現貝氏推論。CausalFM基於結構因果模型(SCM)建立貝氏先驗，並使用因果啟發的貝氏神經網路，在後門、前門和工具變數調整等不同情境下執行貝氏因果推論。我們訓練了一個用於估計條件平均處理效應(CATE)的基礎模型，並證明CausalFM在合成和半合成基準測試中具有競爭力。CausalFM為因果推論提供了一種新範式，有望從根本上改變醫學、經濟學等領域的因果推論方式。", "applications": ["**個人化醫療建議：** 根據你的生活習慣、基因數據和病史，CausalFM能更準確地預測特定治療方案對你的效果，幫助醫生制定更有效的個人化治療計畫。", "**精準行銷：** 商家可以利用CausalFM分析不同行銷活動對顧客購買行為的影響，找出真正有效的策略，避免浪費資源在無效的廣告上。", "**政策影響評估：** 政府可以利用CausalFM預測政策實施後對社會各個層面的影響，例如教育改革對學生學習成果的影響，從而制定更完善的政策。"], "pitch": "各位投資人，我們正處於AI發展的關鍵時刻！CausalFM不僅僅是一個模型，它代表著因果推論領域的革命性突破。想像一下，在醫學上，我們不再僅僅依賴相關性，而是能真正理解藥物對病人的因果效應，從而實現精準醫療，大幅提高治療成功率，降低醫療成本。在金融領域，CausalFM能幫助我們更準確地預測市場走向，降低投資風險。在政策制定上，它能幫助政府預測政策的真實影響，避免決策失誤。CausalFM的潛力是無限的，它能應用於任何需要理解因果關係的領域。我們相信，CausalFM將成為未來AI發展的基石，引領我們進入一個更智慧、更可預測的時代。現在加入我們，共同打造這個劃時代的產品，抓住這波AI浪潮，實現巨大的商業價值！", "audio": "docs/data/audios/2506.10914v1.wav"}
{"query": "Diffusion Model", "id": "2506.10971v1", "url": "http://arxiv.org/abs/2506.10971v1", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "published_date": "2025-06-12", "timestamp": "2025-06-13T09:16:27.169513", "title_zh": "遮罩離散擴散模型中，引導究竟做了什麼？", "summary_zh": "本研究深入探討了帶有Classifier-Free Guidance (CFG) 的遮罩離散擴散模型。在理想條件下，我們推導出引導反向動態的明確解，精確地描述引導如何影響採樣行為。當目標是從特定類別採樣時，引導會放大特定類別區域，同時抑制與其他類別共享的區域。這種效應取決於引導強度，並在採樣分佈中產生不同的協方差結構。值得注意的是，我們觀察到一維和二維空間中存在定量差異。研究還表明，對於較大的引導強度，總變異的衰減率在反向動態中呈雙指數形式。這些發現突顯了引導的作用，不僅在於塑造輸出分佈，還在於控制採樣軌跡的動態。", "applications": ["AI繪圖助手：像Midjourney或Stable Diffusion一樣，透過調整引導強度，讓使用者更精準地控制AI生成的圖片風格和內容，例如指定畫作更偏向印象派或寫實主義。", "醫療影像分析：協助醫生更準確地辨識X光片或MRI中的微小病灶，透過增強特定組織或器官的特徵，減少誤診率。", "語音合成：讓AI語音更具情感，例如，在生成悲傷語氣時，引導模型更強調低沉、緩慢的音調，使語音聽起來更自然、更符合情境。"], "pitch": "各位投資人，我們正在開發一項革命性的AI技術，它能讓生成式AI的控制力提升到前所未有的水平。我們的研究揭示了「引導」在遮罩離散擴散模型中的核心作用，讓AI不再只是隨機生成，而是能精準地按照我們的意願創造內容。想像一下，一個能完全理解你需求的AI藝術家，或者一個能精確診斷疾病的AI醫生，這就是我們技術的潛力。我們相信，這項技術將徹底改變內容創作、醫療診斷、以及更多領域。未來，我們甚至可以將這項技術應用於新藥開發，透過引導模型生成具有特定藥理特性的分子結構。現在加入我們，一起打造AI驅動的未來！", "audio": "docs/data/audios/2506.10971v1.wav"}
{"query": "AI", "id": "2506.10927v1", "url": "http://arxiv.org/abs/2506.10927v1", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:23:33.537698", "title_zh": "生成式AI在促進社交互動中的作用：範圍界定性回顧", "summary_zh": "社交連結減少對心理健康、壽命和整體福祉構成威脅。生成式AI技術，如大型語言模型和圖像生成工具，正被廣泛應用於增強人類社交體驗。本研究回顧了30篇2020年以來發表的文獻，探討了基於生成式AI的應用程式如何促進社交互動，以及它們針對的社交參與形式。研究發現，這些應用程式廣泛應用於故事敘述、社交情緒技能訓練、懷舊、協作學習、音樂創作和一般對話。研究強調了參與式和共同設計方法在促進有效技術使用和社交參與中的作用，同時也審視了文化偏見和可及性等社會倫理問題。生成式AI有潛力支持動態和個人化的互動，但需要更加關注公平設計實踐和包容性評估策略。", "applications": ["爺爺奶奶常常感到孤單？我們可以利用AI生成他們年輕時的照片，並與他們聊天，回憶過去的美好時光，讓他們的生活不再孤單。", "小朋友不擅長表達自己的情緒？透過AI互動遊戲，讓他們學習如何識別和表達情緒，提升社交能力，成為EQ高手。", "想學新樂器卻找不到人一起練習？AI可以化身為你的專屬樂團，隨時隨地和你一起Jam，激發你的音樂潛能。"], "pitch": "各位投資人，想像一下，一個不再有孤獨的世界！我們的技術正是通往這個世界的鑰匙。我們利用生成式AI，不僅能創造個人化的社交體驗，更能解決高齡化社會的孤獨問題，提升年輕世代的社交技能。這不僅是一個技術革新，更是一項社會責任投資！市場規模將會隨著AI技術的成熟與普及，呈現爆炸性成長。我們預計在三年內，成為社交AI領域的領導者，五年內將技術應用於醫療、教育、娛樂等更廣泛的領域。現在投資，您將成為這場社交革命的先驅，共同打造一個更連結、更溫暖的未來！我們的目標是讓每個人都能享受高品質的社交生活，讓AI成為促進人與人之間連結的橋樑，而非阻礙。這是一個千載難逢的投資機會，讓我們一起改變世界！", "audio": "docs/data/audios/2506.10927v1.wav"}
{"query": "Foundation Model", "id": "2506.10579v1", "url": "http://arxiv.org/abs/2506.10579v1", "title": "Equations of state and stability condition of mixed p-spin glass model", "summary": "The Sherrington-Kirkpatrick (SK) is a foundational model for understanding\nspin glass systems. It is based on the pairwise interaction between each two\nspins in a fully connected lattice with quenched disordered interactions. The\nnature of long-range interaction among spins in the (SK) model simplifies the\nstudy of this system by eliminating fluctuations. An advanced (SK) model, known\nas the p-spin model, introduces higher-order interactions that involve the\ninteraction of P spins. This research focuses on the general Hamiltonian of the\nspin glass model with long-range interaction, referred to as the mixed p-spin\nglass model, which consists of adding all p-spin interaction terms. This\nresearch aims to derive the equation of states for this Hamiltonian, formulate\nthe equation of state within the framework of the first replica symmetry\nbreaking, and determine both the stability condition of the replica symmetric\nsolution and the stability of the replicas belonging to the same group in the\nfirst step of replica symmetry breaking.", "authors": ["Ali Talebi"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:24:52.066766", "title_zh": "混合p-自旋玻璃模型的狀態方程式與穩定性條件", "summary_zh": "本研究深入探討混合p-自旋玻璃模型，這是一個更複雜的自旋玻璃模型，它擴展了傳統的Sherrington-Kirkpatrick (SK) 模型，引入了涉及多個自旋之間的高階交互作用。我們的目標是推導出此模型的狀態方程式，並在第一步複製對稱性破壞的框架下建立方程式。此外，我們還致力於確定複製對稱解的穩定性條件，以及在複製對稱性破壞的第一步中，屬於同一群組的複製體的穩定性。這項研究有助於更深入理解複雜系統的行為，並為材料科學和信息科學等領域的應用奠定基礎。", "applications": ["想像一下，我們可以利用這種模型來設計更穩定的記憶體。就像自旋玻璃一樣，記憶體中的數據也需要保持在一個穩定的狀態。這個模型可以幫助我們找到最佳的材料組合和結構，讓數據不會輕易丟失或損壞。", "在金融市場上，股票價格的波動就像自旋一樣，互相影響。這個模型可以幫助我們分析市場的複雜關係，預測風險，並設計出更有效的投資策略。", "在生物學上，蛋白質的摺疊方式決定了它的功能。這個模型可以幫助我們理解蛋白質是如何找到正確的摺疊方式，並設計出新的藥物來治療疾病。"], "pitch": "各位創投先進，我們正在開發一項革命性的技術，基於對混合p-自旋玻璃模型的深入研究，這項技術將徹底改變材料科學、金融和生物技術等領域。想像一下，我們能夠設計出永不失效的記憶體、預測金融市場的崩盤，甚至解開蛋白質摺疊的終極奧秘。我們的模型不僅僅是一個理論框架，更是一個強大的工具，可以幫助我們解決現實世界中的複雜問題。我們相信，這項技術的潛在商業價值是無限的，現在投資，您將站在下一次科技革命的最前沿！未來，我們將把這個模型應用於量子計算領域，開發出更強大、更穩定的量子位元，引領下一個世代的科技發展。不要錯過這個機會，加入我們，一起創造未來！", "audio": "docs/data/audios/2506.10579v1.wav"}
{"query": "Diffusion Model", "id": "2506.10963v1", "url": "http://arxiv.org/abs/2506.10963v1", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published_date": "2025-06-12", "timestamp": "2025-06-13T12:26:23.188296", "title_zh": "MMMG：一個大規模、跨領域、多層級的文本到圖像推理生成基準", "summary_zh": "本研究提出知識圖像生成的新任務，並創建了大規模跨領域多層級知識圖像生成基準（MMMG），旨在評估圖像生成模型的推理能力。知識圖像在人類文明和學習中至關重要。MMMG包含4456個專家驗證的圖像-提示對，涵蓋10個學科、6個教育程度和多種知識格式。為了簡化評估，採用統一的知識圖譜（KG）表示。我們還引入MMMG-Score來評估生成的知識圖像，結合了事實準確性（通過KG的圖編輯距離衡量）和視覺清晰度評估。評估結果顯示，現有模型在推理方面存在嚴重缺陷，為此，我們發布了FLUX-Reason，作為一個有效的開源基準。", "applications": ["**教育輔助：** 想像一下，學生在學習複雜的歷史事件時，只需輸入文字描述，就能自動生成相關的事件時間軸圖或因果關係圖，幫助他們更直觀地理解知識。", "**醫療健康：** 醫生可以利用這項技術，將病患的病歷資料轉換成易於理解的圖表或示意圖，方便與病患溝通病情，提升醫囑遵從性。", "**新聞報導：** 新聞媒體可以快速生成新聞事件的相關圖表或地圖，例如地震災情分布圖、經濟數據走勢圖等，讓讀者更快速地掌握新聞重點。"], "pitch": "各位投資人，我們正在開發一項劃時代的技術：基於MMMG基準的知識圖像生成。想像一下，未來的人工智慧不只能生成逼真的圖像，還能理解複雜的知識並將其視覺化呈現。這將徹底改變教育、醫療、新聞、行銷等各個領域。我們的技術不僅能提升學習效率、改善溝通效果，還能創造全新的商業模式。例如，我們可以為企業客製化生成數據分析報告的視覺化圖表，或為遊戲開發者提供快速生成遊戲素材的工具。隨著AI技術的不斷發展，知識圖像生成將成為一個巨大的市場。現在投資我們，您將站在AI革命的最前沿，共同開創一個充滿無限可能的未來！我們相信，這項技術的潛力遠遠超出我們的想像，它將成為下一代AI的核心引擎。", "audio": "docs/data/audios/2506.10963v1.wav"}
{"query": "AI", "id": "2506.10916v1", "url": "http://arxiv.org/abs/2506.10916v1", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:13:05.843699", "title_zh": "[翻譯失敗] Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "summary_zh": "摘要翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "applications": ["應用場景1：翻譯失敗，請參考原文", "應用場景2：翻譯失敗，請參考原文", "應用場景3：翻譯失敗，請參考原文"], "pitch": "推銷內容翻譯失敗：503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "audio": "docs/data/audios/2506.10916v1.wav"}
{"query": "Foundation Model", "id": "2506.10395v1", "url": "http://arxiv.org/abs/2506.10395v1", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:14:32.966572", "title_zh": "雙魚座：用於圖像理解與生成的自迴歸基礎模型", "summary_zh": "本研究提出名為「雙魚座」的自迴歸多模態基礎模型，旨在解決圖像理解與生成在統一模型中表現不佳的問題。雙魚座採用獨特的解耦視覺編碼架構，並針對多模態生成進行優化訓練。透過精心的數據管理、預訓練與微調，雙魚座在圖像理解與生成任務上均展現了卓越的競爭力。實驗結果顯示，雙魚座在超過20個圖像理解公開基準測試中表現出色，並在GenEval圖像生成基準測試中展現了強大的生成能力。研究揭示了圖像理解與生成之間的協同效應，以及使用獨立視覺編碼器的優勢，推動了統一多模態模型的發展。", "applications": ["**智慧相簿自動分類：** 想像一下，你的手機相簿可以自動辨識照片內容，例如風景、人物、食物等，並進行智慧分類，讓你輕鬆找到想看的照片，省去手動整理的麻煩。", "**AI繪圖助手：** 只要簡單描述你想要的畫面，例如「夕陽下的海灘，有幾個小孩在玩沙」，AI就能根據你的描述生成栩栩如生的圖像，讓每個人都能成為藝術家。", "**醫療影像輔助診斷：** 醫生可以利用AI快速分析X光片、CT掃描等醫療影像，協助早期發現病灶，提升診斷效率和準確性，造福更多患者。"], "pitch": "各位投資人，我們正站在AI圖像技術革命的浪潮之巔！「雙魚座」不僅僅是一個模型，它是一把開啟圖像理解與生成無限可能的鑰匙。試想一下，未來的電商平台，消費者只需上傳一張舊沙發的照片，AI就能立即生成數十種不同風格、材質的新沙發設計方案，並直接連結到供應商，實現個性化定制。在遊戲產業，開發者可以利用「雙魚座」快速生成各種逼真的人物、場景，大幅降低開發成本，提升遊戲品質。更重要的是，我們正在積極探索將「雙魚座」應用於自動駕駛、智慧城市等領域，讓AI成為真正的視覺大腦，賦能各行各業。現在投資「雙魚座」，就是投資AI的未來，我們有信心在短時間內實現指數級增長，為各位帶來豐厚的回報！", "audio": "docs/data/audios/2506.10395v1.wav"}
{"query": "Diffusion Model", "id": "2506.10962v1", "url": "http://arxiv.org/abs/2506.10962v1", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "published_date": "2025-06-12", "timestamp": "2025-06-13T15:15:49.844564", "title_zh": "SpectralAR：頻譜自迴歸視覺生成", "summary_zh": "本研究提出一種名為SpectralAR的視覺生成框架，有別於傳統將圖像分割成空間圖塊的方式，SpectralAR從頻譜角度實現視覺序列的因果關係。它首先使用嵌套頻譜標記將圖像轉換為有序的頻譜標記，代表由低到高的頻率分量。然後，以由粗到細的方式，使用頻譜標記序列執行自迴歸生成。這種方法兼顧了序列因果關係和標記效率，在ImageNet-1K圖像重建和自迴歸生成實驗中，僅使用64個標記和3.1億個參數，就達到了3.02 gFID的優異成果。", "applications": ["AI修復老照片：可以將模糊不清的老照片，透過AI運算，還原成清晰、色彩鮮明的影像，讓珍貴的回憶重現。", "線上遊戲材質生成：遊戲開發者可以利用這項技術，快速生成各種遊戲場景的材質，例如岩石、木紋、金屬等等，大幅縮短開發時間。", "醫療影像增強：醫生可以利用這項技術，提升X光片、斷層掃描等醫療影像的清晰度，幫助診斷疾病。"], "pitch": "各位投資人，我們相信SpectralAR將徹底改變視覺內容生成領域。想像一下，一個AI能夠以驚人的效率和逼真度生成各種圖像，從高解析度的產品渲染圖到個性化的藝術作品，再到複雜的醫療影像。 SpectralAR的頻譜自迴歸方法，不僅在技術上領先，更具有巨大的商業潛力。我們預計，這項技術將廣泛應用於遊戲、廣告、醫療、電商等行業，成為AI圖像生成領域的領頭羊。我們的團隊擁有深厚的技術積累和豐富的市場經驗，我們有信心將SpectralAR打造成一個獨角獸企業，為各位投資人帶來豐厚的回報。現在投資，您將成為這場視覺革命的早期參與者，共同開創AI圖像生成的新時代！未來，我們甚至可以將其應用於元宇宙的建設，快速生成各種虛擬場景和人物，打造一個高度真實且豐富多彩的數位世界。", "audio": "docs/data/audios/2506.10962v1.wav"}
{"query": "AI", "id": "2506.10908v1", "url": "http://arxiv.org/abs/2506.10908v1", "title": "Probably Approximately Correct Labels", "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:17:10.288601", "title_zh": "可能近似正確標籤", "summary_zh": "為了降低獲取高品質標註資料集的成本，本研究提出一種結合專家標籤與預訓練AI模型預測的方法。此方法能以較低的成本建構標註資料集，並確保標籤的整體錯誤率在可接受範圍內，即「可能近似正確」。透過結合人工智慧模型，我們得以更嚴謹且高效地管理資料集。我們已透過大型語言模型的文本標註、預訓練視覺模型的圖像標註，以及AlphaFold的蛋白質摺疊分析，驗證了此方法的優勢。", "applications": ["AI醫生助理：AI可以分析病歷和醫學影像，初步判斷病情。醫生再審核AI的建議，大幅提升診斷效率，並減少誤診率。", "智慧客服：AI客服可以根據用戶問題，快速從龐大的知識庫中找到答案。客服人員則處理較複雜或AI無法解決的問題，降低客服成本，提升客戶滿意度。", "自動駕駛訓練：AI可以模擬各種駕駛場景，產生大量標註數據。工程師只需驗證AI生成的數據，即可加速自動駕駛系統的開發，提高安全性。"], "pitch": "想像一下，我們正處於一個數據爆炸的時代，AI的發展高度依賴於大量的標註數據。然而，傳統的人工標註成本高昂、耗時費力，嚴重阻礙了AI的發展速度。我們的「可能近似正確標籤」技術，就像是AI數據領域的「煉金術」，它能以極低的成本，將預訓練AI模型的初步預測，結合少量專家驗證，快速生成高質量的標註數據。這意味著，AI模型的訓練成本將大幅降低，開發週期將顯著縮短。從醫療診斷、金融風控，到自動駕駛、智慧製造，各行各業都將因此受益。我們預計，未來五年內，隨著AI技術的普及，對高質量標註數據的需求將呈指數級增長。我們的技術將成為AI產業的基石，擁有巨大的市場潛力。現在投資我們，您將與我們一起，引領AI數據革命，共同分享AI時代的巨大紅利！", "audio": "docs/data/audios/2506.10908v1.wav"}
{"query": "Foundation Model", "id": "2506.10386v1", "url": "http://arxiv.org/abs/2506.10386v1", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:18:18.533362", "title_zh": "利用六自由度姿態基礎模型繪製海洋沉積物覆蓋圖", "summary_zh": "本研究提出一個名為PoseIDON的電腦視覺流程，結合深度基礎模型特徵與多視角攝影測量技術，從水下遙控載具（ROV）影片中估算海底物體（如桶子和彈藥）的六自由度姿態以及周圍海床的方位。透過將物體的CAD模型與觀測影像對齊，並擬合海床的局部平面近似，推斷出掩埋深度。實驗結果顯示，該模型能以約10公分的平均誤差估算掩埋深度，並解析反映底層沉積物傳輸過程的空間掩埋模式。此方法實現了海底掩埋的可擴展、非侵入式繪圖，並支持受污染地點的環境評估。", "applications": ["海底電纜或管線巡檢：精確判斷電纜或管線是否被泥沙覆蓋，及時維護，避免損壞。", "尋找失落的飛機或船隻殘骸：透過分析殘骸的姿態和掩埋情況，推斷沉沒時間和原因。", "海洋考古：協助考古學家繪製海底文物的三維地圖，研究古代文明。"], "pitch": "各位創投先進，我們團隊開發的PoseIDON技術，是水下環境監測領域的革命性突破！想像一下，全球海洋污染日益嚴重，各國對海底廢棄物處理的需求迫在眉睫。我們的技術能精準、高效地繪製海底掩埋物地圖，幫助政府和企業評估污染風險、制定清理策略，甚至回收有價值的資源。除了環保應用，PoseIDON還能用於海底基礎設施巡檢、海洋考古等領域，市場潛力巨大。更重要的是，我們的技術具有高度的可擴展性，未來可以整合AI技術，實現自動化的海底環境監測。我們相信，PoseIDON將成為水下機器人視覺領域的領頭羊，為創投帶來豐厚的回報！現在投資，您將站在海洋科技的最前沿，共同開創藍色經濟的無限可能！", "audio": "docs/data/audios/2506.10386v1.wav"}
{"query": "Diffusion Model", "id": "2506.10955v1", "url": "http://arxiv.org/abs/2506.10955v1", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "published_date": "2025-06-12", "timestamp": "2025-06-13T18:19:29.309616", "title_zh": "ReGuidance：一個簡潔的擴散模型封裝器，用於提升困難反問題的樣本品質", "summary_zh": "本研究提出一個名為ReGuidance的簡潔封裝器，旨在提升預訓練擴散模型在解決反問題時的樣本真實性和品質。針對訊號雜訊比低的困難反問題，現有方法容易偏離數據流形，產生不真實的結果。ReGuidance透過逆轉候選解，並將其潛在向量作為擴散後驗抽樣(DPS)的初始化，來改善此問題。實驗證明，在大型圖像修復和高倍率超解析度等任務中，ReGuidance能顯著提升樣本品質和測量一致性，甚至在多模態數據分佈上，ReGuidance能同時提升獎勵值並使候選解更接近數據流形。本研究也為DPS提供了首個嚴格的演算法保證。", "applications": ["老照片修復：將模糊或損壞的老照片變得清晰，重現珍貴回憶。", "醫療影像增強：提升X光片、MRI等醫療影像的清晰度，幫助醫生更準確地診斷疾病。", "犯罪現場重建：根據模糊的監控錄影或目擊者描述，重建清晰的犯罪現場圖像，協助警方破案。"], "pitch": "各位投資人，我們正處於AI生成內容的黃金時代，但現有技術在處理複雜、模糊的反問題時仍力不從心。ReGuidance的出現，正是解決這一痛點的關鍵！想像一下，我們能將模糊的衛星圖像轉化為清晰的城市地圖，為自動駕駛提供更精確的環境資訊；或是將微弱的生物訊號放大，早期發現癌症等疾病。ReGuidance不僅能提升圖像品質，更能應用於工業檢測、科學研究等多個領域，市場潛力巨大。我們相信，ReGuidance將成為AI圖像處理領域的基礎設施，引領新一輪的技術革命，為投資者帶來豐厚的回報！", "audio": "docs/data/audios/2506.10955v1.wav"}
{"query": "AI", "id": "2506.10897v1", "url": "http://arxiv.org/abs/2506.10897v1", "title": "GenPlanX. Generation of Plans and Execution", "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tomás de la Rosa", "Alfredo Garrachón", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:11:48.504597", "title_zh": "GenPlanX：計畫生成與執行", "summary_zh": "GenPlanX結合大型語言模型（LLM）與傳統AI規劃引擎，讓使用者能用自然語言描述複雜任務，系統即可自動生成行動方案並執行。它能理解人類意圖，簡化工作流程，提升生產力。想像一下，只要口頭指示，GenPlanX就能自動安排會議、預訂差旅、甚至管理日常家務。這項技術讓人機協作更無縫，釋放更多時間與精力，專注於更重要的事務。GenPlanX不僅是工具，更是個人助理，開啟智慧生活新篇章。", "applications": ["**會議安排小幫手：** 只要告訴GenPlanX『下週安排與客戶的會議，討論合約細節』，它會自動檢查雙方行程、發送邀請、預訂會議室，甚至準備相關文件。", "**智能家居管家：** 說聲『我到家了』，GenPlanX會自動開燈、調整室溫、播放音樂，讓你一回到家就能享受舒適的環境。如果說『準備晚餐』，它會根據你的飲食偏好，推薦菜單並開始準備食材清單。", "**差旅規劃專家：** 告訴GenPlanX『我要去台北出差三天』，它會自動搜尋機票、酒店、安排交通，並根據你的預算和偏好，提供最佳方案。"], "pitch": "各位投資人，想像一下，一個能聽懂人話、自動完成複雜任務的AI助理，將徹底顛覆工作與生活方式。GenPlanX正是實現這個願景的關鍵。它結合了LLM的理解能力與傳統AI的執行效率，讓使用者能用自然語言控制一切。這不僅僅是技術創新，更是一場效率革命。試想，企業可以大幅降低人力成本，個人可以擺脫繁瑣事務，專注於創造性工作。GenPlanX的應用場景無限廣闊，從智能辦公、智能家居到智能城市，都將迎來指數級的增長。我們預計，GenPlanX將成為未來人機協作的基礎設施，市場規模將達到數千億美元。現在投資GenPlanX，就是投資未來，讓我們一起引領這場AI革命！", "audio": "docs/data/audios/2506.10897v1.wav"}
{"query": "Foundation Model", "id": "2506.10335v1", "url": "http://arxiv.org/abs/2506.10335v1", "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that\nsurpasses the neural radiance field (NeRF) in both rendering speed and visual\nquality by leveraging an explicit 3D scene representation. Existing 3DGS\napproaches require a large number of calibrated views to generate a consistent\nand complete scene representation. When input views are limited, 3DGS tends to\noverfit the training views, leading to noticeable degradation in rendering\nquality. To address this limitation, we propose a Point-wise Feature-Aware\nGaussian Splatting framework that enables real-time, high-quality rendering\nfrom sparse training views. Specifically, we first employ the latest stereo\nfoundation model to estimate accurate camera poses and reconstruct a dense\npoint cloud for Gaussian initialization. We then encode the colour attributes\nof each 3D Gaussian by sampling and aggregating multiscale 2D appearance\nfeatures from sparse inputs. To enhance point-wise appearance representation,\nwe design a point interaction network based on a self-attention mechanism,\nallowing each Gaussian point to interact with its nearest neighbors. These\nenriched features are subsequently decoded into Gaussian parameters through two\nlightweight multi-layer perceptrons (MLPs) for final rendering. Extensive\nexperiments on diverse benchmarks demonstrate that our method significantly\noutperforms NeRF-based approaches and achieves competitive performance under\nfew-shot settings compared to the state-of-the-art 3DGS methods.", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:13:07.644754", "title_zh": "PointGS：基於點注意力感知與高斯濺射的稀疏視角合成", "summary_zh": "本研究提出一個名為PointGS的新框架，旨在解決3D高斯濺射(3DGS)技術在視角稀疏情況下容易過擬合的問題。PointGS首先利用立體視覺模型估算精確的相機位姿，並重建稠密點雲以初始化高斯分佈。接著，透過採樣和聚合稀疏輸入中的多尺度2D外觀特徵，來編碼每個3D高斯的顏色屬性。此外，設計基於自注意力機制的點交互網絡，增強點級別的外觀表示。實驗證明，PointGS在稀疏視角下，能實現高品質、即時的渲染效果，顯著優於傳統NeRF方法，並與最先進的3DGS方法競爭。", "applications": ["線上購物：想像一下，你可以在網路上用手機隨意繞著商品拍攝幾張照片，就能立即產生商品的3D模型，讓你從各個角度仔細觀察，就像在實體店面一樣。", "室內設計：設計師只要用手機掃描房間的幾個角落，就能快速建立房間的3D模型，方便擺放虛擬家具，讓客戶在裝修前就能預覽實際效果。", "遊戲開發：遊戲開發者可以使用少量照片快速建立遊戲場景的3D模型，大幅縮短開發時間，並且能創造出更逼真的遊戲世界。"], "pitch": "各位投資人，我們正處於視覺技術的黃金時代！PointGS技術突破了傳統3D建模的限制，僅需少量圖片即可生成高品質3D模型，這意味著更低的成本、更快的速度和更廣泛的應用。想像一下，未來AR/VR的普及，人們需要大量3D內容，而PointGS將成為內容生成的關鍵引擎。從電商的沉浸式購物體驗、到遊戲的逼真場景、再到工業設計的快速原型製作，PointGS的應用潛力無限。我們預計，隨著技術的成熟，PointGS將成為元宇宙基礎建設的重要一環，市場規模將達到數十億美元。現在投資PointGS，就是投資未來！", "audio": "docs/data/audios/2506.10335v1.wav"}
{"query": "Diffusion Model", "id": "2506.10892v1", "url": "http://arxiv.org/abs/2506.10892v1", "title": "The Diffusion Duality", "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "published_date": "2025-06-12", "timestamp": "2025-06-13T21:14:15.989052", "title_zh": "擴散二元性", "summary_zh": "本研究提出一種名為Duo的方法，旨在提升均勻狀態離散擴散模型在文字生成方面的效能。Duo的核心洞見在於，均勻狀態擴散過程實際上源自底層的高斯擴散。透過將高斯擴散的強大技術轉移到離散擴散模型，Duo在訓練和採樣方面都獲得顯著改善。首先，引入了受高斯過程引導的課程學習策略，透過降低方差，使訓練速度翻倍。其次，提出了離散一致性蒸餾算法，將連續一致性蒸餾適應於離散環境，實現了擴散語言模型中的少步生成，將採樣速度提高了兩個數量級。實驗結果顯示，Duo在多個基準測試中超越了自迴歸模型。", "applications": ["**AI寫作助手：**想像一下，你可以用更短的時間，產出更高品質的文章。無論是撰寫行銷文案、新聞稿，甚至是小說情節，AI都能幫你快速生成初稿，節省大量的時間和精力。", "**即時翻譯校正：**出國旅遊或工作時，透過App即時翻譯對話，AI不僅能快速翻譯，還能自動校正語法和用詞，讓溝通更加流暢自然，避免誤解。", "**遊戲對話生成：**在遊戲中，AI能根據玩家的行為和情境，快速生成豐富多樣的NPC對話，讓遊戲世界更加生動有趣，提升玩家的沉浸感。"], "pitch": "各位投資人，我們正處於AI內容生成的黃金時代！Duo技術不僅解決了現有擴散模型速度慢的問題，更在品質上超越了傳統自迴歸模型。想像一下，未來所有的內容創作都將由AI驅動，Duo將成為這場革命的核心引擎！從自動生成劇本、程式碼，到客製化教育內容，Duo的應用潛力無窮。我們預計，Duo將在三年內佔據AI內容生成市場的領先地位，為投資者帶來數十倍甚至數百倍的回報。這不僅僅是一項技術，更是一個改變世界的機會！現在加入，一起塑造AI驅動的未來！", "audio": "docs/data/audios/2506.10892v1.wav"}
